"""
ë°©ì†¡ í¸ì„± AI ì¶”ì²œ ì›Œí¬í”Œë¡œìš°
LangChain ê¸°ë°˜ 2ë‹¨ê³„ ì›Œí¬í”Œë¡œìš°: AI ë°©í–¥ íƒìƒ‰ + ê³ ì† ë­í‚¹
"""

import asyncio
import json
import logging
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
import pandas as pd
import numpy as np
from sqlalchemy import create_engine, text
import os

from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import ChatPromptTemplate
from .external_apis import ExternalAPIManager

from .dependencies import get_product_embedder
from . import broadcast_recommender as br
from .schemas import BroadcastResponse, BroadcastRecommendation, ProductInfo, Reasoning, BusinessMetrics, NaverProduct, CompetitorProduct, LastBroadcastMetrics
from .external_products_service import ExternalProductsService
from .services.broadcast_history_service import BroadcastHistoryService
from .netezza_config import netezza_conn

logger = logging.getLogger(__name__)

class BroadcastWorkflow:
    """ë°©ì†¡ í¸ì„± AI ì¶”ì²œ ì›Œí¬í”Œë¡œìš°"""
    
    def __init__(self, model):
        self.model = model  # XGBoost ëª¨ë¸
        self.product_embedder = get_product_embedder()
        
        # AI íŠ¸ë Œë“œ ìºì‹œ (ì‹œê°„ëŒ€ë³„)
        self._ai_trends_cache = {}
        self._cache_ttl = 3600  # 1ì‹œê°„ (ì´ˆ)
        
        # LangChain LLM ì´ˆê¸°í™”
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.5,
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        
        # DB ì—°ê²°
        self.engine = create_engine(os.getenv("POSTGRES_URI"))
        
        # ì™¸ë¶€ ìƒí’ˆ ì„œë¹„ìŠ¤
        self.external_products_service = ExternalProductsService()
        
        # ë°©ì†¡ ì´ë ¥ ì„œë¹„ìŠ¤ (Netezza)
        self.broadcast_history_service = BroadcastHistoryService()
    
    async def process_broadcast_recommendation(
        self, 
        broadcast_time: str, 
        recommendation_count: int = 5,
        trend_weight: float = 0.3,  # íŠ¸ë Œë“œ ê°€ì¤‘ì¹˜ (0.3 = 30%)
        selling_weight: float = 0.7   # ë§¤ì¶œ ì˜ˆì¸¡ ê°€ì¤‘ì¹˜ (0.7 = 70%)
    ) -> BroadcastResponse:
        """ë©”ì¸ ì›Œí¬í”Œë¡œìš°: ë°©ì†¡ ì‹œê°„ ê¸°ë°˜ ì¶”ì²œ
        
        Args:
            broadcast_time: ë°©ì†¡ ì‹œê°„
            recommendation_count: ì¶”ì²œ ê°œìˆ˜
            trend_weight: íŠ¸ë Œë“œ ê°€ì¤‘ì¹˜ (0.0~1.0, ê¸°ë³¸ 0.3)
            selling_weight: ë§¤ì¶œ ì˜ˆì¸¡ ê°€ì¤‘ì¹˜ (0.0~1.0, ê¸°ë³¸ 0.7)
                - ì˜ˆ: trend_weight=0.3, selling_weight=0.7 â†’ íŠ¸ë Œë“œ 30%, ë§¤ì¶œ 70%
                - ì˜ˆ: trend_weight=0.5, selling_weight=0.5 â†’ ê· í˜• (50:50)
        """
        
        import time
        workflow_start = time.time()
        
        print("=== [DEBUG] process_broadcast_recommendation ì‹œì‘ ===")
        request_time = datetime.now().isoformat()
        logger.info(f"ë°©ì†¡ ì¶”ì²œ ì›Œí¬í”Œë¡œìš° ì‹œì‘: {broadcast_time}")
        print(f"=== [DEBUG] broadcast_time: {broadcast_time}, recommendation_count: {recommendation_count} ===")
        
        try:
            # 1ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ë° í†µí•© í‚¤ì›Œë“œ ìƒì„±
            step_start = time.time()
            print("=== [DEBUG] _collect_context_and_keywords í˜¸ì¶œ ===")
            context = await self._collect_context_and_keywords(broadcast_time)
            print(f"â±ï¸  [1ë‹¨ê³„] ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘: {time.time() - step_start:.2f}ì´ˆ")
            print(f"=== [DEBUG] í†µí•© í‚¤ì›Œë“œ: {len(context.get('unified_keywords', []))}ê°œ ===")
            
            # 2. í†µí•© ê²€ìƒ‰ ì‹¤í–‰ (1íšŒ)
            step_start = time.time()
            print("=== [DEBUG] _execute_unified_search í˜¸ì¶œ ===")
            search_result = await self._execute_unified_search(context, context.get("unified_keywords", []))
            print(f"â±ï¸  [2ë‹¨ê³„] í†µí•© ê²€ìƒ‰: {time.time() - step_start:.2f}ì´ˆ")
            print(f"=== [DEBUG] ê²€ìƒ‰ ì™„ë£Œ - ì§ì ‘ë§¤ì¹­: {len(search_result['direct_products'])}ê°œ, ì¹´í…Œê³ ë¦¬: {len(search_result['category_groups'])}ê°œ ===")
            
            # ê²€ìƒ‰ì— ì‚¬ìš©ëœ í‚¤ì›Œë“œë¥¼ contextì— ì €ì¥
            context["search_keywords"] = search_result.get("search_keywords", [])
            
            # 3. í›„ë³´êµ° ìƒì„± (ê°€ì¤‘ì¹˜ ê¸°ë°˜ ë¹„ìœ¨ ì¡°ì •)
            step_start = time.time()
            print("=== [DEBUG] _generate_unified_candidates í˜¸ì¶œ ===")
            max_trend = max(1, int(recommendation_count * trend_weight))  # ìµœì†Œ 1ê°œ
            max_sales = recommendation_count - max_trend + 3  # ì—¬ìœ ë¶„ ì¶”ê°€
            print(f"=== [DEBUG] ê°€ì¤‘ì¹˜ ì ìš©: íŠ¸ë Œë“œ {max_trend}ê°œ ({trend_weight:.0%}), ë§¤ì¶œ {max_sales}ê°œ ({selling_weight:.0%}) ===")
            
            candidate_products, category_scores = await self._generate_unified_candidates(
                search_result,
                context,
                max_trend_match=max_trend,
                max_sales_prediction=max_sales
            )
            print(f"â±ï¸  [3ë‹¨ê³„] í›„ë³´êµ° ìƒì„±: {time.time() - step_start:.2f}ì´ˆ")
            print(f"=== [DEBUG] í›„ë³´êµ° ìƒì„± ì™„ë£Œ: {len(candidate_products)}ê°œ ===")
            
            # 4. ìµœì¢… ë­í‚¹ ê³„ì‚°
            step_start = time.time()
            ranked_products = await self._rank_final_candidates(
                candidate_products,
                category_scores=category_scores,
                context=context
            )
            print(f"â±ï¸  [4ë‹¨ê³„] ìµœì¢… ë­í‚¹: {time.time() - step_start:.2f}ì´ˆ")
            
            # 5. API ì‘ë‹µ ìƒì„±
            step_start = time.time()
            response = await self._format_response(ranked_products[:recommendation_count], context)
            response.requestTime = request_time
            print(f"â±ï¸  [5ë‹¨ê³„] ì‘ë‹µ ìƒì„±: {time.time() - step_start:.2f}ì´ˆ")
            
            total_time = time.time() - workflow_start
            print(f"â±ï¸  ===== ì›Œí¬í”Œë¡œìš° ì´ ì‹œê°„: {total_time:.2f}ì´ˆ =====")
            
            logger.info(f"ë°©ì†¡ ì¶”ì²œ ì™„ë£Œ: {len(ranked_products)}ê°œ ì¶”ì²œ")
            return response
            
        except Exception as e:
            print(f"=== [DEBUG] ì˜ˆì™¸ ë°œìƒ: {type(e).__name__}: {e} ===")
            import traceback
            traceback.print_exc()
            logger.error(f"ë°©ì†¡ ì¶”ì²œ ì›Œí¬í”Œë¡œìš° ì˜¤ë¥˜: {e}")
            # OpenAI API ê´€ë ¨ ì˜¤ë¥˜ëŠ” ìƒìœ„ë¡œ ì „íŒŒ (503 ì—ëŸ¬ ë°˜í™˜ìš©)
            if "AI ì„œë¹„ìŠ¤" in str(e) or "OpenAI" in str(e) or "í• ë‹¹ëŸ‰" in str(e):
                raise e
            # ê¸°íƒ€ ë‚´ë¶€ ì˜¤ë¥˜ëŠ” 500 ì—ëŸ¬ë¡œ ì²˜ë¦¬
            raise Exception(f"ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜: {e}")
    
    async def _collect_context_and_keywords(self, broadcast_time: str) -> Dict[str, Any]:
        """ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ë° í†µí•© í‚¤ì›Œë“œ ìƒì„± (ê°œì„ ëœ ë²„ì „)"""
        
        # ë°©ì†¡ ì‹œê°„ íŒŒì‹±
        broadcast_dt = datetime.fromisoformat(broadcast_time.replace('Z', '+00:00'))
        
        # DBì—ì„œ ê³µíœ´ì¼ ì •ë³´ ì¡°íšŒ
        holiday_name = await self._get_holiday_from_db(broadcast_dt.date())
        
        context = {
            "broadcast_time": broadcast_time,
            "broadcast_dt": broadcast_dt,
            "hour": broadcast_dt.hour,
            "weekday": broadcast_dt.weekday(),
            "season": self._get_season(broadcast_dt.month),
            "holiday_name": holiday_name  # ê³µíœ´ì¼ ì •ë³´ ì¶”ê°€
        }
        
        # ë‚ ì”¨ ì •ë³´ ìˆ˜ì§‘
        weather_info = br.get_weather_by_date(broadcast_dt.date())
        context["weather"] = weather_info

        # ì‹œê°„ëŒ€ ì •ë³´
        time_slot = self._get_time_slot(broadcast_dt)
        day_type = "ì£¼ë§" if broadcast_dt.weekday() >= 5 else "í‰ì¼"
        context["time_slot"] = time_slot
        context["day_type"] = day_type

        # AI ê¸°ë°˜ íŠ¸ë Œë“œ ìƒì„± (LLM API) - ìºì‹± ì ìš©
        cache_key = f"{broadcast_dt.hour}_{weather_info.get('weather', 'Clear')}"
        current_time = datetime.now().timestamp()
        
        # ìºì‹œ í™•ì¸
        if cache_key in self._ai_trends_cache:
            cached_data, cached_time = self._ai_trends_cache[cache_key]
            if current_time - cached_time < self._cache_ttl:
                context["ai_trends"] = cached_data
                logger.info(f"âœ… AI íŠ¸ë Œë“œ ìºì‹œ íˆíŠ¸ ({cache_key}): {len(cached_data)}ê°œ í‚¤ì›Œë“œ")
            else:
                # ìºì‹œ ë§Œë£Œ
                del self._ai_trends_cache[cache_key]
                logger.info(f"â° AI íŠ¸ë Œë“œ ìºì‹œ ë§Œë£Œ ({cache_key})")
                context["ai_trends"] = None
        else:
            context["ai_trends"] = None
        
        # ìºì‹œ ë¯¸ìŠ¤ ì‹œ API í˜¸ì¶œ
        if context["ai_trends"] is None:
            api_manager = ExternalAPIManager()
            if api_manager.llm_trend_api:
                try:
                    import time
                    api_start = time.time()
                    # ë°©ì†¡ ì‹œê°„ê³¼ ë‚ ì”¨ ì •ë³´ë¥¼ ì „ë‹¬í•˜ì—¬ ë§¥ë½ ê¸°ë°˜ íŠ¸ë Œë“œ ìƒì„±
                    llm_trends = await api_manager.llm_trend_api.get_trending_searches(
                        hour=broadcast_dt.hour,
                        weather_info=weather_info
                    )
                    api_time = time.time() - api_start
                    # AIê°€ ìƒì„±í•œ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì¶”ê°€
                    context["ai_trends"] = [t["keyword"] for t in llm_trends]
                    # ìºì‹œ ì €ì¥
                    self._ai_trends_cache[cache_key] = (context["ai_trends"], current_time)
                    logger.info(f"ğŸ”¥ AI íŠ¸ë Œë“œ ìƒì„± ì™„ë£Œ ({broadcast_dt.hour}ì‹œ, {weather_info.get('weather', 'N/A')}): {len(llm_trends)}ê°œ í‚¤ì›Œë“œ (ì†Œìš”: {api_time:.2f}ì´ˆ)")
                    logger.info(f"AI íŠ¸ë Œë“œ: {context['ai_trends'][:5]}...")  # ìƒìœ„ 5ê°œë§Œ ë¡œê·¸
                except Exception as e:
                    logger.error(f"AI íŠ¸ë Œë“œ ìƒì„± ì‹¤íŒ¨: {e}")
                    context["ai_trends"] = []
            else:
                logger.warning("OpenAI API í‚¤ ì—†ìŒ - AI íŠ¸ë Œë“œ ìƒì„± ê±´ë„ˆëœ€")
                context["ai_trends"] = []

        # ì»¨í…ìŠ¤íŠ¸ ë¡œê·¸ ì¶œë ¥
        logger.info(f"ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ - ê³„ì ˆ: {context['season']}, ì‹œê°„ëŒ€: {time_slot}, ìš”ì¼: {day_type}")
        if holiday_name:
            logger.info(f"ğŸ‰ ê³µíœ´ì¼: {holiday_name}")
        logger.info(f"ë‚ ì”¨: {weather_info.get('weather', 'N/A')}")
        
        # í†µí•© í‚¤ì›Œë“œ ìƒì„± (AI íŠ¸ë Œë“œ + ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ)
        unified_keywords = []
        
        # 1. AI íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì¶”ê°€
        if context.get("ai_trends"):
            unified_keywords.extend(context["ai_trends"][:10])  # ìƒìœ„ 10ê°œ
            logger.info(f"AI íŠ¸ë Œë“œ í‚¤ì›Œë“œ {len(context['ai_trends'][:10])}ê°œ ì¶”ê°€")
        
        # 2. ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í‚¤ì›Œë“œ ìƒì„±
        context_keywords = await self._generate_context_keywords(context)
        if context_keywords:
            unified_keywords.extend(context_keywords)
            logger.info(f"ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ {len(context_keywords)}ê°œ ì¶”ê°€")
        
        # 3. ì¤‘ë³µ ì œê±° ë° ì €ì¥
        context["unified_keywords"] = list(dict.fromkeys(unified_keywords))  # ìˆœì„œ ìœ ì§€ ì¤‘ë³µ ì œê±°
        logger.info(f"í†µí•© í‚¤ì›Œë“œ ìƒì„± ì™„ë£Œ: ì´ {len(context['unified_keywords'])}ê°œ")
        logger.info(f"í†µí•© í‚¤ì›Œë“œ: {context['unified_keywords']}")

        return context
    
    async def _get_holiday_from_db(self, target_date) -> Optional[str]:
        """DBì—ì„œ ê³µíœ´ì¼ ì •ë³´ ì¡°íšŒ"""
        try:
            with self.engine.connect() as conn:
                query = text("""
                    SELECT holiday_name 
                    FROM TAIHOLIDAYS 
                    WHERE holiday_date = :target_date
                """)
                result = conn.execute(query, {"target_date": target_date})
                row = result.fetchone()
                
                if row:
                    holiday_name = row[0]
                    logger.info(f"ê³µíœ´ì¼ ì¡°íšŒ ì„±ê³µ: {target_date} -> {holiday_name}")
                    return holiday_name
                else:
                    return None
        except Exception as e:
            logger.error(f"ê³µíœ´ì¼ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return None
    
    def _get_season(self, month: int) -> str:
        """ê³„ì ˆ ì •ë³´ ë°˜í™˜"""
        if month in [12, 1, 2]:
            return "ê²¨ìš¸"
        elif month in [3, 4, 5]:
            return "ë´„"
        elif month in [6, 7, 8]:
            return "ì—¬ë¦„"
        else:
            return "ê°€ì„"
    
    def _get_time_slot(self, dt: datetime) -> str:
        """ì‹œê°„ëŒ€ ì •ë³´ ë°˜í™˜"""
        hour = dt.hour
        if 6 <= hour < 12:
            return "ì˜¤ì „"
        elif 12 <= hour < 18:
            return "ì˜¤í›„"
        elif 18 <= hour < 24:
            return "ì €ë…"
        else:
            return "ìƒˆë²½"

    async def _classify_keywords_with_langchain(self, context: Dict[str, Any]) -> Dict[str, List[str]]:
        """LangChainì„ ì‚¬ìš©í•œ í‚¤ì›Œë“œ ë¶„ë¥˜"""
        
        # ëª¨ë“  í‚¤ì›Œë“œ ìˆ˜ì§‘
        all_keywords = []
        
        # ë‚ ì”¨ í‚¤ì›Œë“œ
        if context["weather"].get("weather"):
            all_keywords.append(context["weather"]["weather"])
        
        # ì‹œê°„/ë‚ ì§œ í‚¤ì›Œë“œ
        all_keywords.extend([context["time_slot"], context["day_type"], context["season"]])
        
        # AI ìƒì„± íŠ¸ë Œë“œ ì¶”ê°€! (ë‚ ì”¨/ì‹œê°„ ê¸°ë°˜ íŠ¸ë Œë“œ)
        if "ai_trends" in context and context["ai_trends"]:
            all_keywords.extend(context["ai_trends"][:10])  # ìƒìœ„ 10ê°œë§Œ í¬í•¨
            logger.info(f"AI íŠ¸ë Œë“œ í‚¤ì›Œë“œ {len(context['ai_trends'][:10])}ê°œ ì¶”ê°€ë¨")

        # ìˆ˜ì§‘ëœ í‚¤ì›Œë“œë“¤ ë¡œê·¸ ì¶œë ¥
        logger.info(f"í‚¤ì›Œë“œ ë¶„ë¥˜ ì‹œì‘ - ì´ {len(all_keywords)}ê°œ í‚¤ì›Œë“œ: {all_keywords}")

        # LangChain í”„ë¡¬í”„íŠ¸
        classification_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ í™ˆì‡¼í•‘ ë°©ì†¡ í¸ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì£¼ì–´ì§„ í‚¤ì›Œë“œë“¤ì„ ë‹¤ìŒ ë‘ ê·¸ë£¹ìœ¼ë¡œ ë¶„ë¥˜í•´ì£¼ì„¸ìš”:

1. category_keywords: ìƒí’ˆ ì¹´í…Œê³ ë¦¬ì™€ ì—°ê´€ëœ í‚¤ì›Œë“œ (ì˜ˆ: íë¦°ë‚ ì”¨, ìº í•‘, ê±´ê°•ì‹í’ˆ, ê²¨ìš¸)
2. product_keywords: íŠ¹ì • ìƒí’ˆì„ ì§€ì¹­í•˜ëŠ” í‚¤ì›Œë“œ (ì˜ˆ: ì•„ì´í°, ë¼ë¶€ë¶€, ì •ê´€ì¥)

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µí•´ì£¼ì„¸ìš”."""),
            ("human", "í‚¤ì›Œë“œ ëª©ë¡: {keywords}")
        ])
         
        chain = classification_prompt | self.llm | JsonOutputParser()
        
        try:
            result = await chain.ainvoke({"keywords": ", ".join(all_keywords)})
            logger.info(f"í‚¤ì›Œë“œ ë¶„ë¥˜ ì™„ë£Œ: ì¹´í…Œê³ ë¦¬ {len(result.get('category_keywords', []))}ê°œ, ìƒí’ˆ {len(result.get('product_keywords', []))}ê°œ")
            logger.info(f"ë¶„ë¥˜ëœ ì¹´í…Œê³ ë¦¬ í‚¤ì›Œë“œ: {result.get('category_keywords', [])}")
            logger.info(f"ë¶„ë¥˜ëœ ìƒí’ˆ í‚¤ì›Œë“œ: {result.get('product_keywords', [])}")
            return result
        except Exception as e:
            logger.error(f"í‚¤ì›Œë“œ ë¶„ë¥˜ ì˜¤ë¥˜: {e}")
            # OpenAI API í• ë‹¹ëŸ‰ ì†Œì§„ ë˜ëŠ” API ì˜¤ë¥˜ ì‹œ ì˜ˆì™¸ ë°œìƒ
            if "insufficient_quota" in str(e) or "429" in str(e):
                raise Exception(f"AI ì„œë¹„ìŠ¤ ì¼ì‹œ ì¤‘ë‹¨ - OpenAI API í• ë‹¹ëŸ‰ ì†Œì§„: {e}")
            elif "api" in str(e).lower() or "openai" in str(e).lower():
                raise Exception(f"AI ì„œë¹„ìŠ¤ ì—°ê²° ì˜¤ë¥˜: {e}")
            else:
                # ê¸°íƒ€ ì˜¤ë¥˜ëŠ” í´ë°± ë¡œì§ ì‚¬ìš©
                return {
                    "category_keywords": all_keywords[:10],
                    "product_keywords": []
                }
    
    async def _execute_unified_search(self, context: Dict[str, Any], unified_keywords: List[str]) -> Dict[str, Any]:
        """í†µí•© ê²€ìƒ‰: 1íšŒ Qdrant ê²€ìƒ‰ìœ¼ë¡œ ì§ì ‘ë§¤ì¹­/ì¹´í…Œê³ ë¦¬ ìƒí’ˆ ë¶„ë¥˜"""
        
        print(f"=== [DEBUG Unified Search] ì‹œì‘, keywords: {len(unified_keywords)}ê°œ ===")
        
        if not unified_keywords:
            logger.warning("í†µí•© í‚¤ì›Œë“œ ì—†ìŒ - ë¹ˆ ê²°ê³¼ ë°˜í™˜")
            return {"direct_products": [], "category_groups": {}}
        
        query = " ".join(unified_keywords)
        print(f"=== [DEBUG Unified Search] Qdrant ê²€ìƒ‰ ì¿¼ë¦¬: '{query}' ===")
        
        try:
            # Qdrant í†µí•© ê²€ìƒ‰ (1íšŒ)
            all_results = self.product_embedder.search_products(
                trend_keywords=[query],
                top_k=50,  # í›„ë³´êµ°
                score_threshold=0.3,
                only_ready_products=True
            )
            print(f"=== [DEBUG Unified Search] ê²€ìƒ‰ ê²°ê³¼: {len(all_results)}ê°œ ìƒí’ˆ ===")
            
            # ìœ ì‚¬ë„ ê¸°ë°˜ ë¶„ë¥˜
            direct_products = []      # ê³ ìœ ì‚¬ë„: ì§ì ‘ ì¶”ì²œ
            category_groups = {}      # ì¤‘ìœ ì‚¬ë„: ì¹´í…Œê³ ë¦¬ë³„ ê·¸ë£¹
            
            # ìœ ì‚¬ë„ ì„ê³„ê°’
            HIGH_SIMILARITY_THRESHOLD = 0.7
            
            for product in all_results:
                similarity = product.get("similarity_score", 0)
                category = product.get("category_main", "ê¸°íƒ€")
                
                # ê³ ìœ ì‚¬ë„ ìƒí’ˆ: ì§ì ‘ ë§¤ì¹­ (XGBoost ê±´ë„ˆë›°ê¸° í›„ë³´)
                if similarity >= HIGH_SIMILARITY_THRESHOLD:
                    # ë³´ì™„: ì•ˆì „ì¥ì¹˜ - ë°©ì†¡í…Œì´í”„ í™•ì¸
                    if product.get("tape_code") and product.get("tape_name"):
                        direct_products.append({
                            **product,
                            "source": "direct_match",
                            "similarity_score": similarity
                        })
                        print(f"  - ì§ì ‘ë§¤ì¹­: {product.get('product_name')} (ìœ ì‚¬ë„: {similarity:.2f})")
                
                # ì¤‘ìœ ì‚¬ë„: ì¹´í…Œê³ ë¦¬ ê·¸ë£¹í•‘
                if category not in category_groups:
                    category_groups[category] = []
                category_groups[category].append(product)
            
            print(f"=== [DEBUG Unified Search] ì§ì ‘ë§¤ì¹­: {len(direct_products)}ê°œ, ì¹´í…Œê³ ë¦¬: {len(category_groups)}ê°œ ===")
            
            return {
                "direct_products": direct_products,
                "category_groups": category_groups,
                "search_keywords": unified_keywords[:5]  # ê²€ìƒ‰ì— ì‚¬ìš©ëœ í‚¤ì›Œë“œ ìƒìœ„ 5ê°œ
            }
            
        except Exception as e:
            logger.error(f"í†µí•© ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            return {"direct_products": [], "category_groups": {}}
    
    async def _get_realtime_trend_keywords(self) -> List[str]:
        """ì‹¤ì‹œê°„ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ìˆ˜ì§‘ (OpenAI Web Search)"""
        from openai import OpenAI
        
        try:
            client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            
            prompt = """ë‹¹ì‹ ì€ í•œêµ­ ì‡¼í•‘ íŠ¸ë Œë“œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ì„ë¬´: ì§€ê¸ˆ ì´ ìˆœê°„ í•œêµ­ì—ì„œ ì¸ê¸° ìˆëŠ” ì‡¼í•‘ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ì°¾ìœ¼ì„¸ìš”**

ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ë‹¤ìŒ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”:
- í•œêµ­ ì‹¤ì‹œê°„ ì¸ê¸° ê²€ìƒ‰ì–´
- í˜„ì¬ ì´ìŠˆê°€ ë˜ëŠ” ì´ë²¤íŠ¸ (ìŠ¤í¬ì¸ , ë‚ ì”¨ ì´ìŠˆ, ì‚¬íšŒ ì´ë²¤íŠ¸ ë“±)
- ì‡¼í•‘ íŠ¸ë Œë“œ í‚¤ì›Œë“œ

**ì¤‘ìš”:**
- ì‡¼í•‘/ìƒí’ˆê³¼ ì—°ê´€ ê°€ëŠ¥í•œ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
- 3-5ê°œì˜ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì„ ë³„
- ë°˜ë“œì‹œ JSON ë°°ì—´ë¡œë§Œ ë°˜í™˜: ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", ...]

**ì˜ˆì‹œ:**
- ê°€ì„ì•¼êµ¬ ê²½ê¸° ì¤‘ â†’ ["ì•¼êµ¬", "ì¹˜í‚¨", "ë§¥ì£¼", "ì‘ì›ìš©í’ˆ"]
- í•œíŒŒì£¼ì˜ë³´ â†’ ["ë‚œë°©", "ì˜¨ì—´ê¸°", "í•«íŒ©"]
- í¬ë¦¬ìŠ¤ë§ˆìŠ¤ ì‹œì¦Œ â†’ ["í¬ë¦¬ìŠ¤ë§ˆìŠ¤", "ì„ ë¬¼", "íŒŒí‹°ìš©í’ˆ"]
"""
            
            print("=" * 80)
            print("[2ë‹¨ê³„ - OpenAI Web Search] ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ìˆ˜ì§‘ ì‹œì‘")
            print("=" * 80)
            print(f"[í”„ë¡¬í”„íŠ¸]\n{prompt}")
            print("=" * 80)
            logger.info(f"[2ë‹¨ê³„] ì‹¤ì‹œê°„ íŠ¸ë Œë“œ í”„ë¡¬í”„íŠ¸: {prompt[:200]}...")
            
            response = client.responses.create(
                model="gpt-4o",
                tools=[{
                    "type": "web_search_preview",
                    "search_context_size": "medium",
                    "user_location": {
                        "type": "approximate",
                        "country": "KR",
                        "timezone": "Asia/Seoul"
                    }
                }],
                input=prompt,
                max_output_tokens=200
            )
            
            result_text = response.output_text
            print("=" * 80)
            print(f"[2ë‹¨ê³„ - ì‘ë‹µ] {result_text}")
            print("=" * 80)
            logger.info(f"[2ë‹¨ê³„] ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ì‘ë‹µ: {result_text}")
            
            # JSON ë°°ì—´ ì¶”ì¶œ
            import json
            import re
            json_match = re.search(r'\[.*?\]', result_text, re.DOTALL)
            if json_match:
                keywords = json.loads(json_match.group())
                print(f"[2ë‹¨ê³„ - ì¶”ì¶œ ì„±ê³µ] í‚¤ì›Œë“œ: {keywords}")
                logger.info(f"[2ë‹¨ê³„] ì‹¤ì‹œê°„ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì¶”ì¶œ ì„±ê³µ: {keywords}")
                return keywords[:5]  # ìµœëŒ€ 5ê°œë§Œ
            else:
                print("[2ë‹¨ê³„ - ì‹¤íŒ¨] JSON ë°°ì—´ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                logger.warning("[2ë‹¨ê³„] ì‹¤ì‹œê°„ íŠ¸ë Œë“œì—ì„œ JSON ë°°ì—´ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return []
                
        except Exception as e:
            print("=" * 80)
            print(f"[2ë‹¨ê³„ - ì˜¤ë¥˜] {e}")
            print("=" * 80)
            logger.error(f"[2ë‹¨ê³„] ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []
    
    async def _generate_base_context_keywords(self, context: Dict[str, Any]) -> List[str]:
        """ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ LangChainìœ¼ë¡œ ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„±"""
        
        # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ì¶œ (ì•ˆì „í•˜ê²Œ)
        weather_info = context.get("weather", {})
        logger.info(f"weather_info type: {type(weather_info)}, value: {weather_info}")
        
        if isinstance(weather_info, dict):
            weather = weather_info.get("weather", "ë§‘ìŒ")
            temperature = weather_info.get("temperature", 20)
        else:
            logger.warning(f"weather_info is not dict: {weather_info}")
            weather = "ë§‘ìŒ"
            temperature = 20
        
        time_slot = context.get("time_slot", "ì €ë…")
        season = context.get("season", "ë´„")
        day_type = context.get("day_type", "í‰ì¼")
        holiday_name = context.get("holiday_name")  # ê³µíœ´ì¼ ì •ë³´
        
        logger.info(f"ì¶”ì¶œëœ ì •ë³´ - weather: {weather}, temp: {temperature}, time_slot: {time_slot}, season: {season}, day_type: {day_type}, holiday: {holiday_name}")
        
        # LangChain í”„ë¡¬í”„íŠ¸
        keyword_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ í™ˆì‡¼í•‘ ë°©ì†¡ í¸ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì£¼ì–´ì§„ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ë¶„ì„í•˜ì—¬, í•´ë‹¹ ì‹œê°„/ë‚ ì”¨/ìƒí™©ì— ì í•©í•œ ìƒí’ˆ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”.

ì˜ˆì‹œ:
- ë‚ ì”¨ê°€ 'ë¹„'ì´ê³  ì €ë… ì‹œê°„ â†’ "ìš°ì‚°", "ë°©ìˆ˜", "ì‹¤ë‚´í™œë™", "ë”°ëœ»í•œìŒì‹", "ì§‘ì½•", "ìš”ë¦¬ë„êµ¬"
- ë‚ ì”¨ê°€ 'ë§‘ìŒ'ì´ê³  ì˜¤í›„ ì‹œê°„ â†’ "ì•¼ì™¸í™œë™", "ìš´ë™", "ìº í•‘", "ë ˆì €", "ìì™¸ì„ ì°¨ë‹¨"
- ê²¨ìš¸ì²  ì €ë… ì‹œê°„ â†’ "ë‚œë°©", "ë³´ì˜¨", "ë”°ëœ»í•œ", "ê²¨ìš¸ì˜ë¥˜", "ì˜¨ì—´", "ì°œì§ˆ"
- í¬ë¦¬ìŠ¤ë§ˆìŠ¤ â†’ "ì„ ë¬¼", "íŒŒí‹°", "ì¼€ì´í¬", "ì¥ì‹", "ê°€ì¡±ëª¨ì„", "ì—°ë§ì„ ë¬¼"
- ì¶”ì„ â†’ "ì„ ë¬¼ì„¸íŠ¸", "í•œë³µ", "ì†¡í¸", "ê·€ì„±", "ëª…ì ˆìŒì‹", "ì°¨ë¡€ìƒ"

**ì¤‘ìš”: ê³µíœ´ì¼ì´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ê³µíœ´ì¼ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ìš°ì„ ì ìœ¼ë¡œ í¬í•¨í•˜ì„¸ìš”!**

5-10ê°œì˜ í‚¤ì›Œë“œë¥¼ JSON ë°°ì—´ë¡œ ë°˜í™˜í•´ì£¼ì„¸ìš”."""),
            ("human", """ë‚ ì”¨: {weather}
ê¸°ì˜¨: {temperature}ë„
ì‹œê°„ëŒ€: {time_slot}
ê³„ì ˆ: {season}
ìš”ì¼ íƒ€ì…: {day_type}
ê³µíœ´ì¼: {holiday_name}

ìœ„ ìƒí™©ì— ì í•©í•œ ìƒí’ˆ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”. ê³µíœ´ì¼ì´ ìˆë‹¤ë©´ ê³µíœ´ì¼ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ë°˜ë“œì‹œ í¬í•¨í•˜ì„¸ìš”!""")
        ])
        
        chain = keyword_prompt | self.llm | JsonOutputParser()
        
        try:
            # í”„ë¡¬í”„íŠ¸ ë¡œê¹… (ëˆˆì— ë„ê²Œ)
            prompt_vars = {
                "weather": weather,
                "temperature": temperature,
                "time_slot": time_slot,
                "season": season,
                "day_type": day_type,
                "holiday_name": holiday_name if holiday_name else "ì—†ìŒ"
            }
            print("=" * 80)
            print("[1ë‹¨ê³„ - LangChain í”„ë¡¬í”„íŠ¸] ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ ìƒì„±")
            print("=" * 80)
            print(f"ì…ë ¥ ë³€ìˆ˜:")
            for key, value in prompt_vars.items():
                print(f"  - {key}: {value}")
            print("=" * 80)
            logger.info(f"[1ë‹¨ê³„] ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜: {prompt_vars}")
            
            result = await chain.ainvoke({
                "weather": weather,
                "temperature": temperature,
                "time_slot": time_slot,
                "season": season,
                "day_type": day_type,
                "holiday_name": holiday_name if holiday_name else "ì—†ìŒ"
            })
            # resultê°€ ë¦¬ìŠ¤íŠ¸ì¼ ìˆ˜ë„ ìˆê³  ë”•ì…”ë„ˆë¦¬ì¼ ìˆ˜ë„ ìˆìŒ
            if isinstance(result, list):
                keywords = result
            elif isinstance(result, dict):
                keywords = result.get("keywords", [])
            else:
                keywords = []
            
            print("=" * 80)
            print(f"[1ë‹¨ê³„ - ì‘ë‹µ] LLM ìƒì„± í‚¤ì›Œë“œ: {keywords}")
            print(f"[1ë‹¨ê³„ - ê²°ê³¼] ì´ {len(keywords)}ê°œ í‚¤ì›Œë“œ")
            print("=" * 80)
            logger.info(f"[1ë‹¨ê³„] ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í‚¤ì›Œë“œ ìƒì„± ì™„ë£Œ: {keywords}")
            logger.info(f"[1ë‹¨ê³„] ë°˜í™˜í•  í‚¤ì›Œë“œ ê°œìˆ˜: {len(keywords)}")
            return keywords
        except Exception as e:
            logger.error(f"ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ ìƒì„± ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(f"ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
            # í´ë°±: ì‹œê°„ëŒ€/ê³„ì ˆ ê¸°ë°˜ ì‹¤ìš©ì  í‚¤ì›Œë“œ
            fallback_keywords = []
            
            # ì‹œê°„ëŒ€ë³„ í‚¤ì›Œë“œ
            if time_slot == "ì €ë…":
                fallback_keywords.extend(["ì €ë…ì‹ì‚¬", "ì‹¤ë‚´í™œë™", "íœ´ì‹", "ê°€ì¡±ì‹œê°„"])
            elif time_slot == "ì˜¤ì „":
                fallback_keywords.extend(["ì•„ì¹¨", "ì¶œê·¼", "í™œë ¥", "ê±´ê°•"])
            elif time_slot == "ì˜¤í›„":
                fallback_keywords.extend(["ì ì‹¬", "ì•¼ì™¸í™œë™", "ìš´ë™", "ì‡¼í•‘"])
            else:
                fallback_keywords.extend(["ë°¤", "ìˆ˜ë©´", "íœ´ì‹"])
            
            # ê³„ì ˆë³„ í‚¤ì›Œë“œ
            if season == "ê²¨ìš¸":
                fallback_keywords.extend(["ë”°ëœ»í•œ", "ë³´ì˜¨", "ë‚œë°©"])
            elif season == "ì—¬ë¦„":
                fallback_keywords.extend(["ì‹œì›í•œ", "ëƒ‰ë°©", "íœ´ê°€"])
            elif season == "ë´„":
                fallback_keywords.extend(["ì‹ ì„ í•œ", "ì•¼ì™¸", "ê½ƒ"])
            else:
                fallback_keywords.extend(["ê°€ì„", "ê±´ê°•", "í™˜ì ˆê¸°"])
            
            print(f"[1ë‹¨ê³„ - í´ë°±] í´ë°± í‚¤ì›Œë“œ ì‚¬ìš©: {fallback_keywords}")
            logger.info(f"[1ë‹¨ê³„] í´ë°± í‚¤ì›Œë“œ ì‚¬ìš©: {fallback_keywords}")
            logger.info(f"[1ë‹¨ê³„] í´ë°± í‚¤ì›Œë“œ ê°œìˆ˜: {len(fallback_keywords)}")
            return fallback_keywords
    
    async def _generate_context_keywords(self, context: Dict[str, Any]) -> List[str]:
        """í†µí•© í‚¤ì›Œë“œ ìƒì„±: 1ë‹¨ê³„(ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸) + 2ë‹¨ê³„(ì‹¤ì‹œê°„ íŠ¸ë Œë“œ)"""
        
        print("=" * 80)
        print("[í†µí•© í‚¤ì›Œë“œ ìƒì„±] 1ë‹¨ê³„: ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ")
        print("=" * 80)
        
        # 1ë‹¨ê³„: ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ (ë‚ ì”¨, ì‹œê°„ëŒ€, ê³„ì ˆ, ê³µíœ´ì¼)
        base_keywords = await self._generate_base_context_keywords(context)
        logger.info(f"1ë‹¨ê³„ ê¸°ë³¸ í‚¤ì›Œë“œ: {base_keywords}")
        
        print("=" * 80)
        print("[í†µí•© í‚¤ì›Œë“œ ìƒì„±] 2ë‹¨ê³„: ì‹¤ì‹œê°„ íŠ¸ë Œë“œ í‚¤ì›Œë“œ")
        print("=" * 80)
        
        # 2ë‹¨ê³„: ì‹¤ì‹œê°„ íŠ¸ë Œë“œ í‚¤ì›Œë“œ (OpenAI Web Search)
        realtime_keywords = await self._get_realtime_trend_keywords()
        logger.info(f"2ë‹¨ê³„ ì‹¤ì‹œê°„ íŠ¸ë Œë“œ: {realtime_keywords}")
        
        # í†µí•©: ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ìš°ì„  + ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸
        combined_keywords = realtime_keywords + base_keywords
        
        # ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€)
        unique_keywords = list(dict.fromkeys(combined_keywords))
        
        print("=" * 80)
        print(f"[í†µí•© í‚¤ì›Œë“œ] ìµœì¢… {len(unique_keywords)}ê°œ: {unique_keywords}")
        print("=" * 80)
        logger.info(f"í†µí•© í‚¤ì›Œë“œ ìƒì„± ì™„ë£Œ: {unique_keywords}")
        
        return unique_keywords
    
    async def _generate_unified_candidates(
        self,
        search_result: Dict[str, Any],
        context: Dict[str, Any],
        max_trend_match: int = 3,  # ìœ ì‚¬ë„ ê¸°ë°˜ ìµœëŒ€ ê°œìˆ˜
        max_sales_prediction: int = 10  # ë§¤ì¶œì˜ˆì¸¡ ê¸°ë°˜ ìµœëŒ€ ê°œìˆ˜
    ) -> tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """í†µí•© í›„ë³´êµ° ìƒì„± - ëª¨ë“  ìƒí’ˆ XGBoost ì˜ˆì¸¡ í›„ ê°€ì¤‘ì¹˜ ì¡°ì •"""
        
        candidates = []
        seen_products = set()
        
        print(f"=== [DEBUG Unified Candidates] í›„ë³´êµ° ìƒì„± ì‹œì‘ (ëª©í‘œ: ìµœëŒ€ {max_trend_match + max_sales_prediction}ê°œ) ===")
        
        # 1. ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ë¥¼ í•˜ë‚˜ë¡œ í†µí•©
        all_products = []
        all_products.extend(search_result["direct_products"])  # ê³ ìœ ì‚¬ë„ ìƒí’ˆ
        
        # ì¹´í…Œê³ ë¦¬ ê·¸ë£¹ì˜ ëª¨ë“  ìƒí’ˆë„ ì¶”ê°€
        for category, products in search_result["category_groups"].items():
            all_products.extend(products)
        
        print(f"=== [DEBUG] í†µí•©ëœ ìƒí’ˆ ìˆ˜: {len(all_products)}ê°œ ===")
        
        # 2. ì¤‘ë³µ ì œê±° (ìƒí’ˆì½”ë“œ + ì†Œë¶„ë¥˜ + ë¸Œëœë“œ)
        unique_products = {}
        seen_category_brand_pairs = set()  # (ì†Œë¶„ë¥˜, ë¸Œëœë“œ) ì¡°í•©
        
        for product in all_products:
            product_code = product.get("product_code")
            category_sub = product.get("category_sub", "")
            brand = product.get("brand", "")
            
            # ìƒí’ˆì½”ë“œ ì¤‘ë³µ ì²´í¬
            if product_code in unique_products:
                continue
            
            # ì†Œë¶„ë¥˜ + ë¸Œëœë“œ ì¡°í•© ì¤‘ë³µ ì²´í¬ (ë‹¤ì–‘ì„± ë³´ì¥)
            category_brand_key = (category_sub, brand)
            if category_sub and brand and category_brand_key in seen_category_brand_pairs:
                logger.info(f"ì†Œë¶„ë¥˜+ë¸Œëœë“œ ì¤‘ë³µ ì œì™¸: {product.get('product_name', '')[:30]} (ì†Œë¶„ë¥˜: {category_sub}, ë¸Œëœë“œ: {brand})")
                continue
            
            # í†µê³¼í•œ ê²½ìš° ì¶”ê°€
            unique_products[product_code] = product
            if category_sub and brand:
                seen_category_brand_pairs.add(category_brand_key)
        
        print(f"=== [DEBUG] ì¤‘ë³µ ì œê±° í›„: {len(unique_products)}ê°œ (ì†Œë¶„ë¥˜+ë¸Œëœë“œ ë‹¤ì–‘ì„± ë³´ì¥) ===")
        
        # 3. ë°°ì¹˜ ì˜ˆì¸¡ ì¤€ë¹„ (ìƒìœ„ 30ê°œë§Œ)
        products_list = list(unique_products.values())[:30]
        print(f"=== [DEBUG] ë°°ì¹˜ ì˜ˆì¸¡ ëŒ€ìƒ: {len(products_list)}ê°œ ===")
        
        # 4. ë°°ì¹˜ XGBoost ì˜ˆì¸¡ (í•œ ë²ˆì— ì²˜ë¦¬)
        predicted_sales_list = await self._predict_products_sales_batch(products_list, context)
        
        # 5. ì˜ˆì¸¡ ê²°ê³¼ì™€ ìƒí’ˆ ë§¤ì¹­ + ì ìˆ˜ ê³„ì‚°
        for i, product in enumerate(products_list):
            similarity = product.get("similarity_score", 0.5)
            predicted_sales = predicted_sales_list[i]
            
            # ì ìˆ˜ ê³„ì‚° (ìœ ì‚¬ë„ vs ë§¤ì¶œ ê°€ì¤‘ì¹˜ ì¡°ì •)
            if similarity >= 0.7:
                # ê³ ìœ ì‚¬ë„: ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜ ë†’ì„
                final_score = (
                    similarity * 0.7 +  # ìœ ì‚¬ë„ 70%
                    (predicted_sales / 100000000) * 0.3  # ë§¤ì¶œ 30% (ì •ê·œí™”: 1ì–µ ê¸°ì¤€)
                )
                source = "trend_match"
                print(f"  [ê³ ìœ ì‚¬ë„] {product.get('product_name')[:20]}: ìœ ì‚¬ë„={similarity:.2f}, ë§¤ì¶œ={predicted_sales/10000:.0f}ë§Œì›, ì ìˆ˜={final_score:.3f}")
            else:
                # ì €ìœ ì‚¬ë„: ë§¤ì¶œ ê°€ì¤‘ì¹˜ ë†’ì„
                final_score = (
                    similarity * 0.3 +  # ìœ ì‚¬ë„ 30%
                    (predicted_sales / 100000000) * 0.7  # ë§¤ì¶œ 70%
                )
                source = "sales_prediction"
                print(f"  [ì €ìœ ì‚¬ë„] {product.get('product_name')[:20]}: ìœ ì‚¬ë„={similarity:.2f}, ë§¤ì¶œ={predicted_sales/10000:.0f}ë§Œì›, ì ìˆ˜={final_score:.3f}")
            
            candidates.append({
                "product": product,
                "source": source,
                "similarity_score": similarity,
                "predicted_sales": predicted_sales,
                "final_score": final_score
            })
        
        # 4. ì ìˆ˜ìˆœ ì •ë ¬
        candidates.sort(key=lambda x: x["final_score"], reverse=True)
        
        print(f"=== [DEBUG] ì´ {len(candidates)}ê°œ í›„ë³´ ìƒì„± ì™„ë£Œ, ì ìˆ˜ìˆœ ì •ë ¬ë¨ ===")
        
        # 5. ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚° (ë‚´ë¶€ ì‚¬ìš©ìš©)
        category_scores = {}
        category_sales = {}
        for candidate in candidates:
            category = candidate["product"].get("category_main", "ê¸°íƒ€")
            if category == "ê¸°íƒ€" or not category:
                continue
            if category not in category_sales:
                category_sales[category] = []
            category_sales[category].append(candidate["predicted_sales"])
        
        for category, sales_list in category_sales.items():
            avg_sales = sum(sales_list) / len(sales_list)
            category_scores[category] = {"predicted_sales": avg_sales}
        
        return candidates, category_scores
    
    async def _predict_categories_with_xgboost(
        self, 
        category_groups: Dict[str, List[Dict]], 
        context: Dict[str, Any]
    ) -> Dict[str, Dict[str, float]]:
        """ì¹´í…Œê³ ë¦¬ë³„ XGBoost ë§¤ì¶œ ì˜ˆì¸¡"""
        
        category_scores = {}
        broadcast_dt = context["broadcast_dt"]
        
        for category, products in category_groups.items():
            if not products:
                continue
            
            try:
                # ëŒ€í‘œ ìƒí’ˆìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ë§¤ì¶œ ì˜ˆì¸¡
                representative_product = products[0]
                predicted_sales = await self._predict_product_sales(representative_product, context)
                
                # ì¹´í…Œê³ ë¦¬ ë‚´ ìƒí’ˆ ìˆ˜ë¡œ ë³´ì •
                adjusted_sales = predicted_sales * min(len(products) / 5, 2.0)
                
                category_scores[category] = {
                    "predicted_sales": adjusted_sales,
                    "product_count": len(products),
                    "avg_similarity": sum(p.get("similarity_score", 0) for p in products) / len(products)
                }
                
                print(f"  - ì¹´í…Œê³ ë¦¬ '{category}': {int(adjusted_sales/10000)}ë§Œì› (ìƒí’ˆ: {len(products)}ê°œ)")
                
            except Exception as e:
                logger.error(f"ì¹´í…Œê³ ë¦¬ '{category}' ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                category_scores[category] = {
                    "predicted_sales": 10000000,  # ê¸°ë³¸ê°’ 1000ë§Œì›
                    "product_count": len(products),
                    "avg_similarity": 0.4
                }
        
        return category_scores
    
    async def _generate_candidates(self, promising_categories: List[Any], trend_products: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """í›„ë³´êµ° ìƒì„± ë° í†µí•© (ë ˆê±°ì‹œ, ì‚¬ìš© ì•ˆ í•¨)"""
        candidates = []
        
        # ìœ ë§ ì¹´í…Œê³ ë¦¬ì—ì„œ ì—ì´ìŠ¤ ìƒí’ˆ ì„ ë°œ
        for category in promising_categories[:3]:
            ace_products = await self._get_ace_products_from_category(category.name, 5)
            
            for product in ace_products:
                candidates.append({
                    "product": product,
                    "source": "category",
                    "base_score": product.get("predicted_sales_score", 0.5),
                    "trend_boost": 1.0
                })
        
        return candidates
    
    async def _rank_final_candidates(self, candidates: List[Dict[str, Any]], category_scores: Dict[str, Any], context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ìµœì¢… ë­í‚¹ ê³„ì‚° - ì´ë¯¸ ì •ë ¬ëœ candidates ë°˜í™˜ (XGBoost ì˜ˆì¸¡ ì™„ë£Œë¨)"""
        
        print(f"=== [DEBUG _rank_final_candidates] ì´ë¯¸ ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬ëœ {len(candidates)}ê°œ í›„ë³´ ìˆ˜ì‹  ===")
        
        # ì´ë¯¸ _generate_unified_candidatesì—ì„œ final_score ê³„ì‚° ë° ì •ë ¬ ì™„ë£Œ
        # ì—¬ê¸°ì„œëŠ” ì¶”ê°€ ì²˜ë¦¬ ì—†ì´ ê·¸ëŒ€ë¡œ ë°˜í™˜
        
        for i, candidate in enumerate(candidates[:5]):
            print(f"  {i+1}ìœ„: {candidate['product'].get('product_name')[:25]} (ì ìˆ˜: {candidate['final_score']:.3f}, íƒ€ì…: {candidate['source']})")
        
        return candidates
    
    def _calculate_competition_penalty(self, product: Dict[str, Any], all_candidates: List[Dict[str, Any]]) -> float:
        """ê²½ìŸ í˜ë„í‹° ì ìˆ˜ ê³„ì‚°"""
        category = product.get("category_main", "")
        same_category_count = sum(1 for c in all_candidates if c["product"].get("category_main") == category)
        
        # ê°™ì€ ì¹´í…Œê³ ë¦¬ ìƒí’ˆì´ ë§ì„ìˆ˜ë¡ í˜ë„í‹°
        if same_category_count <= 2:
            return 0.0
        elif same_category_count <= 4:
            return 0.1
        else:
            return 0.2
    
    async def _format_response(self, ranked_products: List[Dict[str, Any]], context: Dict[str, Any] = None) -> BroadcastResponse:
        """API ì‘ë‹µ ìƒì„± (ë¹„ë™ê¸°)"""
        print(f"=== [DEBUG _format_response] context keys: {context.keys() if context else 'None'} ===")
        if context:
            print(f"=== [DEBUG _format_response] generated_keywords: {context.get('generated_keywords', [])} ===")
        
        # 1. í…Œì´í”„ ì½”ë“œ ëª©ë¡ ì¶”ì¶œ
        tape_codes = [p["product"].get("tape_code") for p in ranked_products if p["product"].get("tape_code")]
        
        # 2. ìµœê·¼ ë°©ì†¡ ì‹¤ì  ë°°ì¹˜ ì¡°íšŒ (Netezza)
        broadcast_history_map = {}
        if tape_codes:
            logger.info(f" {len(tape_codes)}ê°œ í…Œì´í”„ì˜ ìµœê·¼ ë°©ì†¡ ì‹¤ì  ì¡°íšŒ ì¤‘...")
            broadcast_history_map = self.broadcast_history_service.get_latest_broadcasts_batch(tape_codes)
            logger.info(f" {sum(1 for v in broadcast_history_map.values() if v is not None)}ê°œ í…Œì´í”„ì˜ ì‹¤ì  ì¡°íšŒ ì„±ê³µ")
        
        recommendations = []
        
        for i, candidate in enumerate(ranked_products):
            product = candidate["product"]
            
            # ìˆœìœ„ ì •ë³´ ì¶”ê°€
            candidate["rank"] = i + 1
            candidate["total_count"] = len(ranked_products)
            
            # LangChain ê¸°ë°˜ ë™ì  ê·¼ê±° ìƒì„± (ë¹„ë™ê¸°)
            reasoning_summary = await self._generate_dynamic_reason_with_langchain(
                candidate, 
                context or {"time_slot": "ì €ë…", "weather": {"weather": "í­ì—¼"}}
            )
            
            # ìµœê·¼ ë°©ì†¡ ì‹¤ì  ì¡°íšŒ
            tape_code = product.get("tape_code")
            last_broadcast_data = broadcast_history_map.get(tape_code) if tape_code else None
            last_broadcast = None
            
            if last_broadcast_data:
                try:
                    last_broadcast = LastBroadcastMetrics(**last_broadcast_data)
                    logger.debug(f"âœ… í…Œì´í”„ {tape_code}ì˜ ìµœê·¼ ë°©ì†¡ ì‹¤ì  ì¶”ê°€")
                except Exception as e:
                    logger.warning(f"âš ï¸ í…Œì´í”„ {tape_code}ì˜ ì‹¤ì  ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {e}")
            
            recommendation = BroadcastRecommendation(
                rank=i+1,
                productInfo=ProductInfo(
                    productId=product.get("product_code", "Unknown"),
                    productName=product.get("product_name", "Unknown"),
                    category=product.get("category_main", "Unknown"),
                    brand=product.get("brand"),
                    price=product.get("price"),
                    tapeCode=product.get("tape_code"),
                    tapeName=product.get("tape_name")
                ),
                reasoning=Reasoning(
                    summary=reasoning_summary
                ),
                businessMetrics=BusinessMetrics(
                    aiPredictedSales=f"{round(candidate['predicted_sales']/10000, 1):,.1f}ë§Œì›",  # AI ì˜ˆì¸¡ ë§¤ì¶œ (XGBoost, ì†Œìˆ˜ì  1ìë¦¬)
                    lastBroadcast=last_broadcast  # ìµœê·¼ ë°©ì†¡ ì‹¤ì  ì¶”ê°€
                )
            )
            recommendations.append(recommendation)
        
        # ë„¤ì´ë²„ ë² ìŠ¤íŠ¸ ìƒí’ˆ ì¡°íšŒ - ì…ë ¥ íŒŒë¼ë¯¸í„°ì™€ ë¬´ê´€í•˜ê²Œ í•­ìƒ TOP 10
        naver_products_data = self.external_products_service.get_latest_best_products(limit=10)
        naver_products = [NaverProduct(**product) for product in naver_products_data]
        
        logger.info(f"âœ… ë„¤ì´ë²„ ìƒí’ˆ {len(naver_products)}ê°œ ì¶”ê°€")
        
        # íƒ€ í™ˆì‡¼í•‘ì‚¬ í¸ì„± ìƒí’ˆ ì¡°íšŒ - Netezzaì—ì„œ ì‹¤ì‹œê°„ ì¡°íšŒ
        print("=== [DEBUG] íƒ€ì‚¬ í¸ì„± ì¡°íšŒ ì‹œì‘ ===")
        try:
            # contextì—ì„œ broadcast_time ê°€ì ¸ì˜¤ê¸°
            broadcast_time_str = context.get("broadcast_time") if context else None
            print(f"=== [DEBUG] broadcast_time_str: {broadcast_time_str} ===")
            if broadcast_time_str:
                print(f"=== [DEBUG] Netezza ì¿¼ë¦¬ ì‹¤í–‰ ì¤‘... ===")
                competitor_data = await netezza_conn.get_competitor_schedules(broadcast_time_str, limit=10)
                print(f"=== [DEBUG] Netezza ì‘ë‹µ: {len(competitor_data)}ê°œ ===")
                competitor_products = [CompetitorProduct(**comp) for comp in competitor_data]
                logger.info(f"âœ… íƒ€ì‚¬ í¸ì„± {len(competitor_products)}ê°œ ì¶”ê°€")
                print(f"âœ… íƒ€ì‚¬ í¸ì„± {len(competitor_products)}ê°œ ì¶”ê°€")
            else:
                logger.warning(f"âš ï¸ broadcast_timeì´ contextì— ì—†ìŒ")
                print(f"âš ï¸ broadcast_timeì´ contextì— ì—†ìŒ")
                competitor_products = []
        except Exception as e:
            logger.warning(f"âš ï¸ íƒ€ì‚¬ í¸ì„± ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            print(f"âš ï¸ íƒ€ì‚¬ í¸ì„± ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            import traceback
            traceback.print_exc()
            competitor_products = []
        
        return BroadcastResponse(
            requestTime="",  # ë©”ì¸ì—ì„œ ì„¤ì •
            recommendations=recommendations,
            naverProducts=naver_products if naver_products else None,
            competitorProducts=competitor_products if competitor_products else None
        )
    
    def _generate_recommendation_reason(self, candidate: Dict[str, Any], context: Dict[str, Any] = None) -> str:
        """ê°œì„ ëœ ì¶”ì²œ ê·¼ê±° ìƒì„±"""
        product = candidate["product"]
        source = candidate["source"]
        trend_boost = candidate.get("trend_boost", 1.0)
        predicted_sales = candidate.get("predicted_sales", 0)
        final_score = candidate.get("final_score", 0)
        
        # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        category = product.get("category_main", "")
        product_name = product.get("product_name", "")
        trend_keyword = candidate.get("trend_keyword", "")
        tape_name = product.get("tape_name", "")
        
        # ì‹œê°„ëŒ€ ì •ë³´
        time_slot = context.get("time_slot", "") if context else ""
        weather = context.get("weather", {}).get("weather", "") if context else ""
        
        # ê·¼ê±° êµ¬ì„± ìš”ì†Œë“¤
        reasons = []
        
        # 1. íŠ¸ë Œë“œ ê´€ë ¨ ê·¼ê±°
        if source == "trend" and trend_keyword:
            if trend_boost > 1.3:
                reasons.append(f"'{trend_keyword}' íŠ¸ë Œë“œ ê¸‰ìƒìŠ¹ ë°˜ì˜")
            elif trend_boost > 1.1:
                reasons.append(f"'{trend_keyword}' íŠ¸ë Œë“œ ìƒìŠ¹ì„¸")
            else:
                reasons.append(f"'{trend_keyword}' í‚¤ì›Œë“œ ì—°ê´€ì„±")
        
        # 2. ì¹´í…Œê³ ë¦¬ ê´€ë ¨ ê·¼ê±°
        elif source == "category":
            reasons.append(f"{category} ì¹´í…Œê³ ë¦¬ ìœ ë§ ìƒí’ˆ")
        
        # 3. ë§¤ì¶œ ì˜ˆì¸¡ ê·¼ê±°
        if predicted_sales > 80000000:  # 8ì²œë§Œì› ì´ìƒ
            reasons.append("ë†’ì€ ë§¤ì¶œ ì˜ˆì¸¡")
        elif predicted_sales > 50000000:  # 5ì²œë§Œì› ì´ìƒ
            reasons.append("ì•ˆì •ì  ë§¤ì¶œ ì˜ˆì¸¡")
        
        # 4. ì‹œê°„ëŒ€ ì í•©ì„±
        if time_slot and weather:
            if time_slot == "ì €ë…" and category in ["ê±´ê°•ì‹í’ˆ", "í™”ì¥í’ˆ"]:
                reasons.append("ì €ë… ì‹œê°„ëŒ€ ìµœì ")
            elif time_slot == "ì˜¤í›„" and category in ["ê°€ì „ì œí’ˆ", "ìƒí™œìš©í’ˆ"]:
                reasons.append("ì˜¤í›„ ì‹œê°„ëŒ€ ì í•©")
            elif weather == "í­ì—¼" and category in ["ê°€ì „ì œí’ˆ"] and "ì„ í’ê¸°" in product_name:
                reasons.append("í­ì—¼ ë‚ ì”¨ ìµœì  ìƒí’ˆ")
        
        # 5. ë°©ì†¡í…Œì´í”„ ì •ë³´
        if tape_name:
            reasons.append("ë°©ì†¡í…Œì´í”„ ì¤€ë¹„ ì™„ë£Œ")
        
        # 6. AI ì‹ ë¢°ë„
        if final_score > 0.8:
            reasons.append("AI ë†’ì€ ì‹ ë¢°ë„")
        elif final_score > 0.6:
            reasons.append("AI ì¶”ì²œ ì í•©")
        
        # ê·¼ê±°ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë©”ì‹œì§€
        if not reasons:
            reasons.append("ì¢…í•© ë¶„ì„ ê²°ê³¼ ì¶”ì²œ")
        
        # ìµœëŒ€ 3ê°œ ê·¼ê±°ë§Œ ì‚¬ìš©
        return " + ".join(reasons[:3])
    
    def _generate_diverse_reason_templates(self, candidate: Dict[str, Any], context: Dict[str, Any] = None) -> List[str]:
        """ë‹¤ì–‘í•œ ì¶”ì²œ ê·¼ê±° í…œí”Œë¦¿ ìƒì„±"""
        product = candidate["product"]
        source = candidate["source"]
        trend_boost = candidate.get("trend_boost", 1.0)
        predicted_sales = candidate.get("predicted_sales", 0)
        
        # ê¸°ë³¸ ì •ë³´
        category = product.get("category_main", "")
        product_name = product.get("product_name", "")
        trend_keyword = candidate.get("trend_keyword", "")
        
        templates = []
        
        # íŠ¸ë Œë“œ ê¸°ë°˜ í…œí”Œë¦¿ë“¤
        if source == "trend" and trend_keyword:
            trend_templates = [
                f"'{trend_keyword}' ê²€ìƒ‰ëŸ‰ ê¸‰ì¦ìœ¼ë¡œ ë†’ì€ ê´€ì‹¬ë„ ì˜ˆìƒ",
                f"ì‹¤ì‹œê°„ '{trend_keyword}' íŠ¸ë Œë“œ ë°˜ì˜í•œ íƒ€ì´ë° ìƒí’ˆ",
                f"'{trend_keyword}' í‚¤ì›Œë“œ ì—°ê´€ ìƒí’ˆìœ¼ë¡œ ì‹œì²­ì ê´€ì‹¬ ì§‘ì¤‘",
                f"íŠ¸ë Œë“œ '{trend_keyword}'ì™€ ì™„ë²½ ë§¤ì¹­ë˜ëŠ” ìµœì  ìƒí’ˆ",
                f"'{trend_keyword}' í™”ì œì„± í™œìš©í•œ ì‹œì˜ì ì ˆí•œ í¸ì„±"
            ]
            templates.extend(trend_templates)
        
        # ë§¤ì¶œ ì˜ˆì¸¡ ê¸°ë°˜ í…œí”Œë¦¿ë“¤
        sales_million = int(predicted_sales / 1000000)
        if sales_million > 80:
            sales_templates = [
                f"AI ì˜ˆì¸¡ ë§¤ì¶œ {sales_million}ë°±ë§Œì›ìœ¼ë¡œ ìµœê³  ìˆ˜ìµ ê¸°ëŒ€",
                f"ê³¼ê±° ë°ì´í„° ë¶„ì„ ê²°ê³¼ {sales_million}ë°±ë§Œì› ë§¤ì¶œ ì˜ˆìƒ",
                f"ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì˜ˆì¸¡ {sales_million}ë°±ë§Œì› ê³ ìˆ˜ìµ ìƒí’ˆ"
            ]
        elif sales_million > 50:
            sales_templates = [
                f"ì•ˆì •ì  {sales_million}ë°±ë§Œì› ë§¤ì¶œ ì˜ˆì¸¡ìœ¼ë¡œ ë¦¬ìŠ¤í¬ ìµœì†Œí™”",
                f"ê²€ì¦ëœ {sales_million}ë°±ë§Œì› ìˆ˜ìµ ëª¨ë¸ ìƒí’ˆ",
                f"ì˜ˆì¸¡ ë§¤ì¶œ {sales_million}ë°±ë§Œì›ìœ¼ë¡œ ì•ˆì „í•œ í¸ì„± ì„ íƒ"
            ]
        else:
            sales_templates = [
                "ë°ì´í„° ê¸°ë°˜ ë§¤ì¶œ ì˜ˆì¸¡ìœ¼ë¡œ ê²€ì¦ëœ ìƒí’ˆ",
                "AI ë¶„ì„ ê²°ê³¼ ìˆ˜ìµì„± í™•ì¸ëœ ì¶”ì²œ ìƒí’ˆ",
                "ê³¼ê±° ì„±ê³¼ ë°ì´í„° ê¸°ë°˜ ì„ ë³„ëœ ìƒí’ˆ"
            ]
        templates.extend(sales_templates)
        
        # ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ í…œí”Œë¦¿ë“¤
        category_templates = [
            f"{category} ë¶„ì•¼ ë² ìŠ¤íŠ¸ì…€ëŸ¬ ê²€ì¦ ìƒí’ˆ",
            f"{category} ì¹´í…Œê³ ë¦¬ ë‚´ ê²½ìŸë ¥ 1ìœ„ ìƒí’ˆ",
            f"{category} ì‹œì¥ì—ì„œ ì…ì¦ëœ ì¸ê¸° ìƒí’ˆ",
            f"{category} ì „ë¬¸ ìƒí’ˆìœ¼ë¡œ íƒ€ê²Ÿ ì‹œì²­ì í™•ë³´",
            f"{category} ë¶„ì•¼ í”„ë¦¬ë¯¸ì—„ ë¸Œëœë“œ ìƒí’ˆ"
        ]
        templates.extend(category_templates)
        
        # ë‚ ì”¨ ê¸°ë°˜ í…œí”Œë¦¿ (ì„ íƒì , AIê°€ íŒë‹¨ ëª»í•  ë•Œë§Œ ì‚¬ìš©)
        if context:
            weather = context.get("weather", {}).get("weather", "")
            
            # ê·¹ë‹¨ì  ë‚ ì”¨ë§Œ í…œí”Œë¦¿ ì œê³µ (AI í´ë°±ìš©)
            if weather in ["í­ì—¼", "í•œíŒŒ", "í­ìš°", "í­ì„¤"]:
                weather_templates = [
                    f"{weather} íŠ¹ìˆ˜ ìƒí™© ëŒ€ì‘ ìƒí’ˆ",
                    f"í˜„ì¬ {weather} ìƒí™©ì— í•„ìš”í•œ ì•„ì´í…œ"
                ]
                templates.extend(weather_templates)
        
        # ë°©ì†¡í…Œì´í”„ ê¸°ë°˜ í…œí”Œë¦¿ë“¤
        tape_name = product.get("tape_name", "")
        if tape_name:
            tape_templates = [
                f"ì „ìš© ë°©ì†¡í…Œì´í”„ '{tape_name}' ì™„ë²½ ì¤€ë¹„ ì™„ë£Œ",
                f"ê²€ì¦ëœ ë°©ì†¡ ì½˜í…ì¸ ë¡œ ì‹œì²­ì ëª°ì…ë„ ê·¹ëŒ€í™”",
                f"ì „ë¬¸ ì œì‘ ë°©ì†¡í…Œì´í”„ë¡œ ìƒí’ˆ ë§¤ë ¥ ì™„ë²½ ì „ë‹¬"
            ]
            templates.extend(tape_templates)
        
        return templates
    
    async def _generate_fallback_response(self, request_time: str, recommendation_count: int) -> BroadcastResponse:
        """API í• ë‹¹ëŸ‰ ì†Œì§„ ì‹œ ì„ì‹œ ë°ì´í„°ë¡œ ì¶”ì²œ ê·¼ê±° ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
        
        # ì„ì‹œ ìƒí’ˆ ë°ì´í„° (ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ìƒí’ˆë“¤)
        mock_products = [
            {
                "product_code": "P001",
                "product_name": "í”„ë¦¬ë¯¸ì—„ ë‹¤ì´ì–´íŠ¸ ë³´ì¡°ì œ",
                "category_main": "ê±´ê°•ì‹í’ˆ",
                "tape_code": "T001",
                "tape_name": "í”„ë¦¬ë¯¸ì—„ ë‹¤ì´ì–´íŠ¸ ë³´ì¡°ì œ"
            },
            {
                "product_code": "P002", 
                "product_name": "í™ˆíŠ¸ë ˆì´ë‹ ì„¸íŠ¸",
                "category_main": "ìŠ¤í¬ì¸ ìš©í’ˆ",
                "tape_code": "T002",
                "tape_name": "í™ˆíŠ¸ë ˆì´ë‹ ì„¸íŠ¸ ì™„ì „ì •ë³µ"
            },
            {
                "product_code": "P005",
                "product_name": "ì‹œì›í•œ ì—¬ë¦„ ì„ í’ê¸°",
                "category_main": "ê°€ì „ì œí’ˆ",
                "tape_code": "T005",
                "tape_name": "ì‹œì›í•œ ì—¬ë¦„ë‚˜ê¸° ì„ í’ê¸°"
            }
        ]
        
        # ì„ì‹œ í›„ë³´ ë°ì´í„° ìƒì„±
        mock_candidates = []
        for i, product in enumerate(mock_products[:recommendation_count]):
            candidate = {
                "product": product,
                "source": "trend" if i == 0 else "category",
                "base_score": 0.8 - i * 0.1,
                "trend_boost": 1.3 if i == 0 else 1.0,
                "predicted_sales": 85000000 - i * 15000000,
                "final_score": 0.85 - i * 0.1,
                "trend_keyword": "ë‹¤ì´ì–´íŠ¸" if i == 0 else ""
            }
            mock_candidates.append(candidate)
        
        # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context = {
            "time_slot": "ì €ë…",
            "weather": {"weather": "í­ì—¼"},
            "competitors": []
        }
        
        # ê°œì„ ëœ ì¶”ì²œ ê·¼ê±° ì‹œìŠ¤í…œìœ¼ë¡œ ì‘ë‹µ ìƒì„±
        response = await self._format_response(mock_candidates, context)
        response.requestTime = request_time
        
        logger.info(f"í´ë°± ì‘ë‹µ ìƒì„± ì™„ë£Œ: {len(mock_candidates)}ê°œ ì¶”ì²œ (ì¶”ì²œ ê·¼ê±° ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸)")
        return response
    
    async def _generate_dynamic_reason_with_langchain(self, candidate: Dict[str, Any], context: Dict[str, Any] = None) -> str:
        """LangChainì„ í™œìš©í•œ ë™ì  ì¶”ì²œ ê·¼ê±° ìƒì„±"""
        try:
            product = candidate["product"]
            source = candidate["source"]
            predicted_sales = candidate.get("predicted_sales", 0)
            similarity_score = candidate.get("similarity_score", 0)
            final_score = candidate.get("final_score", 0)
            rank = candidate.get("rank", 0)
            
            # ìƒí’ˆ ì •ë³´
            category = product.get("category_main", "")
            product_name = product.get("product_name", "")
            trend_keyword = candidate.get("trend_keyword", "")
            
            # ì»¨í…ìŠ¤íŠ¸ ì •ë³´
            time_slot = context.get("time_slot", "") if context else ""
            weather = context.get("weather", {}).get("weather", "") if context else ""
            holiday_name = context.get("holiday_name") if context else None
            competitors = context.get("competitors", []) if context else []
            
            # ê²½ìŸ ìƒí™© ë¶„ì„
            competitor_categories = [comp.get("category_main", "") for comp in competitors]
            has_competition = category in competitor_categories
            
            # ì ìˆ˜ ë¶„ì„ (ì‹¤ì œ ê°€ì¤‘ì¹˜ ê¸°ë°˜)
            if similarity_score >= 0.7:
                # ê³ ìœ ì‚¬ë„: ìœ ì‚¬ë„ 70%, ë§¤ì¶œ 30%
                similarity_ratio = 0.7
                sales_ratio = 0.3
            else:
                # ì €ìœ ì‚¬ë„: ìœ ì‚¬ë„ 30%, ë§¤ì¶œ 70%
                similarity_ratio = 0.3
                sales_ratio = 0.7
            
            # í”„ë¡¬í”„íŠ¸ ë¡œê¹… (ëˆˆì— ë„ê²Œ)
            print("=" * 80)
            print("[LLM í”„ë¡¬í”„íŠ¸] ì¶”ì²œ ê·¼ê±° ìƒì„±")
            print("=" * 80)
            print(f"ìˆœìœ„: {rank}ìœ„ | ì¶”ì²œ íƒ€ì…: {source}")
            print(f"ìƒí’ˆ: {product_name}, ì¹´í…Œê³ ë¦¬: {category}")
            print(f"ìœ ì‚¬ë„: {similarity_score:.3f} | ë§¤ì¶œ: {int(predicted_sales/10000)}ë§Œì› | ìµœì¢…ì ìˆ˜: {final_score:.3f}")
            print(f"ì ìˆ˜ êµ¬ì„±: ìœ ì‚¬ë„ {similarity_ratio*100:.0f}% / ë§¤ì¶œ {sales_ratio*100:.0f}%")
            print(f"ì‹œê°„ëŒ€: {time_slot}, ë‚ ì”¨: {weather}, ê³µíœ´ì¼: {holiday_name or 'ì—†ìŒ'}")
            print("=" * 80)
            
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
            reason_prompt = ChatPromptTemplate.from_messages([
                ("system", """ë‹¹ì‹ ì€ í™ˆì‡¼í•‘ ë°©ì†¡ í¸ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê° ìƒí’ˆë§ˆë‹¤ ë…ì°½ì ì´ê³  ì„¤ë“ë ¥ ìˆëŠ” ì¶”ì²œ ê·¼ê±°ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

# í•µì‹¬ ì›ì¹™
1. **100ì ì´ë‚´** ê°„ê²°í•˜ê²Œ ì‘ì„±
2. ì „ë¬¸ì ì´ê³  ê°ê´€ì ì¸ í†¤ ìœ ì§€
3. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ë°ì´í„° í™œìš©
4. **ê° ìƒí’ˆë§ˆë‹¤ ì™„ì „íˆ ë‹¤ë¥¸ ê´€ì ê³¼ í‘œí˜„ ì‚¬ìš©**
5. ê°™ì€ íŒ¨í„´ì´ë‚˜ ë¬¸êµ¬ ë°˜ë³µ ì ˆëŒ€ ê¸ˆì§€

# í™œìš© ê°€ëŠ¥í•œ ìš”ì†Œë“¤
- ì˜ˆì¸¡ ë§¤ì¶œ ìˆ˜ì¹˜ (í•„ìˆ˜)
- ì¹´í…Œê³ ë¦¬ íŠ¹ì„± (í•„ìˆ˜)
- ì ìˆ˜ êµ¬ì„± ë¹„ìœ¨ (ìœ ì‚¬ë„ vs ë§¤ì¶œ)
- íŠ¸ë Œë“œ í‚¤ì›Œë“œ (ìˆì„ ê²½ìš°)
- ê³µíœ´ì¼ (ìˆì„ ê²½ìš° í•„ìˆ˜ ì–¸ê¸‰)
- ì‹œê°„ëŒ€ íŠ¹ì„± (ì €ë…/ì˜¤ì „/ì˜¤í›„) - **ì‹ ì¤‘í•˜ê²Œ íŒë‹¨**
  * ì´ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ê°€ í•´ë‹¹ ì‹œê°„ëŒ€ì— ì‹¤ì œë¡œ ì í•©í•œì§€ ìŠ¤ìŠ¤ë¡œ íŒë‹¨í•˜ì„¸ìš”
  * ì˜ˆ: ê±´ê°•ì‹í’ˆì€ ì•„ì¹¨/ì €ë… ì í•©, ì˜ë¥˜ëŠ” ë‚® ì‹œê°„ ì í•©, ê°€ì „ì€ ì €ë… ì í•©
  * í™•ì‹ ì´ ì—†ìœ¼ë©´ ì‹œê°„ëŒ€ ì–¸ê¸‰í•˜ì§€ ë§ê³  ë‹¤ë¥¸ ê·¼ê±° ì‚¬ìš©
- ë‚ ì”¨/ê³„ì ˆ (ì„ íƒì , ê³¼ë„í•œ ë°˜ë³µ ê¸ˆì§€)

# ê¸ˆì§€ ì‚¬í•­ (ë‹µë³€ì— ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ ê²ƒ)
- "AI ë¶„ì„ ê²°ê³¼"ë¡œ ì‹œì‘í•˜ì§€ ë§ˆì„¸ìš”
- í…œí”Œë¦¿ì²˜ëŸ¼ ë³´ì´ëŠ” ë°˜ë³µì  í‘œí˜„ ê¸ˆì§€
- ê³¼ì¥ëœ í‘œí˜„ (ëŒ€ë°•, ìµœê³ , ê°•ì¶” ë“±)
- ê°ì •ì  í‘œí˜„ (ê¸°ì˜ê²Œ, í–‰ë³µí•˜ê²Œ ë“±)
- **ê¸°ìˆ  ìš©ì–´ ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€**: 
  * "ìœ ì‚¬ë„", "ìœ ì‚¬ë„ ì ìˆ˜", "similarity"
  * "ë§¤ì¶œ ë¹„ì¤‘", "ì ìˆ˜ êµ¬ì„±", "70%", "30%", "ë¹„ìœ¨"
  * "ìµœì¢… ì ìˆ˜", "final score"
  * ì´ëŸ° ë‚´ë¶€ ì§€í‘œë“¤ì„ ì ˆëŒ€ ë‹µë³€ì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”

# ì°½ì˜ì  ì‘ì„± ê°€ì´ë“œ
- **ìƒí’ˆëª…ì˜ íŠ¹ì§•ì„ í™œìš©** (ë¸Œëœë“œ, ìˆ˜ëŸ‰, íŠ¹ìˆ˜ì„± ë“±)
- ë§¤ì¶œ ìˆ˜ì¹˜ë¥¼ ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ í‘œí˜„
- ì‹œê°„ëŒ€ë¥¼ ë‹¤ë¥´ê²Œ í‘œí˜„ (í™©ê¸ˆì‹œê°„ëŒ€, ì£¼ì‹œì²­ì‹œê°„ ë“±)
- ì¹´í…Œê³ ë¦¬ íŠ¹ì„±ì„ ì°½ì˜ì ìœ¼ë¡œ í™œìš©
- ì ìˆ˜ êµ¬ì„±ì— ë”°ë¼ ê°•ì¡°ì ì„ ë‹¤ë¥´ê²Œ
- **ê° ìƒí’ˆë§ˆë‹¤ ì™„ì „íˆ ë‹¤ë¥¸ ê°ë„ì—ì„œ ì ‘ê·¼**
- **ì ˆëŒ€ ì´ì „ ì‘ë‹µê³¼ ë¹„ìŠ·í•œ íŒ¨í„´ ì‚¬ìš© ê¸ˆì§€**"""),
    
    ("human", """
ìƒí’ˆëª…: {product_name}
ì¹´í…Œê³ ë¦¬: {category}
ì¶”ì²œ ìˆœìœ„: {rank}ìœ„
ì¶”ì²œ íƒ€ì…: {source}
ì˜ˆì¸¡ ë§¤ì¶œ: {predicted_sales}ë§Œì›
ìœ ì‚¬ë„ ì ìˆ˜: {similarity_score}
ìµœì¢… ì ìˆ˜: {final_score}
ì ìˆ˜ êµ¬ì„±: ìœ ì‚¬ë„ {similarity_ratio}% / ë§¤ì¶œ {sales_ratio}%
ì‹œê°„ëŒ€: {time_slot}
ë‚ ì”¨: {weather}
ê³µíœ´ì¼: {holiday_name}
íŠ¸ë Œë“œ í‚¤ì›Œë“œ: {trend_keyword}

ìœ„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì´ ìƒí’ˆë§Œì˜ ë…íŠ¹í•œ ì¶”ì²œ ê·¼ê±°ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

**ì¤‘ìš”:**
- ë‹¤ë¥¸ ìƒí’ˆë“¤ê³¼ ì™„ì „íˆ ë‹¤ë¥¸ ì‹œì‘ ë¬¸êµ¬ ì‚¬ìš©
- ê°™ì€ ë‹¨ì–´ë‚˜ í‘œí˜„ ë°˜ë³µ ê¸ˆì§€
- ê³µíœ´ì¼ì´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ì–¸ê¸‰
- ì ìˆ˜ êµ¬ì„± ë¹„ìœ¨ì— ë”°ë¼ ê°•ì¡°ì  ë‹¤ë¥´ê²Œ
- 100ì ì´ë‚´ë¡œ ì‘ì„±

ì¶”ì²œ ê·¼ê±°:""")
            ])
            
            chain = reason_prompt | self.llm
            
            result = await chain.ainvoke({
                "product_name": product_name,
                "category": category,
                "rank": rank,
                "source": source,  # "trend_match" ë˜ëŠ” "sales_prediction"
                "predicted_sales": int(predicted_sales/10000) if predicted_sales else "ì—†ìŒ",
                "similarity_score": f"{similarity_score:.3f}",
                "final_score": f"{final_score:.3f}",
                "similarity_ratio": f"{similarity_ratio*100:.0f}",
                "sales_ratio": f"{sales_ratio*100:.0f}",
                "time_slot": time_slot or "ë¯¸ì§€ì •",
                "weather": weather or "ë³´í†µ",
                "holiday_name": holiday_name if holiday_name else "ì—†ìŒ",
                "trend_keyword": trend_keyword or "ì—†ìŒ"
            })
            
            return result.content.strip()
            
        except Exception as e:
            logger.error(f"ë™ì  ê·¼ê±° ìƒì„± ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()  # ì—ëŸ¬ ìƒì„¸ ë¡œê·¸
            # í´ë°±: ê°„ë‹¨í•œ ê¸°ë³¸ ë©”ì‹œì§€ (í…œí”Œë¦¿ ì•„ë‹Œ)
            return f"{candidate['product'].get('category_main', 'ìƒí’ˆ')} ì¶”ì²œ"
    
    def _prepare_features_for_product(self, product: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """1ê°œ ìƒí’ˆì˜ XGBoost feature ì¤€ë¹„ (ì˜ˆì¸¡ì€ ì•ˆ í•¨)"""
        broadcast_dt = context["broadcast_dt"]
        
        print(f"=== [_prepare_features_for_product] í˜¸ì¶œë¨: {product.get('product_name', 'Unknown')[:30]} ===")
        
        # ë¡œê·¸ ìŠ¤ì¼€ì¼ë§ ì ìš© (í•™ìŠµ ì‹œì™€ ë™ì¼)
        product_price = product.get("product_price", product.get("price", 100000))
        product_price_log = np.log1p(product_price)
        
        category_main = product.get("category_main", product.get("category", "Unknown"))
        time_slot = context["time_slot"]
        
        return {
            # Numeric features (ë‹¨ìˆœí™”)
            "product_price_log": product_price_log,
            "hour": broadcast_dt.hour,
            "temperature": context["weather"].get("temperature", 20),
            "precipitation": context["weather"].get("precipitation", 0),
            
            # Categorical features
            "product_lgroup": category_main,
            "product_mgroup": product.get("category_middle", "Unknown"),
            "product_sgroup": product.get("category_sub", "Unknown"),
            "brand": product.get("brand", "Unknown"),
            "product_type": product.get("product_type", "ìœ í˜•"),
            "time_slot": time_slot,
            "day_of_week": ["ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ", "í† ", "ì¼"][broadcast_dt.weekday()],
            "season": context["season"],
            "weather": context["weather"].get("weather", "Clear"),
            
            # Boolean features
            "is_weekend": 1 if broadcast_dt.weekday() >= 5 else 0,
            "is_holiday": 0
        }
    
    async def _predict_product_sales(self, product: Dict[str, Any], context: Dict[str, Any]) -> float:
        """ê°œë³„ ìƒí’ˆ XGBoost ë§¤ì¶œ ì˜ˆì¸¡"""
        try:
            import pandas as pd
            
            # Feature ì¤€ë¹„
            features = self._prepare_features_for_product(product, context)
            product_data = pd.DataFrame([features])
            
            logger.info(f"=== XGBoost ë§¤ì¶œ ì˜ˆì¸¡ ì…ë ¥ ë°ì´í„° ===")
            logger.info(f"ìƒí’ˆ: {product.get('product_name', 'Unknown')}")
            logger.info(f"ì¹´í…Œê³ ë¦¬: {product.get('category_main', 'Unknown')}")
            logger.info(f"ê°€ê²©: {product.get('product_price', 100000):,}ì›")
            logger.info(f"ê³¼ê±° í‰ê·  ë§¤ì¶œ: {product.get('avg_sales', 30000000):,}ì›")
            logger.info(f"ë°©ì†¡ ì‹œê°„: {context['broadcast_dt'].hour}ì‹œ")
            logger.info(f"ë‚ ì”¨: {context['weather'].get('weather', 'Clear')}, {context['weather'].get('temperature', 20)}Â°C")
            
            # XGBoost íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì˜ˆì¸¡ (ì „ì²˜ë¦¬ í¬í•¨)
            predicted_sales_log = self.model.predict(product_data)[0]
            # ë¡œê·¸ ì—­ë³€í™˜ (í•™ìŠµ ì‹œ log1p ì‚¬ìš©)
            predicted_sales = np.expm1(predicted_sales_log)
            logger.info(f"=== XGBoost ì˜ˆì¸¡ ê²°ê³¼ ===")
            logger.info(f"ì˜ˆì¸¡ ë§¤ì¶œ: {predicted_sales:,.0f}ì› ({predicted_sales/100000000:.2f}ì–µ)")
            
            return float(predicted_sales)
            
        except Exception as e:
            logger.error(f"ìƒí’ˆ ë§¤ì¶œ ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
            logger.error(f"ìƒí’ˆ ì •ë³´: {product.get('product_name', 'Unknown')}")
            import traceback
            logger.error(f"ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
            return 30000000  # ê¸°ë³¸ê°’ (0.3ì–µ)
    
    async def _predict_products_sales_batch(self, products: List[Dict[str, Any]], context: Dict[str, Any]) -> List[float]:
        """ì—¬ëŸ¬ ìƒí’ˆ XGBoost ë§¤ì¶œ ì˜ˆì¸¡ (ë°°ì¹˜ ì²˜ë¦¬)"""
        try:
            import pandas as pd
            
            if not products:
                return []
            
            # ëª¨ë“  ìƒí’ˆì˜ featuresë¥¼ í•œ ë²ˆì— ì¤€ë¹„
            features_list = [
                self._prepare_features_for_product(product, context)
                for product in products
            ]
            
            batch_df = pd.DataFrame(features_list)
            
            print(f"=== [ë°°ì¹˜ ì˜ˆì¸¡] {len(products)}ê°œ ìƒí’ˆ ì¼ê´„ ì˜ˆì¸¡ ì‹œì‘ ===")
            
            # ì…ë ¥ í”¼ì²˜ ìƒ˜í”Œ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
            print(f"=== [ì…ë ¥ í”¼ì²˜ ìƒ˜í”Œ] ===")
            for i, (product, features) in enumerate(zip(products[:3], features_list[:3])):
                print(f"  ìƒí’ˆ {i+1}: {product.get('product_name', '')[:30]}")
                print(f"    - product_price_log: {features['product_price_log']:.2f}")
                print(f"    - hour: {features['hour']}")
                print(f"    - ì¹´í…Œê³ ë¦¬: {features['product_lgroup']}")
            
            # XGBoost ë°°ì¹˜ ì˜ˆì¸¡ (í•œ ë²ˆì— ì²˜ë¦¬)
            predicted_sales_log = self.model.predict(batch_df)
            # ë¡œê·¸ ì—­ë³€í™˜ (í•™ìŠµ ì‹œ log1p ì‚¬ìš©)
            predicted_sales_array = np.expm1(predicted_sales_log)
            
            print(f"=== [ë°°ì¹˜ ì˜ˆì¸¡] ì™„ë£Œ ===")
            print(f"  í‰ê· : {predicted_sales_array.mean()/10000:.0f}ë§Œì›")
            print(f"  ìµœì†Œ: {predicted_sales_array.min()/10000:.0f}ë§Œì›")
            print(f"  ìµœëŒ€: {predicted_sales_array.max()/10000:.0f}ë§Œì›")
            print(f"  í‘œì¤€í¸ì°¨: {predicted_sales_array.std()/10000:.0f}ë§Œì›")
            
            # ì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ ì¶œë ¥
            print(f"=== [ì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ] ===")
            for i, (product, sales) in enumerate(zip(products[:5], predicted_sales_array[:5])):
                print(f"  {i+1}. {product.get('product_name', '')[:30]:30s} â†’ {sales/10000:.0f}ë§Œì›")
            
            return [float(sales) for sales in predicted_sales_array]
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ë§¤ì¶œ ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(f"ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return [30000000.0] * len(products)
    
    async def _get_all_categories_from_db(self) -> List[str]:
        """PostgreSQLì—ì„œ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì¡°íšŒ"""
        try:
            query = text("""
                SELECT DISTINCT category_main
                FROM broadcast_training_dataset
                WHERE category_main IS NOT NULL
                ORDER BY category_main
            """)
            
            with self.engine.connect() as conn:
                result = conn.execute(query).fetchall()
            
            categories = [row[0] for row in result]
            return categories
            
        except Exception as e:
            logger.error(f"ì „ì²´ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    async def _get_ace_products_from_category(self, category: str, limit: int = 5) -> List[Dict[str, Any]]:
        """ì¹´í…Œê³ ë¦¬ë³„ ì—ì´ìŠ¤ ìƒí’ˆ ì¡°íšŒ"""
        try:
            query = text("""
                SELECT product_code, product_name, category_main, category_middle, category_sub,
                       AVG(gross_profit) as avg_sales, COUNT(*) as broadcast_count,
                       tape_code, tape_name, MAX(price) as price, brand
                FROM broadcast_training_dataset 
                WHERE category_main = :category
                GROUP BY product_code, product_name, category_main, category_middle, category_sub,
                         tape_code, tape_name, brand
                ORDER BY avg_sales DESC 
                LIMIT :limit
            """)
            
            with self.engine.connect() as conn:
                result = conn.execute(query, {"category": category, "limit": limit}).fetchall()
                
            products = []
            for row in result:
                products.append({
                    "product_code": row[0],
                    "product_name": row[1],
                    "category_main": row[2],
                    "category_middle": row[3],
                    "category_sub": row[4],
                    "avg_sales": float(row[5]),
                    "broadcast_count": int(row[6]),
                    "tape_code": row[7],
                    "tape_name": row[8],
                    "price": float(row[9]) if row[9] else None,
                    "brand": row[10] if len(row) > 10 else None
                })
            
            return products
            
        except Exception as e:
            logger.error(f"ì—ì´ìŠ¤ ìƒí’ˆ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    def _remove_duplicates(self, candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì¤‘ë³µ ì œê±° - ê°™ì€ ìƒí’ˆì½”ë“œ ë° ê°™ì€ (ì†Œë¶„ë¥˜ + ë¸Œëœë“œ) ì¡°í•© ì œê±°"""
        seen_products = set()
        seen_category_brand_pairs = set()  # (ì†Œë¶„ë¥˜, ë¸Œëœë“œ) ì¡°í•©
        unique_candidates = []
        
        for candidate in candidates:
            product_code = candidate.get("product_code", "")
            category_sub = candidate.get("category_sub", "")
            brand = candidate.get("brand", "")
            
            # ìƒí’ˆì½”ë“œ ì¤‘ë³µ ì²´í¬
            if product_code and product_code in seen_products:
                continue
            
            # ì†Œë¶„ë¥˜ + ë¸Œëœë“œ ì¡°í•© ì¤‘ë³µ ì²´í¬
            category_brand_key = (category_sub, brand)
            if category_sub and brand and category_brand_key in seen_category_brand_pairs:
                logger.info(f"ì†Œë¶„ë¥˜+ë¸Œëœë“œ ì¤‘ë³µ ì œì™¸: {candidate.get('product_name', '')} (ì†Œë¶„ë¥˜: {category_sub}, ë¸Œëœë“œ: {brand})")
                continue
            
            # í†µê³¼í•œ ê²½ìš° ì¶”ê°€
            if product_code:
                seen_products.add(product_code)
            if category_sub and brand:
                seen_category_brand_pairs.add(category_brand_key)
            unique_candidates.append(candidate)
        
        logger.info(f"ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(candidates)}ê°œ â†’ {len(unique_candidates)}ê°œ (ì†Œë¶„ë¥˜+ë¸Œëœë“œ ë‹¤ì–‘ì„± ë³´ì¥)")
        return unique_candidates
    
    def _rank_candidates(self, candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """í›„ë³´ ë­í‚¹"""
        return sorted(candidates, key=lambda x: x.get("final_score", 0), reverse=True)
    
    def _get_time_slot(self, dt: datetime) -> str:
        """ì‹œê°„ëŒ€ ë¶„ë¥˜"""
        hour = dt.hour
        if 6 <= hour < 9:
            return "ì•„ì¹¨"
        elif 9 <= hour < 12:
            return "ì˜¤ì „"
        elif 12 <= hour < 14:
            return "ì ì‹¬"
        elif 14 <= hour < 18:
            return "ì˜¤í›„"
        elif 18 <= hour < 22:
            return "ì €ë…"
        else:
            return "ì•¼ê°„"
    
    def _get_season(self, month: int) -> str:
        """ê³„ì ˆ ë¶„ë¥˜"""
        if 3 <= month <= 5:
            return "ë´„"
        elif 6 <= month <= 8:
            return "ì—¬ë¦„"
        elif 9 <= month <= 11:
            return "ê°€ì„"
        else:
            return "ê²¨ìš¸"
