"""
ë°©ì†¡ í¸ì„± AI ì¶”ì²œ ì›Œí¬í”Œë¡œìš°
LangChain ê¸°ë°˜ 2ë‹¨ê³„ ì›Œí¬í”Œë¡œìš°: AI ë°©í–¥ íƒìƒ‰ + ê³ ì† ë­í‚¹
"""

import asyncio
import calendar
import json
import logging
import time
from datetime import datetime, timedelta, date
from typing import List, Dict, Any, Optional, Tuple
import pandas as pd
import numpy as np
from sqlalchemy import create_engine, text
import os

from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import ChatPromptTemplate
from .external_apis import ExternalAPIManager

from .dependencies import get_product_embedder
from . import broadcast_recommender as br
from .schemas import BroadcastResponse, BroadcastRecommendation, ProductInfo, BusinessMetrics, NaverProduct, CompetitorProduct, LastBroadcastMetrics, RecommendationSource
from .external_products_service import ExternalProductsService
from .services.broadcast_history_service import BroadcastHistoryService
from .netezza_config import netezza_conn

logger = logging.getLogger(__name__)

class BroadcastWorkflow:
    """ë°©ì†¡ í¸ì„± AI ì¶”ì²œ ì›Œí¬í”Œë¡œìš°"""
    
    def __init__(self, model):
        self.model = model  # XGBoost ëª¨ë¸
        self.product_embedder = get_product_embedder()
        
        # AI íŠ¸ë Œë“œ ìºì‹œ (ì‹œê°„ëŒ€ë³„)
        self._ai_trends_cache = {}
        self._cache_ttl = 3600  # 1ì‹œê°„ (ì´ˆ)
        
        # LangChain LLM ì´ˆê¸°í™”
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.5,
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        
        # DB ì—°ê²°
        self.engine = create_engine(os.getenv("POSTGRES_URI"))
        
        # ì™¸ë¶€ ìƒí’ˆ ì„œë¹„ìŠ¤
        self.external_products_service = ExternalProductsService()
        
        # ë°©ì†¡ ì´ë ¥ ì„œë¹„ìŠ¤ (Netezza)
        self.broadcast_history_service = BroadcastHistoryService()
    
    async def process_broadcast_recommendation(
        self, 
        broadcast_time: str, 
        recommendation_count: int = 5,
        trend_weight: float = 0.3,  # íŠ¸ë Œë“œ ê°€ì¤‘ì¹˜ (0.3 = 30%)
        selling_weight: float = 0.7   # ë§¤ì¶œ ì˜ˆì¸¡ ê°€ì¤‘ì¹˜ (0.7 = 70%)
    ) -> BroadcastResponse:
        """ë©”ì¸ ì›Œí¬í”Œë¡œìš°: ë°©ì†¡ ì‹œê°„ ê¸°ë°˜ ì¶”ì²œ
        
        Args:
            broadcast_time: ë°©ì†¡ ì‹œê°„
            recommendation_count: ì¶”ì²œ ê°œìˆ˜
            trend_weight: íŠ¸ë Œë“œ ê°€ì¤‘ì¹˜ (0.0~1.0, ê¸°ë³¸ 0.3)
            selling_weight: ë§¤ì¶œ ì˜ˆì¸¡ ê°€ì¤‘ì¹˜ (0.0~1.0, ê¸°ë³¸ 0.7)
                - ì˜ˆ: trend_weight=0.3, selling_weight=0.7 â†’ íŠ¸ë Œë“œ 30%, ë§¤ì¶œ 70%
                - ì˜ˆ: trend_weight=0.5, selling_weight=0.5 â†’ ê· í˜• (50:50)
        """
        
        import time
        workflow_start = time.time()
        
        print("=== [DEBUG] process_broadcast_recommendation ì‹œì‘ ===")
        request_time = datetime.now().isoformat()
        logger.info(f"ë°©ì†¡ ì¶”ì²œ ì›Œí¬í”Œë¡œìš° ì‹œì‘: {broadcast_time}")
        print(f"=== [DEBUG] broadcast_time: {broadcast_time}, recommendation_count: {recommendation_count} ===")
        
        try:
            # 1ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ë° í†µí•© í‚¤ì›Œë“œ ìƒì„±
            step_start = time.time()
            print("=== [DEBUG] _collect_context_and_keywords í˜¸ì¶œ ===")
            context = await self._collect_context_and_keywords(broadcast_time)
            print(f"â±ï¸  [1ë‹¨ê³„] ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘: {time.time() - step_start:.2f}ì´ˆ")
            print(f"=== [DEBUG] í†µí•© í‚¤ì›Œë“œ: {len(context.get('unified_keywords', []))}ê°œ ===")
            
            # 2. í†µí•© ê²€ìƒ‰ ì‹¤í–‰ (1íšŒ)
            step_start = time.time()
            print("=== [DEBUG] _execute_unified_search í˜¸ì¶œ ===")
            search_result = await self._execute_unified_search(context, context.get("unified_keywords", []))
            print(f"â±ï¸  [2ë‹¨ê³„] í†µí•© ê²€ìƒ‰: {time.time() - step_start:.2f}ì´ˆ")
            print(f"=== [DEBUG] ê²€ìƒ‰ ì™„ë£Œ - ì§ì ‘ë§¤ì¹­: {len(search_result['direct_products'])}ê°œ, ì¹´í…Œê³ ë¦¬: {len(search_result['category_groups'])}ê°œ ===")
            
            # ê²€ìƒ‰ì— ì‚¬ìš©ëœ í‚¤ì›Œë“œë¥¼ contextì— ì €ì¥
            context["search_keywords"] = search_result.get("search_keywords", [])
            
            # 3. í›„ë³´êµ° ìƒì„± (ê°€ì¤‘ì¹˜ ê¸°ë°˜ ë¹„ìœ¨ ì¡°ì •)
            step_start = time.time()
            print("=== [DEBUG] _generate_unified_candidates í˜¸ì¶œ ===")
            max_trend = max(1, int(recommendation_count * trend_weight))  # ìµœì†Œ 1ê°œ
            max_sales = recommendation_count - max_trend + 3  # ì—¬ìœ ë¶„ ì¶”ê°€
            print(f"=== [DEBUG] ê°€ì¤‘ì¹˜ ì ìš©: íŠ¸ë Œë“œ {max_trend}ê°œ ({trend_weight:.0%}), ë§¤ì¶œ {max_sales}ê°œ ({selling_weight:.0%}) ===")
            
            candidate_products, category_scores = await self._generate_unified_candidates(
                search_result,
                context,
                max_trend_match=max_trend,
                max_sales_prediction=max_sales
            )
            print(f"â±ï¸  [3ë‹¨ê³„] í›„ë³´êµ° ìƒì„±: {time.time() - step_start:.2f}ì´ˆ")
            print(f"=== [DEBUG] í›„ë³´êµ° ìƒì„± ì™„ë£Œ: {len(candidate_products)}ê°œ ===")
            
            # 4. ìµœì¢… ë­í‚¹ ê³„ì‚°
            step_start = time.time()
            ranked_products = await self._rank_final_candidates(
                candidate_products,
                category_scores=category_scores,
                context=context
            )
            print(f"â±ï¸  [4ë‹¨ê³„] ìµœì¢… ë­í‚¹: {time.time() - step_start:.2f}ì´ˆ")
            
            # 5. API ì‘ë‹µ ìƒì„±
            step_start = time.time()
            response = await self._format_response(ranked_products[:recommendation_count], context)
            response.requestTime = request_time
            step_time = time.time() - step_start
            print(f"â±ï¸  [5ë‹¨ê³„] ì‘ë‹µ ìƒì„± ì´: {step_time:.2f}ì´ˆ")
            
            total_time = time.time() - workflow_start
            print(f"â±ï¸  ===== ì›Œí¬í”Œë¡œìš° ì´ ì‹œê°„: {total_time:.2f}ì´ˆ =====")
            
            logger.info(f"ë°©ì†¡ ì¶”ì²œ ì™„ë£Œ: {len(ranked_products)}ê°œ ì¶”ì²œ")
            return response
            
        except Exception as e:
            print(f"=== [DEBUG] ì˜ˆì™¸ ë°œìƒ: {type(e).__name__}: {e} ===")
            import traceback
            traceback.print_exc()
            logger.error(f"ë°©ì†¡ ì¶”ì²œ ì›Œí¬í”Œë¡œìš° ì˜¤ë¥˜: {e}")
            # OpenAI API ê´€ë ¨ ì˜¤ë¥˜ëŠ” ìƒìœ„ë¡œ ì „íŒŒ (503 ì—ëŸ¬ ë°˜í™˜ìš©)
            if "AI ì„œë¹„ìŠ¤" in str(e) or "OpenAI" in str(e) or "í• ë‹¹ëŸ‰" in str(e):
                raise e
            # ê¸°íƒ€ ë‚´ë¶€ ì˜¤ë¥˜ëŠ” 500 ì—ëŸ¬ë¡œ ì²˜ë¦¬
            raise Exception(f"ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜: {e}")
    
    async def _collect_context_and_keywords(self, broadcast_time: str) -> Dict[str, Any]:
        """ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ë° í†µí•© í‚¤ì›Œë“œ ìƒì„± (ê°œì„ ëœ ë²„ì „)"""
        
        # ë°©ì†¡ ì‹œê°„ íŒŒì‹±
        broadcast_dt = datetime.fromisoformat(broadcast_time.replace('Z', '+00:00'))
        
        # DBì—ì„œ ê³µíœ´ì¼ ì •ë³´ ì¡°íšŒ
        holiday_name = await self._get_holiday_from_db(broadcast_dt.date())
        
        context = {
            "broadcast_time": broadcast_time,
            "broadcast_dt": broadcast_dt,
            "hour": broadcast_dt.hour,
            "weekday": broadcast_dt.weekday(),
            "season": self._get_season(broadcast_dt.month),
            "holiday_name": holiday_name  # ê³µíœ´ì¼ ì •ë³´ ì¶”ê°€
        }
        
        # ë‚ ì”¨ ì •ë³´ ìˆ˜ì§‘
        weather_info = br.get_weather_by_date(broadcast_dt.date())
        context["weather"] = weather_info

        # ì‹œê°„ëŒ€ ì •ë³´
        time_slot = self._get_time_slot(broadcast_dt)
        day_type = "ì£¼ë§" if broadcast_dt.weekday() >= 5 else "í‰ì¼"
        context["time_slot"] = time_slot
        context["day_type"] = day_type

        # AI ê¸°ë°˜ íŠ¸ë Œë“œ ìƒì„± (LLM API) - ìºì‹± ì ìš©
        cache_key = f"{broadcast_dt.hour}_{weather_info.get('weather', 'Clear')}"
        current_time = datetime.now().timestamp()
        
        # ìºì‹œ í™•ì¸
        if cache_key in self._ai_trends_cache:
            cached_data, cached_time = self._ai_trends_cache[cache_key]
            if current_time - cached_time < self._cache_ttl:
                context["ai_trends"] = cached_data
                logger.info(f"âœ… AI íŠ¸ë Œë“œ ìºì‹œ íˆíŠ¸ ({cache_key}): {len(cached_data)}ê°œ í‚¤ì›Œë“œ")
            else:
                # ìºì‹œ ë§Œë£Œ
                del self._ai_trends_cache[cache_key]
                logger.info(f"â° AI íŠ¸ë Œë“œ ìºì‹œ ë§Œë£Œ ({cache_key})")
                context["ai_trends"] = None
        else:
            context["ai_trends"] = None
        
        # ìºì‹œ ë¯¸ìŠ¤ ì‹œ API í˜¸ì¶œ
        if context["ai_trends"] is None:
            api_manager = ExternalAPIManager()
            if api_manager.llm_trend_api:
                try:
                    import time
                    api_start = time.time()
                    # ë°©ì†¡ ì‹œê°„ê³¼ ë‚ ì”¨ ì •ë³´ë¥¼ ì „ë‹¬í•˜ì—¬ ë§¥ë½ ê¸°ë°˜ íŠ¸ë Œë“œ ìƒì„±
                    llm_trends = await api_manager.llm_trend_api.get_trending_searches(
                        hour=broadcast_dt.hour,
                        weather_info=weather_info,
                        broadcast_date=broadcast_dt  # ë°©ì†¡ ë‚ ì§œ ì „ë‹¬
                    )
                    api_time = time.time() - api_start
                    # AIê°€ ìƒì„±í•œ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì¶”ê°€
                    context["ai_trends"] = [t["keyword"] for t in llm_trends]
                    # ìºì‹œ ì €ì¥
                    self._ai_trends_cache[cache_key] = (context["ai_trends"], current_time)
                    logger.info(f"ğŸ”¥ AI íŠ¸ë Œë“œ ìƒì„± ì™„ë£Œ ({broadcast_dt.hour}ì‹œ, {weather_info.get('weather', 'N/A')}): {len(llm_trends)}ê°œ í‚¤ì›Œë“œ (ì†Œìš”: {api_time:.2f}ì´ˆ)")
                    logger.info(f"AI íŠ¸ë Œë“œ: {context['ai_trends'][:5]}...")  # ìƒìœ„ 5ê°œë§Œ ë¡œê·¸
                except Exception as e:
                    logger.error(f"AI íŠ¸ë Œë“œ ìƒì„± ì‹¤íŒ¨: {e}")
                    context["ai_trends"] = []
            else:
                logger.warning("OpenAI API í‚¤ ì—†ìŒ - AI íŠ¸ë Œë“œ ìƒì„± ê±´ë„ˆëœ€")
                context["ai_trends"] = []

        # ì»¨í…ìŠ¤íŠ¸ ë¡œê·¸ ì¶œë ¥
        logger.info(f"ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ - ê³„ì ˆ: {context['season']}, ì‹œê°„ëŒ€: {time_slot}, ìš”ì¼: {day_type}")
        if holiday_name:
            logger.info(f"ğŸ‰ ê³µíœ´ì¼: {holiday_name}")
        logger.info(f"ë‚ ì”¨: {weather_info.get('weather', 'N/A')}")
        
        # í†µí•© í‚¤ì›Œë“œ ìƒì„± (ì»¨í…ìŠ¤íŠ¸ ìš°ì„ , ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ë³´ì¡°)
        unified_keywords = []
        
        # 1. ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í‚¤ì›Œë“œ ìƒì„± (ë‚ ì§œ/ì‹œê°„/ë‚ ì”¨ ê¸°ë°˜ - ìš°ì„ ìˆœìœ„ ë†’ìŒ)
        context_keywords = await self._generate_context_keywords(context)
        if context_keywords:
            unified_keywords.extend(context_keywords)
            logger.info(f"[ìš°ì„ ìˆœìœ„ 1] ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ {len(context_keywords)}ê°œ ì¶”ê°€")
        
        # 2. AI íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì¶”ê°€ (1ë‹¨ê³„ LLM - ì‹œê°„ëŒ€/ë‚ ì”¨ ê¸°ë°˜ ìƒí’ˆ í‚¤ì›Œë“œ)
        if context.get("ai_trends"):
            ai_trend_limit = 10  # 3ê°œ â†’ 10ê°œë¡œ ì¦ê°€ (ê²¨ìš¸ ì‹œì¦Œ ìƒí’ˆ ë°˜ì˜)
            ai_keywords = context["ai_trends"][:ai_trend_limit]
            unified_keywords.extend(ai_keywords)
            print(f"[ìš°ì„ ìˆœìœ„ 2] AI íŠ¸ë Œë“œ í‚¤ì›Œë“œ {len(ai_keywords)}ê°œ ì¶”ê°€: {ai_keywords}")
            logger.info(f"[ìš°ì„ ìˆœìœ„ 2] AI íŠ¸ë Œë“œ í‚¤ì›Œë“œ {len(ai_keywords)}ê°œ ì¶”ê°€: {ai_keywords}")
        
        # 3. ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ íŠ¸ë Œë“œ ì¶”ê°€ (2ë‹¨ê³„ LLM - Web Search)
        print("=" * 80)
        print("[í†µí•© í‚¤ì›Œë“œ ìƒì„±] 2ë‹¨ê³„: ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ íŠ¸ë Œë“œ")
        print("=" * 80)
        try:
            realtime_result = await self._get_realtime_trend_keywords()
            # íŠœí”Œ ë°˜í™˜ ì²˜ë¦¬ (í‚¤ì›Œë“œ, ë‰´ìŠ¤ì¶œì²˜)
            if isinstance(realtime_result, tuple):
                realtime_keywords, news_sources = realtime_result
            else:
                realtime_keywords = realtime_result if realtime_result else []
                news_sources = {}
            
            if realtime_keywords:
                unified_keywords.extend(realtime_keywords)
                context["realtime_trends"] = realtime_keywords  # ì»¨í…ìŠ¤íŠ¸ì—ë„ ì €ì¥
                context["news_sources"] = news_sources  # ë‰´ìŠ¤ ì¶œì²˜ ì •ë³´ ì €ì¥
                print(f"[2ë‹¨ê³„ ì™„ë£Œ] ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ í‚¤ì›Œë“œ {len(realtime_keywords)}ê°œ: {realtime_keywords}")
                print(f"[2ë‹¨ê³„ ì™„ë£Œ] ë‰´ìŠ¤ ì¶œì²˜ {len(news_sources)}ê°œ: {list(news_sources.keys())}")
                logger.info(f"[ìš°ì„ ìˆœìœ„ 3] ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ í‚¤ì›Œë“œ {len(realtime_keywords)}ê°œ ì¶”ê°€: {realtime_keywords}")
            else:
                context["realtime_trends"] = []
                context["news_sources"] = {}
        except Exception as e:
            print(f"[2ë‹¨ê³„ ì‹¤íŒ¨] {e}")
            logger.warning(f"ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ íŠ¸ë Œë“œ ìˆ˜ì§‘ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
            context["realtime_trends"] = []
            context["news_sources"] = {}
        
        # 4. ì¤‘ë³µ ì œê±° ë° ì €ì¥
        context["unified_keywords"] = list(dict.fromkeys(unified_keywords))  # ìˆœì„œ ìœ ì§€ ì¤‘ë³µ ì œê±°
        
        # í†µí•© í‚¤ì›Œë“œ ë¡œê·¸ ì¶œë ¥
        print("=" * 80)
        print(f"[í‚¤ì›Œë“œ í†µí•© ì™„ë£Œ] ì´ {len(context['unified_keywords'])}ê°œ í‚¤ì›Œë“œ")
        print("=" * 80)
        print(f"[í†µí•© í‚¤ì›Œë“œ ì „ì²´]: {context['unified_keywords']}")
        print("=" * 80)
        logger.info(f"í†µí•© í‚¤ì›Œë“œ ìƒì„± ì™„ë£Œ: ì´ {len(context['unified_keywords'])}ê°œ")
        logger.info(f"í†µí•© í‚¤ì›Œë“œ (ìš°ì„ ìˆœìœ„ìˆœ): {context['unified_keywords']}")

        return context
    
    async def _get_holiday_from_db(self, target_date) -> Optional[str]:
        """DBì—ì„œ ê³µíœ´ì¼ ì •ë³´ ì¡°íšŒ"""
        try:
            with self.engine.connect() as conn:
                query = text("""
                    SELECT holiday_name 
                    FROM TAIHOLIDAYS 
                    WHERE holiday_date = :target_date
                """)
                result = conn.execute(query, {"target_date": target_date})
                row = result.fetchone()
                
                if row:
                    holiday_name = row[0]
                    logger.info(f"ê³µíœ´ì¼ ì¡°íšŒ ì„±ê³µ: {target_date} -> {holiday_name}")
                    return holiday_name
                else:
                    return None
        except Exception as e:
            logger.error(f"ê³µíœ´ì¼ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return None
    
    def _get_season(self, month: int) -> str:
        """ê³„ì ˆ ì •ë³´ ë°˜í™˜"""
        if month in [12, 1, 2]:
            return "ê²¨ìš¸"
        elif month in [3, 4, 5]:
            return "ë´„"
        elif month in [6, 7, 8]:
            return "ì—¬ë¦„"
        else:
            return "ê°€ì„"
    
    def _get_time_slot(self, dt: datetime) -> str:
        """ì‹œê°„ëŒ€ ì •ë³´ ë°˜í™˜"""
        hour = dt.hour
        if 6 <= hour < 12:
            return "ì˜¤ì „"
        elif 12 <= hour < 18:
            return "ì˜¤í›„"
        elif 18 <= hour < 24:
            return "ì €ë…"
        else:
            return "ìƒˆë²½"
    
    def _get_historical_broadcast_periods(
        self, 
        target_date: date, 
        years_back: int = 5
    ) -> List[Tuple[date, date]]:
        """
        ì…ë ¥ ë‚ ì§œ ê¸°ì¤€ ì•ë’¤ 1ê°œì›”ì”© ì´ 2ê°œì›” ê¸°ê°„ì„ ëª¨ë“  ê³¼ê±° ì—°ë„ì— ëŒ€í•´ ìƒì„±
        
        Args:
            target_date: ê¸°ì¤€ ë‚ ì§œ
            years_back: ëª‡ ë…„ ì „ê¹Œì§€ ì¡°íšŒí• ì§€ (ê¸°ë³¸ 5ë…„)
            
        Returns:
            [(ì‹œì‘ì¼, ì¢…ë£Œì¼), ...] ë¦¬ìŠ¤íŠ¸
            
        Example:
            target_date = 2025-03-15 â†’ 
            [(2024-02-15, 2024-04-15), (2023-02-15, 2023-04-15), ...]
        """
        periods = []
        current_year = target_date.year
        print(f"=== [DEBUG _get_historical_broadcast_periods] target_date: {target_date}, years_back: {years_back} ===")
        
        for year_offset in range(1, years_back + 1):
            target_year = current_year - year_offset
            
            try:
                # 1ê°œì›” ì „ ê³„ì‚°
                start_month = target_date.month - 1
                start_year = target_year
                if start_month < 1:
                    start_month = 12
                    start_year -= 1
                
                # 1ê°œì›” í›„ ê³„ì‚°
                end_month = target_date.month + 1
                end_year = target_year
                if end_month > 12:
                    end_month = 1
                    end_year += 1
                
                # ì¼ì ì²˜ë¦¬ (ì›”ë§ ì´ˆê³¼ ë°©ì§€)
                start_day = min(target_date.day, calendar.monthrange(start_year, start_month)[1])
                end_day = min(target_date.day, calendar.monthrange(end_year, end_month)[1])
                
                start_date = date(start_year, start_month, start_day)
                end_date = date(end_year, end_month, end_day)
                
                periods.append((start_date, end_date))
                
            except Exception as e:
                logger.warning(f"ê¸°ê°„ ê³„ì‚° ì˜¤ë¥˜ (year_offset={year_offset}): {e}")
                print(f"=== [DEBUG] ê¸°ê°„ ê³„ì‚° ì˜¤ë¥˜: {e} ===")
                import traceback
                print(traceback.format_exc())
                continue
        
        print(f"=== [DEBUG _get_historical_broadcast_periods] ìƒì„±ëœ ê¸°ê°„: {len(periods)}ê°œ ===", flush=True)
        if periods:
            print(f"  ì²« ë²ˆì§¸ ê¸°ê°„: {periods[0]}", flush=True)
        return periods

    # _classify_keywords_with_langchain í•¨ìˆ˜ ì œê±°ë¨
    # ì´ì œ _generate_base_context_keywordsì—ì„œ í‚¤ì›Œë“œ ìƒì„±ê³¼ í™•ì¥ì„ í†µí•© ì²˜ë¦¬
    
    async def _execute_unified_search(self, context: Dict[str, Any], unified_keywords: List[str]) -> Dict[str, Any]:
        """ë‹¤ë‹¨ê³„ Qdrant ê²€ìƒ‰: ê³¼ê±° ë™ì¼ ì‹œê¸° ë°©ì†¡ ìƒí’ˆ í›„ë³´êµ° ë‚´ì—ì„œ í‚¤ì›Œë“œ ê²€ìƒ‰"""
        
        print(f"=== [DEBUG Multi-Stage Search] ì‹œì‘, keywords: {len(unified_keywords)}ê°œ ===")
        
        if not unified_keywords:
            logger.warning("í†µí•© í‚¤ì›Œë“œ ì—†ìŒ - ë¹ˆ ê²°ê³¼ ë°˜í™˜")
            return {"direct_products": [], "category_groups": {}}
        
        try:
            # [ì£¼ì„ì²˜ë¦¬] 0ë‹¨ê³„: ë°©ì†¡ ê¸°ê°„ í•„í„° ê³„ì‚° (ì…ë ¥ ë‚ ì§œ ê¸°ì¤€ ì•ë’¤ 1ê°œì›”, ê³¼ê±° 5ë…„)
            # TODO: ìš”êµ¬ì‚¬í•­ í™•ì • í›„ ë‹¤ì‹œ í™œì„±í™”
            # broadcast_periods = None
            # target_date_str = context.get("broadcast_time", "")
            # print(f"=== [DEBUG] broadcast_time from context: {target_date_str} ===")
            # 
            # if target_date_str:
            #     try:
            #         # broadcast_timeì—ì„œ ë‚ ì§œ ì¶”ì¶œ (ì˜ˆ: "2025-03-15 20:00:00" â†’ date(2025, 3, 15))
            #         if isinstance(target_date_str, str):
            #             target_dt = datetime.fromisoformat(target_date_str.replace("Z", "+00:00"))
            #         else:
            #             target_dt = target_date_str
            #         target_date = target_dt.date()
            #         
            #         # ê³¼ê±° 5ë…„ê°„ ë™ì¼ ì‹œê¸° ë°©ì†¡ ê¸°ê°„ ê³„ì‚°
            #         broadcast_periods = self._get_historical_broadcast_periods(target_date, years_back=5)
            #         
            #         if broadcast_periods:
            #             print(f"=== [ë°©ì†¡ ê¸°ê°„ í•„í„°] ê¸°ì¤€ì¼: {target_date}, {len(broadcast_periods)}ê°œ ê¸°ê°„ ===")
            #             for i, (start, end) in enumerate(broadcast_periods[:3]):
            #                 print(f"  - {start} ~ {end}")
            #             if len(broadcast_periods) > 3:
            #                 print(f"  - ... ì™¸ {len(broadcast_periods) - 3}ê°œ ê¸°ê°„")
            #     except Exception as e:
            #         logger.warning(f"ë°©ì†¡ ê¸°ê°„ í•„í„° ê³„ì‚° ì‹¤íŒ¨: {e}, í•„í„° ì—†ì´ ê²€ìƒ‰")
            #         broadcast_periods = None
            
            # ëª¨ë“  í‚¤ì›Œë“œë¥¼ ê°œë³„ì ìœ¼ë¡œ ê²€ìƒ‰ (í‚¤ì›Œë“œë³„ ë‹¤ì–‘ì„± í™•ë³´)
            all_results = []
            seen_products = set()
            keyword_results = {}  # í‚¤ì›Œë“œë³„ ê²€ìƒ‰ ê²°ê³¼ ì¶”ì 
            product_matched_keywords = {}  # ìƒí’ˆë³„ ë§¤ì¹­ëœ í‚¤ì›Œë“œ ì¶”ì 
            
            print(f"=== [ê°œë³„ í‚¤ì›Œë“œ ê²€ìƒ‰] ì´ {len(unified_keywords)}ê°œ í‚¤ì›Œë“œ ===")
            
            for keyword in unified_keywords:
                # [ì£¼ì„ì²˜ë¦¬] ë°©ì†¡ ê¸°ê°„ í•„í„° ì ìš© ë¡œì§ - ìš”êµ¬ì‚¬í•­ í™•ì • í›„ í™œì„±í™”
                # if broadcast_periods:
                #     results = self.product_embedder.search_products_with_broadcast_filter(
                #         trend_keywords=[keyword],
                #         broadcast_periods=broadcast_periods,
                #         top_k=10,
                #         score_threshold=0.3,
                #         only_ready_products=True
                #     )
                # else:
                results = self.product_embedder.search_products(
                    trend_keywords=[keyword],
                    top_k=5,
                    score_threshold=0.3,
                    only_ready_products=True
                )
                
                new_count = 0
                for r in results:
                    code = r.get("product_code")
                    if code not in seen_products:
                        # ìƒí’ˆì— ë§¤ì¹­ëœ í‚¤ì›Œë“œ ì •ë³´ ì¶”ê°€
                        r["matched_keyword"] = keyword
                        all_results.append(r)
                        seen_products.add(code)
                        product_matched_keywords[code] = keyword
                        new_count += 1
                    elif code in product_matched_keywords:
                        # ì´ë¯¸ ìˆëŠ” ìƒí’ˆì´ë©´ ì¶”ê°€ í‚¤ì›Œë“œë§Œ ê¸°ë¡ (ì²« ë²ˆì§¸ í‚¤ì›Œë“œê°€ ê°€ì¥ ê´€ë ¨ì„± ë†’ìŒ)
                        pass
                
                if new_count > 0:
                    keyword_results[keyword] = new_count
            
            # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ (ì „ì²´ ì¶œë ¥)
            print(f"=== [í‚¤ì›Œë“œë³„ ê²€ìƒ‰ ê²°ê³¼] ì´ {len(keyword_results)}ê°œ í‚¤ì›Œë“œ ===")
            for kw, count in keyword_results.items():
                print(f"  - {kw}: {count}ê°œ")
            
            # ìœ ì‚¬ë„ ê¸°ì¤€ ì •ë ¬
            all_results.sort(key=lambda x: x.get("similarity_score", 0), reverse=True)
            
            print(f"=== [ê²€ìƒ‰ ì™„ë£Œ] ì´ {len(all_results)}ê°œ ìƒí’ˆ (ìœ ì‚¬ë„ìˆœ ì •ë ¬) ===")
            
            # ìœ ì‚¬ë„ ë¶„í¬ í™•ì¸ (ë””ë²„ê¹…)
            if all_results:
                similarities = [p.get("similarity_score", 0) for p in all_results]
                print(f"[ìœ ì‚¬ë„ ë¶„í¬] ìµœê³ : {max(similarities):.3f}, í‰ê· : {sum(similarities)/len(similarities):.3f}, ìµœì €: {min(similarities):.3f}")
                print(f"[ìƒìœ„ 5ê°œ ìœ ì‚¬ë„]")
                for i, p in enumerate(all_results[:5], 1):
                    sim = p.get("similarity_score", 0)
                    name = p.get("product_name", "")[:40]
                    tape = "ğŸ“¼" if (p.get("tape_code") and p.get("tape_name")) else "âŒ"
                    print(f"  {i}. {name} | ìœ ì‚¬ë„: {sim:.3f} | í…Œì´í”„: {tape}")
            
            # ìœ ì‚¬ë„ ê¸°ë°˜ ë¶„ë¥˜
            direct_products = []      # ê³ ìœ ì‚¬ë„: ì§ì ‘ ì¶”ì²œ
            category_groups = {}      # ì¤‘ìœ ì‚¬ë„: ì¹´í…Œê³ ë¦¬ë³„ ê·¸ë£¹
            
            # ìœ ì‚¬ë„ ì„ê³„ê°’
            HIGH_SIMILARITY_THRESHOLD = 0.45  # ì‹¤ì œ ìœ ì‚¬ë„ ë¶„í¬(ìµœê³  0.498)ì— ë§ì¶¤
            
            for product in all_results:
                similarity = product.get("similarity_score", 0)
                category = product.get("category_main", "ê¸°íƒ€")
                
                # ê³ ìœ ì‚¬ë„ ìƒí’ˆ: ì§ì ‘ ë§¤ì¹­
                if similarity >= HIGH_SIMILARITY_THRESHOLD:
                    if product.get("tape_code") and product.get("tape_name"):
                        direct_products.append({
                            **product,
                            "source": "direct_match",
                            "similarity_score": similarity
                        })
                        print(f"  âœ… ì§ì ‘ë§¤ì¹­: {product.get('product_name')[:30]} (ìœ ì‚¬ë„: {similarity:.2f})")
                
                # ì¤‘ìœ ì‚¬ë„: ì¹´í…Œê³ ë¦¬ ê·¸ë£¹í•‘
                if category not in category_groups:
                    category_groups[category] = []
                category_groups[category].append(product)
            
            print(f"=== [ë¶„ë¥˜ ì™„ë£Œ] ì§ì ‘ë§¤ì¹­: {len(direct_products)}ê°œ, ì¹´í…Œê³ ë¦¬: {len(category_groups)}ê°œ ===")
            
            return {
                "direct_products": direct_products,
                "category_groups": category_groups,
                "search_keywords": unified_keywords[:5]
            }
            
        except Exception as e:
            logger.error(f"ë‹¤ë‹¨ê³„ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(f"ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
            return {"direct_products": [], "category_groups": {}}
    
    async def _get_realtime_trend_keywords(self) -> List[str]:
        """ì‹¤ì‹œê°„ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ìˆ˜ì§‘ (OpenAI Web Search)"""
        from openai import OpenAI
        from datetime import datetime
        
        try:
            client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            
            
            # 1. [ë‚ ì§œ ë™ì  ê³„ì‚°]
            now = datetime.now()
            
            # í˜„ì¬ ì—°ë„/ì›”
            current_year = now.year        # 2025

            one_month_ago = now - timedelta(days=30)
            search_start_date = one_month_ago.strftime("%Y-%m-%d")
            
            current_date_str = now.strftime("%Yë…„ %mì›” %dì¼")


            # *** ì œì™¸í•  ê³¼ê±° ì—°ë„ ê³„ì‚° ***
            # (ì˜¬í•´ê°€ 2025ë…„ì´ë©´ 2024ë…„ ë°ì´í„°ëŠ” ë°°ì œí•˜ë¼ê³  ì‹œí‚¤ê¸° ìœ„í•¨)
            prev_year = current_year - 1   # 2024

            #target_period_str = f"{last_month_str} ~ {current_month_str}"
            
            prompt = f"""**[ì¦‰ì‹œ ì‹¤í–‰ ëª…ë ¹]**
1. ì§€ê¸ˆ ì¦‰ì‹œ ì›¹ ê²€ìƒ‰ ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì—¬ ë‰´ìŠ¤ ê¸°ì‚¬ë¥¼ ê²€ìƒ‰í•˜ì„¸ìš”.
2. ë¶„ì„ì´ë‚˜ ì‚¬ì¡± ì—†ì´ **ê²°ê³¼ JSON**ë§Œ ì¶œë ¥í•˜ì„¸ìš”.


**[ì‹œì  ì •ë³´ (ìë™ ê³„ì‚°ë¨)]**
- **í˜„ì¬ ì‹œì :** {current_date_str}
- **ê²€ìƒ‰ ìœ íš¨ ì‹œì‘ì¼:** {search_start_date}

**[í•„ìˆ˜ ì œì•½ ì¡°ê±´ 1: ë‚ ì§œ í•„í„°ë§]**
**ë‚ ì§œ í•„í„°ë§ (ì¹˜ëª…ì ):**
   - ê²€ìƒ‰ ê²°ê³¼ì—ì„œ **'{prev_year}ë…„'** ë˜ëŠ” ê·¸ ì´ì „ ë‚ ì§œì˜ ê¸°ì‚¬ëŠ” **ì ˆëŒ€ ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.**
   - ê²€ìƒ‰ ì¿¼ë¦¬ë¥¼ ìƒì„±í•  ë•Œ, ë°˜ë“œì‹œ **`after:{search_start_date}`** ì—°ì‚°ìë¥¼ ì¿¼ë¦¬ ë’¤ì— ë¶™ì—¬ì•¼ í•©ë‹ˆë‹¤. 
   (ì´ ì—°ì‚°ìë¥¼ ì“°ì§€ ì•Šìœ¼ë©´ ê³¼ê±° ê¸°ì‚¬ê°€ ê²€ìƒ‰ë˜ì–´ ë¶„ì„ì´ ì‹¤íŒ¨í•©ë‹ˆë‹¤.)

**[í•„ìˆ˜ ì œì•½ ì¡°ê±´ 2: DB ê²€ìƒ‰ìš© í‚¤ì›Œë“œ ì¶”ì¶œ (ëª…ì‚¬í™”)]**
- ë‰´ìŠ¤ ê¸°ì‚¬ì—ì„œ **'êµ¬ì²´ì ì¸ ìƒí’ˆëª…'**, **'ì¹´í…Œê³ ë¦¬'**, **'ë¸Œëœë“œ'**ë§Œ ì¶”ì¶œí•˜ì„¸ìš”.
- **ë¬¸ì¥í˜• ê¸ˆì§€:** '~~í•˜ëŠ” ìƒí™©', '~~ë¡œ ì¸í•œ í’ˆì ˆ' ê°™ì€ ì„œìˆ ì–´ì™€ ì¡°ì‚¬ë¥¼ ëª¨ë‘ ì œê±°í•˜ì„¸ìš”.
- **í•˜ë‚˜ì˜ ê¸°ì‚¬(URL)ì—ì„œ ì¶”ì¶œí•  ìˆ˜ ìˆëŠ” í‚¤ì›Œë“œëŠ” 'ìµœëŒ€ 3ê°œ'ë¡œ ì œí•œí•©ë‹ˆë‹¤.**
- 5ê°œì˜ í‚¤ì›Œë“œë¥¼ ì±„ìš°ê¸° ìœ„í•´ ìµœì†Œ 3ê°œ ì´ìƒì˜ ì„œë¡œ ë‹¤ë¥¸ ê¸°ì‚¬ë¥¼ ì°¸ì¡°í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.

**[í•µì‹¬ ê³¼ì œ]**
ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ 20ë…„ì°¨ ìœ í†µ ì „ë¬¸ ê¸°ìì´ì í™ˆì‡¼í•‘ MDì…ë‹ˆë‹¤.
ìœ„ ê¸°ê°„ ë™ì•ˆ í™ˆì‡¼í•‘ ë° ìœ í†µ ì—…ê³„ì—ì„œ ë°œìƒí•œ **ê°€ì¥ ëœ¨ê±°ìš´ 'íŠ¸ë Œë“œ í‚¤ì›Œë“œ' 5ê°œ**ë¥¼ ì°¾ì•„ë‚´ì„¸ìš”.

**[ê²€ìƒ‰ í‚¤ì›Œë“œ ì¡°í•© ì§€ì¹¨]**
ì •í™•í•œ ë‹¨ì–´ ë§¤ì¹­ë¿ë§Œ ì•„ë‹ˆë¼, ì•„ë˜ì™€ ê°™ì´ **ì—°ë„ + í˜„ìƒ**ì„ ì¡°í•©í•˜ì—¬ ê²€ìƒ‰í•˜ì„¸ìš”.

*(ê¶Œì¥ ê²€ìƒ‰ì–´ ì˜ˆì‹œ)*
- "í™ˆì‡¼í•‘ ì£¼ë¬¸ í­ì£¼ after:{search_start_date}"
- "ìœ í†µ ì™„íŒ ëŒ€ë€ after:{search_start_date}"
- "í™ˆì‡¼í•‘ ì¸ê¸° ìƒí’ˆ after:{search_start_date}"

**[ì •ë‹µ í•„í„°ë§ & Fallback]**
1. **í•„ìˆ˜:** ë°˜ë“œì‹œ 'ë‰´ìŠ¤ ê¸°ì‚¬'ì— ê·¼ê±°í•  ê²ƒ. (ë¸”ë¡œê·¸/ì¹´í˜ ë‡Œí”¼ì…œ ì œì™¸)
2. **í•„ìˆ˜:** í•˜ë‚˜ì˜ ê¸°ì‚¬(URL)ì—ì„œ ëª¨ë“  í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì§€ ë§ˆì„¸ìš”.
3. **Fallback:** 5ê°œë¥¼ ëª» ì±„ì›Œë„ ì¢‹ìœ¼ë‹ˆ, **ì°¾ì€ ê°œìˆ˜ë§Œí¼ë§Œì´ë¼ë„** ì¶œë ¥í•˜ì„¸ìš”. (ë¹ˆ ë°°ì—´ `[]` ê¸ˆì§€)
4. **ì œì™¸ ëŒ€ìƒ:** ë¹„ì‹¤ë¬¼ ìƒí’ˆ(ì•±, ì£¼ì‹, ë¶€ë™ì‚°) ì œì™¸.
5. **ì œì™¸ ëŒ€ìƒ:** ê¸°ì‚¬ ë‚ ì§œë¥¼ í™•ì¸í•˜ê³  {search_start_date} ì´ì „ ê¸°ì‚¬ì¼ ê²½ìš° ì œì™¸.
6. **ì œì™¸ ëŒ€ìƒ:** DBì—ì„œ ìƒí’ˆ ê²€ìƒ‰ì— ì‚¬ìš©í•  ìˆ˜ ìˆëŠ” **'êµ¬ì²´ì ì¸ ìƒí’ˆëª…'**, **'ì¹´í…Œê³ ë¦¬'**, **'ë¸Œëœë“œ'**ë§Œ ì¶”ì¶œ. ì•„ë‹ˆë©´ ì œì™¸.

**[ì¶œë ¥ í˜•ì‹]**
ë°˜ë“œì‹œ ì•„ë˜ JSON í¬ë§·ì„ ì§€í‚¬ ê²ƒ.
```json
{{
  "trend_keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3"],
  "sources": [
    {{"keyword": "í‚¤ì›Œë“œ1", "title": "ê¸°ì‚¬ì œëª©...", "URL": "ê¸°ì‚¬ ì¶œì²˜ URL..."}},
    {{"keyword": "í‚¤ì›Œë“œ2", "title": "ê¸°ì‚¬ì œëª©...", "URL": "ê¸°ì‚¬ ì¶œì²˜ URL..."}}
  ]
}}

"""
            
            print("=" * 80)
            print("[2ë‹¨ê³„ - OpenAI Web Search] ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ìˆ˜ì§‘ ì‹œì‘")
            print("=" * 80)
            print(f"[í”„ë¡¬í”„íŠ¸]\n{prompt}")
            print("=" * 80)
            logger.info(f"[2ë‹¨ê³„] ì‹¤ì‹œê°„ íŠ¸ë Œë“œ í”„ë¡¬í”„íŠ¸: {prompt[:200]}...")
            
            response = client.responses.create(
                model="gpt-5-nano",
                reasoning={"effort": "low"},
                instructions=f"ë‹¹ì‹ ì€ í•œêµ­ì–´ ë°ì´í„° ë¶„ì„ê°€ì…ë‹ˆë‹¤. ë°˜ë“œì‹œ `web_search` ë„êµ¬ë¥¼ ì‚¬ìš©í•˜ì„¸ìš”. ê²€ìƒ‰ì–´ ë’¤ì—ëŠ” `after:{search_start_date}`ë¥¼ ë¶™ì—¬ ìµœì‹  ê¸°ì‚¬ë§Œ ì°¾ìœ¼ì„¸ìš”. ê²°ê³¼ëŠ” ì˜¤ì§ JSONë§Œ ì¶œë ¥í•˜ì„¸ìš”.",
                tools=[{
                    "type": "web_search",
                    "search_context_size": "high",
                    "user_location": {
                        "type": "approximate",
                        "country": "KR",
                        "timezone": "Asia/Seoul"
                    }
                }],
                tool_choice="required",  # ì›¹ ê²€ìƒ‰ ë„êµ¬ ì‚¬ìš© ê°•ì œ
                input=prompt,
                max_output_tokens=8000,
                max_tool_calls=15
            )
            
            result_text = response.output_text
            print("=" * 80)
            print(f"[2ë‹¨ê³„ - ì‘ë‹µ (ì „ì²´)]")
            print("-" * 80)
            print(response.model_dump_json(indent=2))
            print("-" * 80)
            print(result_text)
            print("-" * 80)
            logger.info(f"[2ë‹¨ê³„] ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ì‘ë‹µ: {result_text}")
            
            # JSON ë°°ì—´ ì¶”ì¶œ (```json ì½”ë“œë¸”ë¡ ë‚´ë¶€ ìš°ì„ )
            import json
            import re
            
            # 1ì°¨: ì „ì²´ JSON ê°ì²´ íŒŒì‹± ì‹œë„
            try:
                # ```json ì½”ë“œë¸”ë¡ ì œê±°
                clean_text = re.sub(r'```json\s*|\s*```', '', result_text)
                data = json.loads(clean_text)
                
                # trend_keywords í•„ë“œ ì¶”ì¶œ
                if isinstance(data, dict) and 'trend_keywords' in data:
                    keywords = data['trend_keywords']
                    sources = data.get('sources', [])
                    print(f"[2ë‹¨ê³„ - ì¶”ì¶œ ì„±ê³µ] í‚¤ì›Œë“œ: {keywords}")
                    print(f"[2ë‹¨ê³„ - ë‰´ìŠ¤ ì¶œì²˜] {len(sources)}ê°œ ì¶œì²˜ ì •ë³´")
                    
                    # í‚¤ì›Œë“œë³„ ë‰´ìŠ¤ ì¶œì²˜ ë§¤í•‘ ë°˜í™˜ (íŠœí”Œë¡œ)
                    keyword_sources = {}
                    for src in sources:
                        kw = src.get('keyword', '')
                        if kw:
                            keyword_sources[kw] = {
                                'news_title': src.get('title', ''),
                                'news_url': src.get('URL', src.get('url', ''))
                            }
                    
                    return keywords[:5], keyword_sources
                elif isinstance(data, list):
                    # êµ¬ë²„ì „ í˜¸í™˜: ë°°ì—´ë§Œ ì˜¨ ê²½ìš°
                    print(f"[2ë‹¨ê³„ - ì¶”ì¶œ ì„±ê³µ] í‚¤ì›Œë“œ: {data}")
                    return data[:5], {}
            except json.JSONDecodeError:
                # 2ì°¨: ì •ê·œì‹ìœ¼ë¡œ trend_keywordsë§Œ ì¶”ì¶œ
                match = re.search(r'"trend_keywords"\s*:\s*(\[.*?\])', result_text, re.DOTALL)
                if match:
                    keywords = json.loads(match.group(1))
                    print(f"[2ë‹¨ê³„ - ì¶”ì¶œ ì„±ê³µ] í‚¤ì›Œë“œ: {keywords}")
                    return keywords[:5], {}
                
                print("[2ë‹¨ê³„ - ì‹¤íŒ¨] JSON ì¶”ì¶œ ì‹¤íŒ¨")
                return [], {}
                
        except Exception as e:
            print("=" * 80)
            print(f"[2ë‹¨ê³„ - ì˜¤ë¥˜] {e}")
            print("=" * 80)
            logger.error(f"[2ë‹¨ê³„] ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return [], {}
    
    async def _generate_base_context_keywords(self, context: Dict[str, Any]) -> List[str]:
        """ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ LangChainìœ¼ë¡œ ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„±"""
        
        # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ì¶œ (ì•ˆì „í•˜ê²Œ)
        weather_info = context.get("weather", {})
        logger.info(f"weather_info type: {type(weather_info)}, value: {weather_info}")
        
        if isinstance(weather_info, dict):
            weather = weather_info.get("weather", "ë§‘ìŒ")
            temperature = weather_info.get("temperature", 20)
        else:
            logger.warning(f"weather_info is not dict: {weather_info}")
            weather = "ë§‘ìŒ"
            temperature = 20
        
        time_slot = context.get("time_slot", "ì €ë…")
        day_type = context.get("day_type", "í‰ì¼")
        holiday_name = context.get("holiday_name")  # ê³µíœ´ì¼ ì •ë³´
        
        # ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
        broadcast_dt = context.get("broadcast_dt")
        month = broadcast_dt.month if broadcast_dt else 11
        day = broadcast_dt.day if broadcast_dt else 19
        
        logger.info(f"ì¶”ì¶œëœ ì •ë³´ - weather: {weather}, temp: {temperature}, time_slot: {time_slot}, month: {month}, day: {day}, day_type: {day_type}, holiday: {holiday_name}")
        
        # LangChain í”„ë¡¬í”„íŠ¸ (í‚¤ì›Œë“œ ìƒì„± + í™•ì¥ í†µí•©)
        keyword_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ 20ë…„ì°¨ í™ˆì‡¼í•‘ ìƒí’ˆ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì£¼ì–´ì§„ ìƒí™©ì— ë§ëŠ” **êµ¬ì²´ì ì¸ ìƒí’ˆëª… í‚¤ì›Œë“œ**ë¥¼ ìƒì„±í•˜ê³ , ì¶”ìƒì ì¸ í‚¤ì›Œë“œëŠ” í™•ì¥í•´ì£¼ì„¸ìš”.

**2ë‹¨ê³„ ì‘ì—…:**
1. ìƒí™©ì— ë§ëŠ” í‚¤ì›Œë“œ 10-15ê°œ ìƒì„±
2. ì¶”ìƒì  í‚¤ì›Œë“œë¥¼ êµ¬ì²´ì  ìƒí’ˆëª…ìœ¼ë¡œ í™•ì¥

**í•µì‹¬ ì›ì¹™: ì‹¤ì œ ìƒí’ˆëª…ì²˜ëŸ¼ êµ¬ì²´ì ìœ¼ë¡œ!**

âŒ ë‚˜ìœ ì˜ˆ (ì¶”ìƒì ):
- "ê²¨ìš¸ì¤€ë¹„", "ê±´ê°•ê´€ë¦¬", "ê°€ì¡±ëª¨ì„", "ë”°ëœ»í•œ", "í¸ë¦¬í•œ"

âœ… ì¢‹ì€ ì˜ˆ (êµ¬ì²´ì ):
- "íŒ¨ë”©", "ê¸°ëª¨ë°”ì§€", "ë‹´ìš”", "ì˜¤ë©”ê°€3", "ë½í† í•", "ì˜¨ì—´ê¸°", "ì „ê¸°ì¥íŒ"

**ì‹œì¦Œë³„ êµ¬ì²´ì  í‚¤ì›Œë“œ ì˜ˆì‹œ:**

11ì›”-12ì›” (ê²¨ìš¸):
- ì˜ë¥˜: "íŒ¨ë”©", "ê¸°ëª¨", "ëª©ë„ë¦¬", "ì¥ê°‘", "ê²¨ìš¸ì½”íŠ¸"
- ê°€ì „: "ì˜¨ì—´ê¸°", "ì „ê¸°ì¥íŒ", "íˆí„°", "ê°€ìŠµê¸°"
- ê±´ê°•: "ì˜¤ë©”ê°€3", "ìœ ì‚°ê· ", "í™ì‚¼", "ë¹„íƒ€ë¯¼", "ë©´ì—­"
- ì‹í’ˆ: "êµ°ê³ êµ¬ë§ˆ", "í˜¸ë¹µ", "ì–´ë¬µ", "í•«íŒ©"

7-8ì›” (ì—¬ë¦„):
- ì˜ë¥˜: "ë°˜íŒ”", "ë°˜ë°”ì§€", "ì›í”¼ìŠ¤", "ìƒŒë“¤"
- ê°€ì „: "ì„ í’ê¸°", "ì—ì–´ì»¨", "ì œìŠµê¸°", "ëƒ‰í’ê¸°"
- ê±´ê°•: "ìˆ˜ë¶„í¬ë¦¼", "ìì™¸ì„ ì°¨ë‹¨ì œ", "ë¹„íƒ€ë¯¼C"
- ì‹í’ˆ: "ì•„ì´ìŠ¤í¬ë¦¼", "ëƒ‰ë©´", "ìˆ˜ë°•", "ìŒë£Œ"

**í™•ì¥ ê·œì¹™:**
- "ìˆ˜ëŠ¥ ê°„ì‹" â†’ ["ì´ˆì½œë¦¿", "ê²¬ê³¼ë¥˜", "ì—ë„ˆì§€ë°”", "í™ì‚¼"]
- "ë¸”ë™í”„ë¼ì´ë°ì´" â†’ ["í• ì¸", "íŠ¹ê°€", "ì„¸ì¼"]
- "ê¹€ì¥ ì¬ë£Œ" â†’ ["ê¹€ì¹˜ëƒ‰ì¥ê³ ", "ì ˆì„ë°°ì¶”", "ê³ ì¶§ê°€ë£¨"]
- "ê²¨ìš¸ íŒ¨ì…˜" â†’ ["íŒ¨ë”©", "ê¸°ëª¨", "ì½”íŠ¸", "ëª©ë„ë¦¬"]
- ì´ë¯¸ êµ¬ì²´ì ì´ë©´ í™•ì¥ ë¶ˆí•„ìš”

**ì¤‘ìš” ì§€ì¹¨:**
1. ë¸Œëœë“œëª…ë„ í¬í•¨ ê°€ëŠ¥: "ë½í† í•", "ì¢…ê·¼ë‹¹", "ì¿ ì¿ ", "í•´í”¼ì½œ"
2. ìƒí’ˆ ì¹´í…Œê³ ë¦¬ëª…: "ê±´ê°•ì‹í’ˆ", "ìƒí™œê°€ì „", "ì˜ë¥˜", "ì‹í’ˆ"
3. ì‹œì¦Œ íŠ¹í™” ìƒí’ˆ: 11-12ì›”ì´ë©´ "í¬ë¦¬ìŠ¤ë§ˆìŠ¤", "ì—°ë§ì„ ë¬¼", "ìˆ˜ëŠ¥ê°„ì‹"
4. ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬ í¬í•¨ (ìµœì†Œ 3ê°œ ì´ìƒ ì¹´í…Œê³ ë¦¬)

**ì‹œê°„ëŒ€ë³„ ì¹´í…Œê³ ë¦¬ ìš°ì„ ìˆœìœ„ ë° ê°€ì¤‘ì¹˜ (ì¤‘ìš”!):**

ğŸŒ… ì•„ì¹¨ (06:00-09:59):
- ë§¤ìš° ì í•© (1.2): ê±´ê°•ì‹í’ˆ, ì¼ë°˜ì‹í’ˆ, ì£¼ë°©ìš©í’ˆ
- ë³´í†µ (0.9): ì˜ë¥˜, ê°€ì „
- ë¶€ì í•© (0.8): íŒ¨ì…˜ì¡í™”, ì‹ ë°œ
- ì˜ˆ: "ì˜¤ë©”ê°€3"(1.2), "ìœ ì‚°ê· "(1.2), "ì»¤í”¼"(1.2), "íŒ¨ë”©"(0.9)

ğŸŒ ì ì‹¬ (10:00-13:59):
- ë§¤ìš° ì í•© (1.2): ì¼ë°˜ì‹í’ˆ, ì£¼ë°©ìš©í’ˆ, ìƒí™œìš©í’ˆ
- ë³´í†µ (0.9): ì˜ë¥˜, ê°€ì „, ê±´ê°•ì‹í’ˆ
- ë¶€ì í•© (0.8): íŒ¨ì…˜ì¡í™”, ì‹ ë°œ
- ì˜ˆ: "ê°„í¸ì‹"(1.2), "ë„ì‹œë½"(1.2), "ì²­ì†Œìš©í’ˆ"(1.2), "íŒ¨ë”©"(0.9)

ğŸŒ¤ï¸ ì˜¤í›„ (14:00-17:59):
- ë§¤ìš° ì í•© (1.2): ê°€êµ¬/ì¹¨êµ¬, ìƒí™œìš©í’ˆ, ê°€ì „
- ë³´í†µ (1.0): ê±´ê°•ì‹í’ˆ, ì˜ë¥˜, ì‹í’ˆ
- ì˜ˆ: "ì¹¨ëŒ€"(1.2), "ë§¤íŠ¸ë¦¬ìŠ¤"(1.2), "ì²­ì†Œê¸°"(1.2), "íŒ¨ë”©"(1.0)

ğŸŒ™ ì €ë…/ë°¤ (18:00-05:59):
- ë§¤ìš° ì í•© (1.2): ì˜ë¥˜, íŒ¨ì…˜ì¡í™”/ë³´ì„, ì‹ ë°œ, í™”ì¥í’ˆ/ë·°í‹°
- ì í•© (1.1): ê±´ê°•ì‹í’ˆ, ê°€ì „, ê°€êµ¬/ì¹¨êµ¬
- ë³´í†µ (0.9): ì‹í’ˆ, ì£¼ë°©ìš©í’ˆ
- ì˜ˆ: "íŒ¨ë”©"(1.2), "ê¸°ëª¨"(1.2), "ëª©ë„ë¦¬"(1.2), "ìŠ¤í‚¨ì¼€ì–´"(1.2), "í™”ì¥í’ˆ"(1.2), "í™ì‚¼"(1.1)

**ê°€ì¤‘ì¹˜ ê·œì¹™:**
- 1.2: í•´ë‹¹ ì‹œê°„ëŒ€ì— ë§¤ìš° ì í•©í•œ ì¹´í…Œê³ ë¦¬
- 1.1: ì í•©í•œ ì¹´í…Œê³ ë¦¬
- 1.0: ë³´í†µ (ê¸°ë³¸ê°’)
- 0.9: ë‹¤ì†Œ ë¶€ì í•©
- 0.8: ë¶€ì í•©

JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜ (ê° í‚¤ì›Œë“œì— ê°€ì¤‘ì¹˜ í¬í•¨):
{{
  "keywords": [
    {{"keyword": "í‚¤ì›Œë“œ1", "weight": 1.2}},
    {{"keyword": "í‚¤ì›Œë“œ2", "weight": 1.0}},
    {{"keyword": "í‚¤ì›Œë“œ3", "weight": 0.9}}
  ],
  "expanded": {{
    "ì¶”ìƒí‚¤ì›Œë“œ1": ["êµ¬ì²´1", "êµ¬ì²´2", "êµ¬ì²´3"],
    "ì¶”ìƒí‚¤ì›Œë“œ2": ["êµ¬ì²´1", "êµ¬ì²´2"]
  }}
}}"""),
            ("human", """ë‚ ì§œ: {month}ì›” {day}ì¼
ë‚ ì”¨: {weather}
ê¸°ì˜¨: {temperature}ë„
ì‹œê°„ëŒ€: {time_slot}
ìš”ì¼ íƒ€ì…: {day_type}
ê³µíœ´ì¼: {holiday_name}

ìœ„ ìƒí™©ì— ì í•©í•œ ìƒí’ˆ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”. 
**íŠ¹íˆ ì‹œê°„ëŒ€({time_slot})ë¥¼ ê³ ë ¤í•´ì„œ í•´ë‹¹ ì‹œê°„ëŒ€ì— ì í•©í•œ ì¹´í…Œê³ ë¦¬ì˜ í‚¤ì›Œë“œë¥¼ ìš°ì„ ì ìœ¼ë¡œ ìƒì„±í•˜ì„¸ìš”!**""")
        ])
        
        chain = keyword_prompt | self.llm | JsonOutputParser()
        
        try:
            # í”„ë¡¬í”„íŠ¸ ë¡œê¹… (ëˆˆì— ë„ê²Œ)
            prompt_vars = {
                "month": month,
                "day": day,
                "weather": weather,
                "temperature": temperature,
                "time_slot": time_slot,
                "day_type": day_type,
                "holiday_name": holiday_name if holiday_name else "ì—†ìŒ"
            }
            print("=" * 80)
            print("[1ë‹¨ê³„ - LangChain í”„ë¡¬í”„íŠ¸] ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ ìƒì„±")
            print("=" * 80)
            print(f"ì…ë ¥ ë³€ìˆ˜:")
            for key, value in prompt_vars.items():
                print(f"  - {key}: {value}")
            print("=" * 80)
            logger.info(f"[1ë‹¨ê³„] ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜: {prompt_vars}")
            
            result = await chain.ainvoke({
                "month": month,
                "day": day,
                "weather": weather,
                "temperature": temperature,
                "time_slot": time_slot,
                "day_type": day_type,
                "holiday_name": holiday_name if holiday_name else "ì—†ìŒ"
            })
            
            # ê²°ê³¼ íŒŒì‹±
            keyword_weights = {}  # í‚¤ì›Œë“œë³„ ê°€ì¤‘ì¹˜ ì €ì¥
            
            if isinstance(result, dict):
                keywords_data = result.get("keywords", [])
                expansion_map = result.get("expanded", {})
                
                # í‚¤ì›Œë“œì™€ ê°€ì¤‘ì¹˜ ë¶„ë¦¬
                keywords = []
                for item in keywords_data:
                    if isinstance(item, dict):
                        kw = item.get("keyword", "")
                        weight = item.get("weight", 1.0)
                        keywords.append(kw)
                        keyword_weights[kw] = weight
                    else:
                        # í´ë°±: ë¬¸ìì—´ë¡œ ì˜¨ ê²½ìš°
                        keywords.append(item)
                        keyword_weights[item] = 1.0
            else:
                # í´ë°±: ë¦¬ìŠ¤íŠ¸ë¡œ ì˜¨ ê²½ìš°
                keywords = result if isinstance(result, list) else []
                expansion_map = {}
                for kw in keywords:
                    keyword_weights[kw] = 1.0
            
            print("=" * 80)
            print(f"[1ë‹¨ê³„ - ì‘ë‹µ] LLM ìƒì„± í‚¤ì›Œë“œ (ê°€ì¤‘ì¹˜ í¬í•¨):")
            for kw in keywords[:10]:
                weight = keyword_weights.get(kw, 1.0)
                print(f"  - {kw}: {weight}x")
            print(f"[1ë‹¨ê³„ - ê²°ê³¼] ì´ {len(keywords)}ê°œ í‚¤ì›Œë“œ")
            print("=" * 80)
            
            # í™•ì¥ í‚¤ì›Œë“œ ì²˜ë¦¬ ë° ë§¤í•‘ ìƒì„±
            expanded_keywords = []
            keyword_mapping = {}
            
            print(f"[1ë‹¨ê³„ - í™•ì¥] LLM í™•ì¥ ê²°ê³¼:")
            for original_kw in keywords:
                # ì›ë³¸ í‚¤ì›Œë“œ ì¶”ê°€ (ê°€ì¤‘ì¹˜ ìœ ì§€)
                expanded_keywords.append(original_kw)
                keyword_mapping[original_kw] = original_kw
                
                # í™•ì¥ëœ í‚¤ì›Œë“œ ì¶”ê°€
                if original_kw in expansion_map:
                    expanded_list = expansion_map[original_kw]
                    print(f"  ğŸ”„ '{original_kw}' â†’ {expanded_list}")
                    expanded_keywords.extend(expanded_list)
                    
                    # ë§¤í•‘ ì €ì¥
                    for exp_kw in expanded_list:
                        keyword_mapping[exp_kw] = original_kw
            
            # ì¤‘ë³µ ì œê±°
            expanded_keywords = list(dict.fromkeys(expanded_keywords))
            
            print("=" * 80)
            print(f"[1ë‹¨ê³„ - LLM í™•ì¥ ì™„ë£Œ] ì›ë³¸ {len(keywords)}ê°œ â†’ í™•ì¥ {len(expanded_keywords)}ê°œ")
            print(f"[1ë‹¨ê³„ - í™•ì¥ í‚¤ì›Œë“œ] {expanded_keywords}")
            print("=" * 80)
            
            # RAG ë°©ì‹: ì‹¤ì œ DB ìƒí’ˆëª… ê¸°ë°˜ í‚¤ì›Œë“œ ì¬í™•ì¥
            rag_keywords = await self._extract_keywords_from_actual_products(expanded_keywords)
            
            # RAG í‚¤ì›Œë“œë„ ë§¤í•‘ì— ì¶”ê°€ (ì›ë³¸ í‚¤ì›Œë“œë¡œ ì—­ì¶”ì )
            for rag_kw in rag_keywords:
                if rag_kw not in keyword_mapping:
                    # RAGë¡œ ì¶”ì¶œëœ í‚¤ì›Œë“œëŠ” ê°€ì¥ ê´€ë ¨ ìˆëŠ” ì›ë³¸ í‚¤ì›Œë“œë¡œ ë§¤í•‘
                    # ê°„ë‹¨í•˜ê²Œ ì²« ë²ˆì§¸ ì›ë³¸ í‚¤ì›Œë“œë¡œ ë§¤í•‘ (ê°œì„  ê°€ëŠ¥)
                    keyword_mapping[rag_kw] = keywords[0] if keywords else rag_kw
            
            # ìµœì¢… í‚¤ì›Œë“œ: ëª¨ë“  í‚¤ì›Œë“œ í†µí•© (ì¤‘ë³µ ì œê±°)
            final_keywords = []
            
            # 1ìˆœìœ„: ì›ë³¸ í‚¤ì›Œë“œ (LLM ìƒì„±)
            final_keywords.extend(keywords)
            
            # 2ìˆœìœ„: LLM í™•ì¥ í‚¤ì›Œë“œ
            for exp_kw in expanded_keywords:
                if exp_kw not in final_keywords:
                    final_keywords.append(exp_kw)
            
            # 3ìˆœìœ„: RAG í‚¤ì›Œë“œ (ëª¨ë‘ í¬í•¨)
            for rag_kw in rag_keywords:
                if rag_kw not in final_keywords:
                    final_keywords.append(rag_kw)
            
            # contextì— ë§¤í•‘ ì •ë³´ ë° ê°€ì¤‘ì¹˜ ì €ì¥
            context["keyword_mapping"] = keyword_mapping
            context["original_keywords"] = keywords
            context["keyword_weights"] = keyword_weights  # ì‹œê°„ëŒ€ë³„ ê°€ì¤‘ì¹˜
            
            print("=" * 80)
            print(f"[1ë‹¨ê³„ - ìµœì¢… ì™„ë£Œ] ì›ë³¸ {len(keywords)}ê°œ + í™•ì¥ {len(expanded_keywords)}ê°œ + RAG {len(rag_keywords)}ê°œ â†’ ìµœì¢… {len(final_keywords)}ê°œ")
            print(f"[í‚¤ì›Œë“œ í†µí•© ì™„ë£Œ]")
            print(f"  - ì›ë³¸: {keywords[:5]}...")
            print(f"  - í™•ì¥: {expanded_keywords[:5]}...")
            print(f"  - RAG: {rag_keywords[:5]}...")
            print(f"[1ë‹¨ê³„ - ìµœì¢… í‚¤ì›Œë“œ ìˆœì„œ] {final_keywords[:20]}...")
            print(f"[1ë‹¨ê³„ - ë§¤í•‘] {len(keyword_mapping)}ê°œ ë§¤í•‘ ì €ì¥")
            print("=" * 80)
            
            logger.info(f"[1ë‹¨ê³„] ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í‚¤ì›Œë“œ ìƒì„± ì™„ë£Œ: {keywords}")
            logger.info(f"[1ë‹¨ê³„] LLM í™•ì¥: {expanded_keywords}")
            logger.info(f"[1ë‹¨ê³„] RAG ì¶”ì¶œ: {rag_keywords[:10]}")
            logger.info(f"[1ë‹¨ê³„] ìµœì¢… í‚¤ì›Œë“œ: {final_keywords[:15]}")
            logger.info(f"[1ë‹¨ê³„] í‚¤ì›Œë“œ ë§¤í•‘: {len(keyword_mapping)}ê°œ")
            return final_keywords
        except Exception as e:
            logger.error(f"ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ ìƒì„± ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(f"ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
            # í´ë°±: ì‹œê°„ëŒ€/ê³„ì ˆ ê¸°ë°˜ ì‹¤ìš©ì  í‚¤ì›Œë“œ
            fallback_keywords = []
            
            # ì‹œê°„ëŒ€ë³„ í‚¤ì›Œë“œ
            if time_slot == "ì €ë…":
                fallback_keywords.extend(["ì €ë…ì‹ì‚¬", "ì‹¤ë‚´í™œë™", "íœ´ì‹", "ê°€ì¡±ì‹œê°„"])
            elif time_slot == "ì˜¤ì „":
                fallback_keywords.extend(["ì•„ì¹¨", "ì¶œê·¼", "í™œë ¥", "ê±´ê°•"])
            elif time_slot == "ì˜¤í›„":
                fallback_keywords.extend(["ì ì‹¬", "ì•¼ì™¸í™œë™", "ìš´ë™", "ì‡¼í•‘"])
            else:
                fallback_keywords.extend(["ë°¤", "ìˆ˜ë©´", "íœ´ì‹"])
            
            # ê³„ì ˆë³„ í‚¤ì›Œë“œ
            if season == "ê²¨ìš¸":
                fallback_keywords.extend(["ë”°ëœ»í•œ", "ë³´ì˜¨", "ë‚œë°©"])
            elif season == "ì—¬ë¦„":
                fallback_keywords.extend(["ì‹œì›í•œ", "ëƒ‰ë°©", "íœ´ê°€"])
            elif season == "ë´„":
                fallback_keywords.extend(["ì‹ ì„ í•œ", "ì•¼ì™¸", "ê½ƒ"])
            else:
                fallback_keywords.extend(["ê°€ì„", "ê±´ê°•", "í™˜ì ˆê¸°"])
            
            print(f"[1ë‹¨ê³„ - í´ë°±] í´ë°± í‚¤ì›Œë“œ ì‚¬ìš©: {fallback_keywords}")
            logger.info(f"[1ë‹¨ê³„] í´ë°± í‚¤ì›Œë“œ ì‚¬ìš©: {fallback_keywords}")
            logger.info(f"[1ë‹¨ê³„] í´ë°± í‚¤ì›Œë“œ ê°œìˆ˜: {len(fallback_keywords)}")
            return fallback_keywords
    
    # ì£¼ì„: _expand_keywords_to_product_terms í•¨ìˆ˜ëŠ” ì œê±°ë¨
    # ì´ì œ _generate_base_context_keywordsì—ì„œ í‚¤ì›Œë“œ ìƒì„±ê³¼ í™•ì¥ì„ í•œ ë²ˆì— ì²˜ë¦¬
    
    async def _extract_keywords_from_actual_products(self, trend_keywords: List[str]) -> List[str]:
        """
        RAG ë°©ì‹: ì‹¤ì œ DB ìƒí’ˆëª… ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        1. íŠ¸ë Œë“œ í‚¤ì›Œë“œë¡œ ëŠìŠ¨í•˜ê²Œ ê²€ìƒ‰
        2. ê²€ìƒ‰ëœ ì‹¤ì œ ìƒí’ˆëª… ë¶„ì„
        3. LLMìœ¼ë¡œ ìœ ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Returns:
            ì‹¤ì œ DBì— ì¡´ì¬í•˜ëŠ” ìƒí’ˆ ê¸°ë°˜ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        
        print("=" * 80)
        print("[RAG í‚¤ì›Œë“œ ì¶”ì¶œ] ì‹¤ì œ ìƒí’ˆëª… ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘")
        print("=" * 80)
        
        try:
            # 1ë‹¨ê³„: ëŠìŠ¨í•œ ê²€ìƒ‰ (ìƒìœ„ 5ê°œ í‚¤ì›Œë“œë§Œ ì‚¬ìš©)
            query = " ".join(trend_keywords[:5])
            print(f"[1ë‹¨ê³„] ëŠìŠ¨í•œ ê²€ìƒ‰ ì¿¼ë¦¬: {query}")
            
            search_results = self.product_embedder.search_products(
                trend_keywords=[query],
                top_k=30,  # ì¶©ë¶„í•œ ìƒ˜í”Œ
                score_threshold=0.25,  # ë§¤ìš° ë‚®ì€ threshold
                only_ready_products=True
            )
            
            if not search_results:
                print("[RAG] ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ - ì›ë³¸ í‚¤ì›Œë“œ ë°˜í™˜")
                return trend_keywords
            
            # 2ë‹¨ê³„: ì‹¤ì œ ìƒí’ˆëª… ì¶”ì¶œ
            actual_product_names = [
                result.get("product_name", "")
                for result in search_results[:20]  # ìƒìœ„ 20ê°œë§Œ
            ]
            
            print(f"[2ë‹¨ê³„] ê²€ìƒ‰ëœ ìƒí’ˆ {len(actual_product_names)}ê°œ:")
            for i, name in enumerate(actual_product_names[:5], 1):
                print(f"  {i}. {name[:50]}")
            
            # 3ë‹¨ê³„: LLMìœ¼ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
            extraction_prompt = ChatPromptTemplate.from_messages([
                ("system", """ë‹¹ì‹ ì€ í™ˆì‡¼í•‘ ìƒí’ˆ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ì„ë¬´**: ì‹¤ì œ DB ìƒí’ˆëª…ë“¤ì„ ë¶„ì„í•´ì„œ ê²€ìƒ‰ì— ìœ ìš©í•œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

**ì¶”ì¶œ ê·œì¹™**:
1. ë¸Œëœë“œëª… ì¶”ì¶œ (ì˜ˆ: "ì¿ ì¿ ", "í•„ë¦½ìŠ¤", "ë½í† í•", "ì¢…ê·¼ë‹¹ê±´ê°•")
2. ìƒí’ˆ ì¹´í…Œê³ ë¦¬ (ì˜ˆ: "ì••ë ¥ì†¥", "ì—ì–´í”„ë¼ì´ì–´", "ìœ ì‚°ê· ", "ì˜¤ë©”ê°€3")
3. í•µì‹¬ í‚¤ì›Œë“œ (ì˜ˆ: "IH", "XXL", "í”„ë¡œë°”ì´ì˜¤í‹±ìŠ¤", "ì•Œí‹°ì§€")
4. ì¤‘ë³µ ì œê±°

**ì œì™¸ ê·œì¹™ (ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”)**:
- âŒ ìˆ«ì+ë‹¨ìœ„ ì¡°í•©: "12ê°œì›”", "15ê°œì›”", "3ë°•ìŠ¤", "6í†µ", "18ë°•ìŠ¤" ë“±
- âŒ ìˆœìˆ˜ ìˆ«ì: "12", "15", "3" ë“±
- âŒ ì ‘ë‘ì‚¬/ì ‘ë¯¸ì‚¬: "ì§_", "ë‹¨_", "ì„¸ì¼_", "[ì„¸ì¼]", "[í™˜ì›]" ë“±
- âŒ ì˜ë¯¸ì—†ëŠ” ë‹¨ì–´: "ê°œì›”", "ë°•ìŠ¤", "í†µ", "ê°œì›”ë¶„" ë“±

**ì˜ˆì‹œ**:
ìƒí’ˆëª…: "ì¢…ê·¼ë‹¹ê±´ê°• í”„ë¡œë©”ê°€ ì•Œí‹°ì§€ë¹„íƒ€ë¯¼D 12ê°œì›”"
ì¶”ì¶œ: ["ì¢…ê·¼ë‹¹ê±´ê°•", "í”„ë¡œë©”ê°€", "ì•Œí‹°ì§€", "ë¹„íƒ€ë¯¼D", "ì˜¤ë©”ê°€3"]
(âŒ "12ê°œì›”" ì œì™¸)

ìƒí’ˆëª…: "[ì„¸ì¼]ì•ˆêµ­ê±´ê°• ì´ˆì„ê³„ ì•Œí‹°ì§€ì˜¤ë©”ê°€3 12ê°œì›”"
ì¶”ì¶œ: ["ì•ˆêµ­ê±´ê°•", "ì´ˆì„ê³„", "ì•Œí‹°ì§€", "ì˜¤ë©”ê°€3"]
(âŒ "[ì„¸ì¼]", "12ê°œì›”" ì œì™¸)

JSON í˜•ì‹:
{{"keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", ...]}}""")
,
                ("human", """íŠ¸ë Œë“œ í‚¤ì›Œë“œ: {trend_keywords}

ìš°ë¦¬ DBì—ì„œ ê²€ìƒ‰ëœ ì‹¤ì œ ìƒí’ˆëª…ë“¤:
{product_names}

ìœ„ ìƒí’ˆëª…ë“¤ì„ ë¶„ì„í•´ì„œ ê²€ìƒ‰ì— ìœ ìš©í•œ í‚¤ì›Œë“œ 15-20ê°œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.""")
            ])
            
            chain = extraction_prompt | self.llm | JsonOutputParser()
            
            result = await chain.ainvoke({
                "trend_keywords": ", ".join(trend_keywords[:5]),
                "product_names": "\n".join([f"{i+1}. {name}" for i, name in enumerate(actual_product_names)])
            })
            
            extracted_keywords = result.get("keywords", [])
            
            # í›„ì²˜ë¦¬: ì˜ë¯¸ì—†ëŠ” í‚¤ì›Œë“œ í•„í„°ë§
            import re
            invalid_patterns = [
                r'^\d+ê°œì›”ë¶„?$',      # "12ê°œì›”", "15ê°œì›”ë¶„"
                r'^\d+ë°•ìŠ¤$',         # "3ë°•ìŠ¤", "18ë°•ìŠ¤"
                r'^\d+í†µ$',           # "6í†µ"
                r'^\d+$',             # ìˆœìˆ˜ ìˆ«ì
                r'^[\[\(].*[\]\)]$',  # "[ì„¸ì¼]", "(í™˜ì›)" ë“±
                r'^ì§_',              # "ì§_" ì ‘ë‘ì‚¬
                r'^ë‹¨_',              # "ë‹¨_" ì ‘ë‘ì‚¬
                r'^ì„¸ì¼_',            # "ì„¸ì¼_" ì ‘ë‘ì‚¬
            ]
            
            filtered_keywords = []
            removed_keywords = []
            for kw in extracted_keywords:
                is_invalid = False
                for pattern in invalid_patterns:
                    if re.match(pattern, kw):
                        is_invalid = True
                        removed_keywords.append(kw)
                        break
                if not is_invalid and len(kw) >= 2:  # ìµœì†Œ 2ê¸€ì
                    filtered_keywords.append(kw)
            
            if removed_keywords:
                print(f"[3ë‹¨ê³„ - í•„í„°ë§] ì œê±°ëœ í‚¤ì›Œë“œ: {removed_keywords}")
            
            print("=" * 80)
            print(f"[3ë‹¨ê³„] LLM ì¶”ì¶œ ì™„ë£Œ: {len(extracted_keywords)}ê°œ â†’ í•„í„°ë§ í›„ {len(filtered_keywords)}ê°œ")
            print(f"[ì¶”ì¶œ í‚¤ì›Œë“œ] {filtered_keywords[:10]}...")
            print("=" * 80)
            
            return filtered_keywords
            
        except Exception as e:
            logger.error(f"RAG í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(f"ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
            
            # í´ë°±: ì›ë³¸ í‚¤ì›Œë“œ ë°˜í™˜
            print(f"[RAG ì‹¤íŒ¨] ì›ë³¸ í‚¤ì›Œë“œ ì‚¬ìš©: {trend_keywords}")
            return trend_keywords
    
    async def _generate_context_keywords(self, context: Dict[str, Any]) -> List[str]:
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í‚¤ì›Œë“œ ìƒì„± (1ë‹¨ê³„ë§Œ - 2ë‹¨ê³„ëŠ” ìƒìœ„ì—ì„œ ë³„ë„ í˜¸ì¶œ)"""
        
        print("=" * 80)
        print("[í†µí•© í‚¤ì›Œë“œ ìƒì„±] 1ë‹¨ê³„: ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ")
        print("=" * 80)
        
        # 1ë‹¨ê³„: ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ (ë‚ ì”¨, ì‹œê°„ëŒ€, ê³„ì ˆ, ê³µíœ´ì¼)
        base_keywords = await self._generate_base_context_keywords(context)
        logger.info(f"1ë‹¨ê³„ ê¸°ë³¸ í‚¤ì›Œë“œ: {base_keywords}")
        
        # 2ë‹¨ê³„(ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰)ëŠ” _collect_context_and_keywords()ì—ì„œ ë³„ë„ í˜¸ì¶œë¨
        # ì—¬ê¸°ì„œëŠ” 1ë‹¨ê³„ ê²°ê³¼ë§Œ ë°˜í™˜
        
        print("=" * 80)
        print(f"[1ë‹¨ê³„ ì™„ë£Œ] ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ {len(base_keywords)}ê°œ")
        print(f"  í‚¤ì›Œë“œ: {base_keywords[:10]}...")
        print("=" * 80)
        logger.info(f"ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ ìƒì„± ì™„ë£Œ: {base_keywords[:20]}")
        
        return base_keywords
    
    async def _generate_unified_candidates(
        self,
        search_result: Dict[str, Any],
        context: Dict[str, Any],
        max_trend_match: int = 8,  # ìœ ì‚¬ë„ ê¸°ë°˜ ìµœëŒ€ ê°œìˆ˜ (ì˜ë¥˜ í¸ì¤‘ ë°©ì§€)
        max_sales_prediction: int = 32  # ë§¤ì¶œì˜ˆì¸¡ ê¸°ë°˜ ìµœëŒ€ ê°œìˆ˜ (ë‹¤ì–‘ì„± í™•ë³´)
    ) -> tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """í†µí•© í›„ë³´êµ° ìƒì„± - Track A (í‚¤ì›Œë“œ ë§¤ì¹­) + Track B (ë§¤ì¶œ ìƒìœ„) + Track C (ê³¼ê±° ì‹¤ì ) ë³‘í•©"""
        
        candidates = []
        seen_products = set()
        predicted_sales_cache = {}  # XGBoost ì˜ˆì¸¡ ìºì‹œ (ì¤‘ë³µ ì˜ˆì¸¡ ë°©ì§€)
        
        broadcast_dt = context["broadcast_dt"]
        target_month = broadcast_dt.month
        target_hour = broadcast_dt.hour
        
        print(f"=== [DEBUG Unified Candidates] í›„ë³´êµ° ìƒì„± ì‹œì‘ (ëª©í‘œ: ìµœëŒ€ {max_trend_match + max_sales_prediction}ê°œ) ===")
        print(f"=== [DEBUG] ëŒ€ìƒ ì‹œê°„: {target_month}ì›” {target_hour}ì‹œ ===")
        
        # ========== Track A: í‚¤ì›Œë“œ ë§¤ì¹­ ìƒí’ˆ ==========
        all_products = []
        # Track A ìƒí’ˆë“¤ì— source_tracks ì´ˆê¸°í™”
        for product in search_result["direct_products"]:
            product["source_tracks"] = ["keyword"]  # í‚¤ì›Œë“œ ë§¤ì¹­
        all_products.extend(search_result["direct_products"])  # ê³ ìœ ì‚¬ë„ ìƒí’ˆ
        
        # ì¹´í…Œê³ ë¦¬ ê·¸ë£¹ì˜ ëª¨ë“  ìƒí’ˆë„ ì¶”ê°€
        for category, products in search_result["category_groups"].items():
            for product in products:
                product["source_tracks"] = ["keyword"]  # í‚¤ì›Œë“œ ë§¤ì¹­
            all_products.extend(products)
        
        print(f"=== [Track A] í‚¤ì›Œë“œ ë§¤ì¹­ ìƒí’ˆ: {len(all_products)}ê°œ ===")
        
        # ========== Track B: ë§¤ì¶œ ì˜ˆì¸¡ ìƒìœ„ ìƒí’ˆ (í‚¤ì›Œë“œ ë¬´ê´€) ==========
        sales_top_products = await self._get_sales_top_products(context, limit=20)
        print(f"=== [Track B] ë§¤ì¶œ ì˜ˆì¸¡ ìƒìœ„ ìƒí’ˆ: {len(sales_top_products)}ê°œ ===")
        
        # Track B ì˜ˆì¸¡ê°’ ìºì‹œì— ì €ì¥ (ì¤‘ë³µ ì˜ˆì¸¡ ë°©ì§€)
        for product in sales_top_products:
            product_code = product.get("product_code")
            if product_code and "predicted_sales" in product:
                predicted_sales_cache[product_code] = product["predicted_sales"]
        
        # Track A + Track B ë³‘í•© (ì—¬ëŸ¬ ì¶œì²˜ ë³‘í•©)
        for product in sales_top_products:
            product_code = product.get("product_code")
            existing = next((p for p in all_products if p.get("product_code") == product_code), None)
            if existing:
                # ê¸°ì¡´ ìƒí’ˆì— ì¶œì²˜ ì¶”ê°€ (ë¦¬ìŠ¤íŠ¸ë¡œ ê´€ë¦¬)
                if "source_tracks" not in existing:
                    existing["source_tracks"] = [existing.get("source_track", "keyword")]
                if "sales_top" not in existing["source_tracks"]:
                    existing["source_tracks"].append("sales_top")
            else:
                product["source_track"] = "sales_top"
                product["source_tracks"] = ["sales_top"]
                all_products.append(product)
        
        print(f"=== [DEBUG] Track A + B í†µí•©: {len(all_products)}ê°œ ===")
        
        # ========== Track C: ê³¼ê±° ìœ ì‚¬ ì‹œê°„ëŒ€/ì›” íŒë§¤ ì‹¤ì  ìƒí’ˆ ==========
        historical_products = self.product_embedder.get_historical_top_products(
            target_month=target_month,
            target_hour=target_hour,
            month_range=1,  # Â±1ê°œì›” (ì˜ˆ: 12ì›” â†’ 11~1ì›”)
            hour_range=1,   # Â±1ì‹œê°„ (ì˜ˆ: 9ì‹œ â†’ 8~10ì‹œ)
            limit=20
        )
        print(f"=== [Track C] ê³¼ê±° ìœ ì‚¬ ì¡°ê±´ ìƒí’ˆ: {len(historical_products)}ê°œ (ì›”: {target_month}Â±1, ì‹œê°„: {target_hour}Â±1) ===")
        
        # Track C ìƒí’ˆ ë³‘í•© (ì—¬ëŸ¬ ì¶œì²˜ ë³‘í•©)
        track_c_added = 0
        track_c_updated = 0
        for product in historical_products:
            product_code = product.get("product_code")
            existing = next((p for p in all_products if p.get("product_code") == product_code), None)
            if existing:
                # ê¸°ì¡´ ìƒí’ˆì— ì¶œì²˜ ì¶”ê°€
                if "source_tracks" not in existing:
                    existing["source_tracks"] = [existing.get("source_track", "keyword")]
                if "historical" not in existing["source_tracks"]:
                    existing["source_tracks"].append("historical")
                existing["historical_avg_profit"] = product.get("historical_avg_profit", 0)
                existing["historical_broadcast_count"] = product.get("historical_broadcast_count", 0)
                track_c_updated += 1
            else:
                product["source_track"] = "historical"
                product["source_tracks"] = ["historical"]
                all_products.append(product)
                track_c_added += 1
        
        print(f"=== [DEBUG] Track A + B + C í†µí•©: {len(all_products)}ê°œ (Track C ì‹ ê·œ: {track_c_added}ê°œ, ì—…ë°ì´íŠ¸: {track_c_updated}ê°œ) ===")
        
        # ë³µí•© ì¶œì²˜ ìƒí’ˆ ë””ë²„ê·¸
        multi_source_products = [p for p in all_products if len(p.get("source_tracks", [])) > 1]
        if multi_source_products:
            print(f"=== [DEBUG] ë³µí•© ì¶œì²˜ ìƒí’ˆ {len(multi_source_products)}ê°œ ===")
            for p in multi_source_products[:3]:
                print(f"  - {p.get('product_name', '')[:25]}: {p.get('source_tracks', [])}")
        
        # ========== Track D: ê²½ìŸì‚¬ í¸ì„± ê¸°ë°˜ RAG ê²€ìƒ‰ ==========
        competitor_products = await self._get_competitor_based_products(context, limit=15)
        print(f"=== [Track D] ê²½ìŸì‚¬ ëŒ€ì‘ ìƒí’ˆ: {len(competitor_products)}ê°œ ===")
        
        # Track D ìƒí’ˆ ë³‘í•© (ì—¬ëŸ¬ ì¶œì²˜ ë³‘í•©)
        track_d_added = 0
        track_d_updated = 0
        for product in competitor_products:
            product_code = product.get("product_code")
            existing = next((p for p in all_products if p.get("product_code") == product_code), None)
            if existing:
                # ê¸°ì¡´ ìƒí’ˆì— ì¶œì²˜ ì¶”ê°€
                if "source_tracks" not in existing:
                    existing["source_tracks"] = [existing.get("source_track", "keyword")]
                if "competitor" not in existing["source_tracks"]:
                    existing["source_tracks"].append("competitor")
                existing["competitor_info"] = product.get("competitor_info", {})
                track_d_updated += 1
            else:
                product["source_track"] = "competitor"
                product["source_tracks"] = ["competitor"]
                all_products.append(product)
                track_d_added += 1
        
        print(f"=== [DEBUG] Track A + B + C + D í†µí•©: {len(all_products)}ê°œ (Track D ì‹ ê·œ: {track_d_added}ê°œ, ì—…ë°ì´íŠ¸: {track_d_updated}ê°œ) ===")
        
        # 2. ì¤‘ë³µ ì œê±° ì „ ì •ë ¬ (Track B/C/D ìƒí’ˆ ìš°ì„ )
        # ìš°ì„ ìˆœìœ„: competitor > sales_top > historical > keyword
        def get_track_priority(product):
            tracks = product.get("source_tracks", [])
            if "competitor" in tracks:
                return 0
            elif "sales_top" in tracks:
                return 1
            elif "historical" in tracks:
                return 2
            else:
                return 3
        
        all_products.sort(key=get_track_priority)
        
        # 2. ì¤‘ë³µ ì œê±° (ìƒí’ˆì½”ë“œ + ì†Œë¶„ë¥˜ + ë¸Œëœë“œ) - Track B/C/D ìƒí’ˆ ìš°ì„  ìœ ì§€
        unique_products = {}
        seen_category_brand_pairs = set()  # (ì†Œë¶„ë¥˜, ë¸Œëœë“œ) ì¡°í•©
        
        for product in all_products:
            product_code = product.get("product_code")
            category_sub = product.get("category_sub", "")
            brand = product.get("brand", "")
            
            # ìƒí’ˆì½”ë“œ ì¤‘ë³µ ì²´í¬
            if product_code in unique_products:
                continue
            
            # ì†Œë¶„ë¥˜ + ë¸Œëœë“œ ì¡°í•© ì¤‘ë³µ ì²´í¬ (ë‹¤ì–‘ì„± ë³´ì¥)
            category_brand_key = (category_sub, brand)
            if category_sub and brand and category_brand_key in seen_category_brand_pairs:
                logger.info(f"ì†Œë¶„ë¥˜+ë¸Œëœë“œ ì¤‘ë³µ ì œì™¸: {product.get('product_name', '')[:30]} (ì†Œë¶„ë¥˜: {category_sub}, ë¸Œëœë“œ: {brand})")
                continue
            
            # í†µê³¼í•œ ê²½ìš° ì¶”ê°€
            unique_products[product_code] = product
            if category_sub and brand:
                seen_category_brand_pairs.add(category_brand_key)
        
        print(f"=== [DEBUG] ì¤‘ë³µ ì œê±° í›„: {len(unique_products)}ê°œ (ì†Œë¶„ë¥˜+ë¸Œëœë“œ ë‹¤ì–‘ì„± ë³´ì¥) ===")
        
        # 3. ë°°ì¹˜ ì˜ˆì¸¡ ì¤€ë¹„ (ìƒìœ„ 50ê°œë¡œ í™•ëŒ€)
        products_list = list(unique_products.values())[:50]
        print(f"=== [DEBUG] ë°°ì¹˜ ì˜ˆì¸¡ ëŒ€ìƒ: {len(products_list)}ê°œ ===")
        
        # 4. ë°°ì¹˜ XGBoost ì˜ˆì¸¡ (ìºì‹œ í™œìš©ìœ¼ë¡œ ì¤‘ë³µ ì˜ˆì¸¡ ë°©ì§€)
        # Track Bì—ì„œ ì´ë¯¸ ì˜ˆì¸¡ëœ ìƒí’ˆì€ ìºì‹œì—ì„œ ê°€ì ¸ì˜¤ê³ , ë‚˜ë¨¸ì§€ë§Œ ìƒˆë¡œ ì˜ˆì¸¡
        products_to_predict = []
        cached_indices = {}  # {index: cached_sales}
        
        for i, product in enumerate(products_list):
            product_code = product.get("product_code")
            if product_code in predicted_sales_cache:
                cached_indices[i] = predicted_sales_cache[product_code]
            else:
                products_to_predict.append((i, product))
        
        print(f"=== [DEBUG] ìºì‹œ í™œìš©: {len(cached_indices)}ê°œ ìºì‹œë¨, {len(products_to_predict)}ê°œ ìƒˆë¡œ ì˜ˆì¸¡ ===")
        
        # ìƒˆë¡œ ì˜ˆì¸¡í•  ìƒí’ˆë§Œ ë°°ì¹˜ ì˜ˆì¸¡
        predicted_sales_list = [0.0] * len(products_list)
        
        # ìºì‹œëœ ê°’ ë¨¼ì € ì±„ìš°ê¸°
        for idx, cached_sales in cached_indices.items():
            predicted_sales_list[idx] = cached_sales
        
        # ìƒˆë¡œ ì˜ˆì¸¡í•  ìƒí’ˆì´ ìˆìœ¼ë©´ ë°°ì¹˜ ì˜ˆì¸¡
        if products_to_predict:
            new_products = [p for _, p in products_to_predict]
            new_predictions = await self._predict_products_sales_batch(new_products, context)
            
            for j, (idx, _) in enumerate(products_to_predict):
                predicted_sales_list[idx] = new_predictions[j]
        
        # 5. ì˜ˆì¸¡ ê²°ê³¼ì™€ ìƒí’ˆ ë§¤ì¹­ + ì ìˆ˜ ê³„ì‚° + ì¶œì²˜ ì •ë³´ ìˆ˜ì§‘
        # ë‰´ìŠ¤ ì¶œì²˜ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
        news_sources = context.get("news_sources", {})
        realtime_trends = context.get("realtime_trends", [])
        ai_trends = context.get("ai_trends", [])
        context_keywords = context.get("context_keywords", [])  # ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í‚¤ì›Œë“œ
        keyword_mapping = context.get("keyword_mapping", {})
        unified_keywords = context.get("unified_keywords", [])
        
        print(f"=== [ì¶œì²˜ ì¶”ì ] ai_trends: {ai_trends[:5]}... ===")
        print(f"=== [ì¶œì²˜ ì¶”ì ] realtime_trends: {realtime_trends[:5]}... ===")
        print(f"=== [ì¶œì²˜ ì¶”ì ] news_sources keys: {list(news_sources.keys())[:5]}... ===")
        
        for i, product in enumerate(products_list):
            similarity = product.get("similarity_score", 0.5)
            predicted_sales = predicted_sales_list[i]
            matched_keyword = product.get("matched_keyword", "")
            
            # ì¶”ì²œ ì¶œì²˜ ì •ë³´ ìˆ˜ì§‘
            recommendation_sources = []
            
            # í‚¤ì›Œë“œ ì¶œì²˜ íŒë³„
            keyword_source_type = "unknown"
            keyword_source_detail = ""
            
            if matched_keyword:
                # ì¶œì²˜ íŒë³„: ë‰´ìŠ¤ > AI íŠ¸ë Œë“œ > ì»¨í…ìŠ¤íŠ¸ ìˆœì„œë¡œ í™•ì¸
                if matched_keyword in news_sources:
                    keyword_source_type = "news"
                    keyword_source_detail = f"ë‰´ìŠ¤ íŠ¸ë Œë“œì—ì„œ ì¶”ì¶œ"
                elif matched_keyword in realtime_trends:
                    keyword_source_type = "news"
                    keyword_source_detail = f"ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ íŠ¸ë Œë“œ"
                elif matched_keyword in ai_trends:
                    keyword_source_type = "ai"
                    keyword_source_detail = f"AIê°€ {context.get('time_slot', '')} {context.get('season', '')} ì‹œì¦Œì— ë§ê²Œ ìƒì„±"
                elif matched_keyword in context_keywords:
                    keyword_source_type = "context"
                    keyword_source_detail = f"ë‚ ì”¨/ì‹œê°„ëŒ€ ê¸°ë°˜ ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ"
                else:
                    keyword_source_type = "ai"  # ê¸°ë³¸ê°’: AI íŠ¸ë Œë“œë¡œ ê°„ì£¼
                    keyword_source_detail = f"AI íŠ¸ë Œë“œ í‚¤ì›Œë“œ"
                
                print(f"  [ì¶œì²˜] {matched_keyword} â†’ {keyword_source_type} ({keyword_source_detail})")
            
            # 1. RAG ë§¤ì¹­ ì¶œì²˜ (í‚¤ì›Œë“œ ì¶œì²˜ ì •ë³´ í¬í•¨)
            if matched_keyword:
                rag_source = {
                    "source_type": "rag_match",
                    "matched_keyword": matched_keyword,
                    "similarity_score": similarity,
                    "keyword_origin": keyword_source_type,  # í‚¤ì›Œë“œê°€ ì–´ë””ì„œ ì™”ëŠ”ì§€
                    "keyword_origin_detail": keyword_source_detail
                }
                recommendation_sources.append(rag_source)
                
                # 2. ë‰´ìŠ¤ íŠ¸ë Œë“œ ì¶œì²˜ (ë§¤ì¹­ëœ í‚¤ì›Œë“œê°€ ë‰´ìŠ¤ì—ì„œ ì˜¨ ê²½ìš°)
                if matched_keyword in news_sources:
                    news_info = news_sources[matched_keyword]
                    news_source = {
                        "source_type": "news_trend",
                        "news_keyword": matched_keyword,
                        "news_title": news_info.get("news_title", ""),
                        "news_url": news_info.get("news_url", "")
                    }
                    recommendation_sources.append(news_source)
                
                # 3. AI íŠ¸ë Œë“œ ì¶œì²˜ (ë§¤ì¹­ëœ í‚¤ì›Œë“œê°€ AI ìƒì„±ì¸ ê²½ìš°)
                if keyword_source_type == "ai" or matched_keyword in ai_trends:
                    ai_source = {
                        "source_type": "ai_trend",
                        "ai_keyword": matched_keyword,
                        "ai_reason": f"{context.get('time_slot', '')} ì‹œê°„ëŒ€ {context.get('season', '')} ì‹œì¦Œ íŠ¸ë Œë“œ ë¶„ì„ìœ¼ë¡œ ìƒì„±"
                    }
                    recommendation_sources.append(ai_source)
            
            # 4. XGBoost ë§¤ì¶œ ì˜ˆì¸¡ ì¶œì²˜ (í•­ìƒ ì¶”ê°€)
            xgboost_source = {
                "source_type": "xgboost_sales",
                "xgboost_rank": i + 1,
                "predicted_sales": predicted_sales
            }
            recommendation_sources.append(xgboost_source)
            
            # 5. Track B ì¶œì²˜ (ë§¤ì¶œ ì˜ˆì¸¡ ìƒìœ„ - í‚¤ì›Œë“œ ë¬´ê´€)
            if product.get("source_track") == "sales_top":
                sales_top_source = {
                    "source_type": "sales_top",
                    "reason": "í‚¤ì›Œë“œ ë¬´ê´€ ë§¤ì¶œ ì˜ˆì¸¡ ìƒìœ„ ìƒí’ˆ"
                }
                recommendation_sources.append(sales_top_source)
            
            # 6. Track C ì¶œì²˜ (ê³¼ê±° ìœ ì‚¬ ì‹œê°„ëŒ€/ì›” íŒë§¤ ì‹¤ì )
            if product.get("source_track") == "historical":
                historical_source = {
                    "source_type": "historical",
                    "reason": f"ê³¼ê±° {target_month}ì›”Â±1, {target_hour}ì‹œÂ±1 ì‹œê°„ëŒ€ì— ì‹¤ì œë¡œ ì˜ íŒ”ë¦° ìƒí’ˆ",
                    "historical_avg_profit": product.get("historical_avg_profit", 0),
                    "historical_broadcast_count": product.get("historical_broadcast_count", 0)
                }
                recommendation_sources.append(historical_source)
            
            # 7. Track D ì¶œì²˜ (ê²½ìŸì‚¬ í¸ì„± ëŒ€ì‘)
            if product.get("source_track") == "competitor":
                comp_info = product.get("competitor_info", {})
                competitor_source = {
                    "source_type": "competitor",
                    "competitor_company": comp_info.get("company", ""),
                    "competitor_title": comp_info.get("title", ""),
                    "competitor_keyword": product.get("matched_keyword", ""),
                    "reason": f"ê²½ìŸì‚¬ {comp_info.get('company', '')} ë™ì‹œê°„ëŒ€ í¸ì„± ëŒ€ì‘"
                }
                recommendation_sources.append(competitor_source)
            
            # 8. ì»¨í…ìŠ¤íŠ¸ ì¶œì²˜ (ë‚ ì”¨, ì‹œê°„ëŒ€ ë“±)
            context_factors = []
            if context.get("weather", {}).get("weather"):
                context_factors.append(f"ë‚ ì”¨: {context['weather']['weather']}")
            if context.get("time_slot"):
                context_factors.append(f"ì‹œê°„ëŒ€: {context['time_slot']}")
            if context.get("holiday_name"):
                context_factors.append(f"ê³µíœ´ì¼: {context['holiday_name']}")
            
            if context_factors:
                context_source = {
                    "source_type": "context",
                    "context_factor": ", ".join(context_factors)
                }
                recommendation_sources.append(context_source)
            
            # ì ìˆ˜ ê³„ì‚° (Trackë³„ ê°€ì‚°ì  ì ìš© - ì—¬ëŸ¬ ì¶œì²˜ í•©ì‚°)
            # ê¸°ë³¸ ì ìˆ˜: ë§¤ì¶œ ì˜ˆì¸¡ ê¸°ë°˜ (ìœ ì‚¬ë„ ë¹„ì¤‘ ëŒ€í­ ì¶•ì†Œ)
            base_score = (
                similarity * 0.2 +  # ìœ ì‚¬ë„ 20% (AIë¶„ì„ ë¹„ì¤‘ ì¶•ì†Œ)
                (predicted_sales / 100000000) * 0.8  # ë§¤ì¶œ 80% (ì •ê·œí™”: 1ì–µ ê¸°ì¤€)
            )
            
            # ì—¬ëŸ¬ ì¶œì²˜ì—ì„œ ì¶”ì²œëœ ê²½ìš° ê°€ì‚°ì  í•©ì‚°
            source_tracks = product.get("source_tracks", [])
            track_bonus = 0.0
            source_labels = []  # ì¶œì²˜ ë¼ë²¨ ë¦¬ìŠ¤íŠ¸
            
            # Trackë³„ ê°€ì‚°ì  (ì—¬ëŸ¬ ì¶œì²˜ë©´ í•©ì‚°)
            if "competitor" in source_tracks:
                track_bonus += 0.15
                source_labels.append("ê²½ìŸì‚¬")
            if "sales_top" in source_tracks:
                track_bonus += 0.12
                source_labels.append("ë§¤ì¶œìƒìœ„")
            if "historical" in source_tracks:
                track_bonus += 0.10
                source_labels.append("ê³¼ê±°ì‹¤ì ")
            
            # keyword ì¶œì²˜ (ë‰´ìŠ¤ ë˜ëŠ” AIë¶„ì„) - í•­ìƒ í‘œì‹œ
            # ë‰´ìŠ¤ ê¸°ì¤€: ìœ ì‚¬ë„ 0.50 ì´ìƒ (ì‹¤ì œ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ë§¤ì¹­)
            if "keyword" in source_tracks:
                if similarity >= 0.50:
                    track_bonus += 0.08
                    source_labels.append("ë‰´ìŠ¤")
                else:
                    # AIë¶„ì„ë„ ë³µí•© ì¶œì²˜ë¡œ í‘œì‹œ (ê°€ì‚°ì  ì—†ìŒ)
                    source_labels.append("AIë¶„ì„")
            
            # ì¶œì²˜ê°€ ì—†ìœ¼ë©´ AIë¶„ì„ (íŒ¨ë„í‹°)
            if not source_labels:
                track_bonus = -0.05
                source_labels.append("AIë¶„ì„")
            
            # ë³µí•© ì¶œì²˜ ê°€ì‚°ì  (2ê°œ ì´ìƒ ì¶œì²˜ë©´ ì¶”ê°€ ê°€ì‚°ì )
            if len(source_labels) >= 2:
                track_bonus += 0.05 * (len(source_labels) - 1)  # ì¶œì²˜ 1ê°œ ì¶”ê°€ë‹¹ 0.05
            
            # ëŒ€í‘œ source ê²°ì • (ìš°ì„ ìˆœìœ„: ê²½ìŸì‚¬ > ë§¤ì¶œìƒìœ„ > ê³¼ê±°ì‹¤ì  > ë‰´ìŠ¤ > AIë¶„ì„)
            if "competitor" in source_tracks:
                source = "competitor"
            elif "sales_top" in source_tracks:
                source = "sales_top"
            elif "historical" in source_tracks:
                source = "historical"
            elif similarity >= 0.7:
                source = "news_trend"
            else:
                source = "ai_trend"
            
            final_score = base_score + track_bonus
            
            # source_labelsë¥¼ productì— ì €ì¥ (ì¶”ì²œ ê·¼ê±° ìƒì„±ìš©)
            product["source_labels"] = source_labels
            
            print(f"  [{'/'.join(source_labels)}] {product.get('product_name')[:20]}: ìœ ì‚¬ë„={similarity:.2f}, ë§¤ì¶œ={predicted_sales/10000:.0f}ë§Œì›, ê°€ì‚°ì ={track_bonus:.2f}, ì ìˆ˜={final_score:.3f}")
            
            candidates.append({
                "product": product,
                "source": source,
                "similarity_score": similarity,
                "predicted_sales": predicted_sales,
                "final_score": final_score,
                "recommendation_sources": recommendation_sources  # ì¶”ì²œ ì¶œì²˜ ì •ë³´ ì¶”ê°€
            })
        
        # 4. ì ìˆ˜ìˆœ ì •ë ¬
        candidates.sort(key=lambda x: x["final_score"], reverse=True)
        
        print(f"=== [DEBUG] ì´ {len(candidates)}ê°œ í›„ë³´ ìƒì„± ì™„ë£Œ, ì ìˆ˜ìˆœ ì •ë ¬ë¨ ===")
        
        # 4-1. Trackë³„ ìµœì†Œ ì¿¼í„° ë³´ì¥ (source_labels ê¸°ë°˜)
        # ë¼ë²¨ â†’ ì¿¼í„° ë§¤í•‘
        label_quotas = {"ê²½ìŸì‚¬": 2, "ë§¤ì¶œìƒìœ„": 2, "ê³¼ê±°ì‹¤ì ": 2, "ë‰´ìŠ¤": 2}
        label_counts = {"ê²½ìŸì‚¬": 0, "ë§¤ì¶œìƒìœ„": 0, "ê³¼ê±°ì‹¤ì ": 0, "ë‰´ìŠ¤": 0, "AIë¶„ì„": 0}
        
        final_candidates = []
        remaining_candidates = []
        
        # ë¨¼ì € ê° ë¼ë²¨ë³„ë¡œ ì¿¼í„°ë§Œí¼ ì„ íƒ (source_labels ê¸°ë°˜)
        for candidate in candidates:
            source_labels = candidate["product"].get("source_labels", ["AIë¶„ì„"])
            selected = False
            
            # ì¿¼í„°ê°€ ë‚¨ì€ ë¼ë²¨ì´ ìˆìœ¼ë©´ ì„ íƒ
            for label in source_labels:
                if label in label_quotas and label_counts.get(label, 0) < label_quotas[label]:
                    final_candidates.append(candidate)
                    # í•´ë‹¹ ìƒí’ˆì˜ ëª¨ë“  ë¼ë²¨ ì¹´ìš´íŠ¸ ì¦ê°€
                    for lbl in source_labels:
                        label_counts[lbl] = label_counts.get(lbl, 0) + 1
                    selected = True
                    break
            
            if not selected:
                remaining_candidates.append(candidate)
        
        # ë‚˜ë¨¸ì§€ëŠ” ì ìˆ˜ìˆœìœ¼ë¡œ ì±„ì›€
        final_candidates.extend(remaining_candidates)
        
        # ë‹¤ì‹œ ì ìˆ˜ìˆœ ì •ë ¬
        final_candidates.sort(key=lambda x: x["final_score"], reverse=True)
        
        print(f"=== [DEBUG] Trackë³„ ì¿¼í„° ì ìš© í›„: {len(final_candidates)}ê°œ ===")
        print(f"  - ê²½ìŸì‚¬: {label_counts.get('ê²½ìŸì‚¬', 0)}ê°œ, ë§¤ì¶œìƒìœ„: {label_counts.get('ë§¤ì¶œìƒìœ„', 0)}ê°œ, ê³¼ê±°ì‹¤ì : {label_counts.get('ê³¼ê±°ì‹¤ì ', 0)}ê°œ, ë‰´ìŠ¤: {label_counts.get('ë‰´ìŠ¤', 0)}ê°œ")
        
        candidates = final_candidates
        
        # 5. ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚° (ë‚´ë¶€ ì‚¬ìš©ìš©)
        category_scores = {}
        category_sales = {}
        for candidate in candidates:
            category = candidate["product"].get("category_main", "ê¸°íƒ€")
            if category == "ê¸°íƒ€" or not category:
                continue
            if category not in category_sales:
                category_sales[category] = []
            category_sales[category].append(candidate["predicted_sales"])
        
        for category, sales_list in category_sales.items():
            avg_sales = sum(sales_list) / len(sales_list)
            category_scores[category] = {"predicted_sales": avg_sales}
        
        return candidates, category_scores
    
    async def _get_sales_top_products(self, context: Dict[str, Any], limit: int = 20) -> List[Dict]:
        """
        Track B: ë§¤ì¶œ ì˜ˆì¸¡ ìƒìœ„ ìƒí’ˆ ì¡°íšŒ (í‚¤ì›Œë“œ ë§¤ì¹­ ë¬´ê´€)
        ë°©ì†¡í…Œì´í”„ê°€ ìˆëŠ” ì „ì²´ ìƒí’ˆ ì¤‘ XGBoost ë§¤ì¶œ ì˜ˆì¸¡ ìƒìœ„ Nê°œ ë°˜í™˜
        """
        try:
            # 1. ë°©ì†¡í…Œì´í”„ ìˆëŠ” ì „ì²´ ìƒí’ˆ ì¡°íšŒ
            all_products = self.product_embedder.get_all_products_with_tape(limit=100)
            
            if not all_products:
                logger.warning("[Track B] ë°©ì†¡í…Œì´í”„ ë³´ìœ  ìƒí’ˆ ì—†ìŒ")
                return []
            
            print(f"=== [Track B] ë°©ì†¡í…Œì´í”„ ë³´ìœ  ìƒí’ˆ: {len(all_products)}ê°œ ===")
            
            # 2. ë°°ì¹˜ XGBoost ë§¤ì¶œ ì˜ˆì¸¡
            predicted_sales_list = await self._predict_products_sales_batch(all_products, context)
            
            # 3. ë§¤ì¶œ ì˜ˆì¸¡ ê²°ê³¼ì™€ ìƒí’ˆ ë§¤ì¹­
            products_with_sales = []
            for i, product in enumerate(all_products):
                product["predicted_sales"] = predicted_sales_list[i]
                products_with_sales.append(product)
            
            # 4. ë§¤ì¶œ ì˜ˆì¸¡ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬
            products_with_sales.sort(key=lambda x: x.get("predicted_sales", 0), reverse=True)
            
            # 5. ìƒìœ„ Nê°œ ë°˜í™˜
            top_products = products_with_sales[:limit]
            
            print(f"=== [Track B] ë§¤ì¶œ ì˜ˆì¸¡ ìƒìœ„ {len(top_products)}ê°œ ì„ ì • ===")
            for i, p in enumerate(top_products[:5], 1):
                print(f"  {i}. {p.get('product_name', '')[:30]} | ì˜ˆì¸¡: {int(p.get('predicted_sales', 0)/10000)}ë§Œì›")
            
            return top_products
            
        except Exception as e:
            logger.error(f"[Track B] ë§¤ì¶œ ìƒìœ„ ìƒí’ˆ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            return []
    
    async def _get_competitor_based_products(self, context: Dict[str, Any], limit: int = 15) -> List[Dict]:
        """
        Track D: ê²½ìŸì‚¬ í¸ì„± ê¸°ë°˜ RAG ê²€ìƒ‰ (LLM ë¯¸ì‚¬ìš©)
        
        1. Netezzaì—ì„œ ê²½ìŸì‚¬ í¸ì„± ì¡°íšŒ
        2. í¸ì„± ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (ì½”ë”© ë°©ì‹)
        3. RAG ê²€ìƒ‰ìœ¼ë¡œ ìœ ì‚¬ ìƒí’ˆ ì°¾ê¸°
        """
        try:
            broadcast_time_str = context.get("broadcast_time")
            if not broadcast_time_str:
                logger.warning("[Track D] broadcast_timeì´ contextì— ì—†ìŒ")
                return []
            
            # 1. ê²½ìŸì‚¬ í¸ì„± ì¡°íšŒ
            competitor_data = await netezza_conn.get_competitor_schedules(broadcast_time_str)
            
            if not competitor_data:
                logger.info("[Track D] ê²½ìŸì‚¬ í¸ì„± ë°ì´í„° ì—†ìŒ")
                return []
            
            print(f"=== [Track D] ê²½ìŸì‚¬ í¸ì„± {len(competitor_data)}ê°œ ì¡°íšŒë¨ ===")
            
            # 2. í¸ì„± ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (LLM ë¯¸ì‚¬ìš©)
            competitor_keywords = []
            competitor_info = {}  # í‚¤ì›Œë“œ â†’ ê²½ìŸì‚¬ ì •ë³´ ë§¤í•‘
            
            for comp in competitor_data:
                title = comp.get("broadcast_title", "")
                company = comp.get("company_name", "")
                category = comp.get("category_main", "")
                start_time = comp.get("start_time", "")
                
                # í‚¤ì›Œë“œ ì¶”ì¶œ (ì½”ë”© ë°©ì‹)
                keywords = self._extract_keywords_from_title(title, category)
                
                for kw in keywords:
                    if kw not in competitor_info:
                        competitor_keywords.append(kw)
                        competitor_info[kw] = {
                            "company": company,
                            "title": title[:40],  # ì œëª© 40ìë¡œ ì œí•œ
                            "start_time": str(start_time)[:16] if start_time else "",
                            "category": category,
                            "keyword": kw  # ë§¤ì¹­ëœ í‚¤ì›Œë“œ ì €ì¥
                        }
            
            if not competitor_keywords:
                logger.info("[Track D] ê²½ìŸì‚¬ í¸ì„±ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹¤íŒ¨")
                return []
            
            print(f"=== [Track D] ê²½ìŸì‚¬ í‚¤ì›Œë“œ ì¶”ì¶œ: {competitor_keywords[:10]}... ===")
            
            # 3. RAG ê²€ìƒ‰ìœ¼ë¡œ ìœ ì‚¬ ìƒí’ˆ ì°¾ê¸°
            search_results = self.product_embedder.search_products(
                trend_keywords=competitor_keywords[:10],  # ìƒìœ„ 10ê°œ í‚¤ì›Œë“œë§Œ
                top_k=limit,
                score_threshold=0.3,
                only_ready_products=True
            )
            
            # 4. ê²½ìŸì‚¬ ì •ë³´ ì¶”ê°€ - ì²« ë²ˆì§¸ í‚¤ì›Œë“œ ì •ë³´ë¥¼ ê¸°ë³¸ìœ¼ë¡œ ì‚¬ìš©
            first_keyword = competitor_keywords[0] if competitor_keywords else ""
            first_comp_info = competitor_info.get(first_keyword, {})
            
            for product in search_results:
                # ìƒí’ˆëª…ì—ì„œ ë§¤ì¹­ë˜ëŠ” í‚¤ì›Œë“œ ì°¾ê¸°
                product_name = product.get("product_name", "").lower()
                matched_kw = ""
                matched_info = {}
                
                for kw in competitor_keywords:
                    if kw.lower() in product_name or any(word in product_name for word in kw.lower().split()):
                        matched_kw = kw
                        matched_info = competitor_info.get(kw, {})
                        break
                
                # ë§¤ì¹­ëœ í‚¤ì›Œë“œê°€ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ í‚¤ì›Œë“œ ì •ë³´ ì‚¬ìš©
                if not matched_kw:
                    matched_kw = first_keyword
                    matched_info = first_comp_info
                
                product["matched_keyword"] = matched_kw
                product["competitor_info"] = matched_info
                product["source_track"] = "competitor"
            
            print(f"=== [Track D] RAG ê²€ìƒ‰ ê²°ê³¼: {len(search_results)}ê°œ ìƒí’ˆ ===")
            for i, p in enumerate(search_results[:3], 1):
                comp_info = p.get("competitor_info", {})
                print(f"  {i}. {p.get('product_name', '')[:25]} | í‚¤ì›Œë“œ: {p.get('matched_keyword', '')} | ê²½ìŸì‚¬: {comp_info.get('company', 'N/A')} ({comp_info.get('start_time', '')[:16]})")
            
            return search_results
            
        except Exception as e:
            logger.error(f"[Track D] ê²½ìŸì‚¬ ê¸°ë°˜ ìƒí’ˆ ì¡°íšŒ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(f"ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
            return []
    
    def _extract_keywords_from_title(self, title: str, category: str = "") -> List[str]:
        """
        í¸ì„± ì œëª©ì—ì„œ í‚¤ì›Œë“œ ì¶”ì¶œ (LLM ë¯¸ì‚¬ìš©, ì½”ë”© ë°©ì‹)
        
        ì˜ˆì‹œ:
        - "ê²¨ìš¸ íŒ¨ë”© íŠ¹ê°€ì „" â†’ ["ê²¨ìš¸ íŒ¨ë”©", "íŒ¨ë”©"]
        - "[íŠ¹ê°€] ë¡œë´‡ì²­ì†Œê¸° ëŒ€ì „" â†’ ["ë¡œë´‡ì²­ì†Œê¸°"]
        - "í”„ë¦¬ë¯¸ì—„ ì˜¤ë©”ê°€3 12ê°œì›”" â†’ ["ì˜¤ë©”ê°€3"]
        """
        import re
        
        if not title:
            return []
        
        # ë¶ˆìš©ì–´ (ì œê±°í•  ë‹¨ì–´)
        stopwords = {
            # í”„ë¡œëª¨ì…˜ ê´€ë ¨
            "íŠ¹ê°€", "íŠ¹ê°€ì „", "ëŒ€ì „", "ê¸°íšì „", "ì„¸ì¼", "í• ì¸", "í”„ë¦¬ë¯¸ì—„", "ìŠ¤í˜ì…œ",
            "ë‹¨ë…", "í•œì •", "ë² ìŠ¤íŠ¸", "ì¸ê¸°", "ì¶”ì²œ", "ì‹ ìƒ", "ì‹ ìƒí’ˆ", "íˆíŠ¸",
            # ìˆ˜ëŸ‰/ë‹¨ìœ„ ê´€ë ¨
            "ê°œì›”", "ê°œì›”ë¶„", "ë°•ìŠ¤", "ì„¸íŠ¸", "íŒ©", "í†µ", "ê°œ", "ë§¤",
            # ë°©ì†¡ ê´€ë ¨
            "ë°©ì†¡", "í™ˆì‡¼í•‘", "ë¼ì´ë¸Œ", "ìƒë°©ì†¡", "ì•µì½œ", "ì¬ë°©ì†¡",
            # í˜œíƒ ê´€ë ¨
            "ë¬´ë£Œë°°ì†¡", "ì‚¬ì€í’ˆ", "ì¦ì •", "ì„ ë¬¼", "ì´ë²¤íŠ¸",
            # ì‹œì¦Œ ì½”ë“œ (ì˜ë¯¸ì—†ëŠ” í‚¤ì›Œë“œ)
            "24FW", "25FW", "24SS", "25SS", "23FW", "23SS", "22FW", "22SS",
            "FW", "SS", "AW", "ë´„", "ì—¬ë¦„", "ê°€ì„", "ê²¨ìš¸",
            # ê¸°íƒ€ ì˜ë¯¸ì—†ëŠ” í‚¤ì›Œë“œ
            "ì‹ ê·œ", "ëŸ°ì¹­", "ì˜¤í”ˆ", "ë¦¬ë‰´ì–¼", "ì—…ê·¸ë ˆì´ë“œ", "ë‰´", "NEW",
            "ì´", "ì „", "ì¢…", "êµ¬ì„±", "ë”ë¸”", "íŠ¸ë¦¬í”Œ", "í’€", "ì˜¬"
        }
        
        # 1. íŠ¹ìˆ˜ë¬¸ì ë° ê´„í˜¸ ë‚´ìš© ì œê±°
        clean_title = re.sub(r'\[.*?\]|\(.*?\)|ã€.*?ã€‘', '', title)
        clean_title = re.sub(r'[^\w\sê°€-í£]', ' ', clean_title)
        
        # 2. ìˆ«ì+ë‹¨ìœ„ íŒ¨í„´ ì œê±° (12ê°œì›”, 3ë°•ìŠ¤ ë“±)
        clean_title = re.sub(r'\d+\s*(ê°œì›”ë¶„?|ë°•ìŠ¤|ì„¸íŠ¸|íŒ©|í†µ|ê°œ|ë§¤|g|kg|ml|L)', '', clean_title)
        
        # 3. ê³µë°±ìœ¼ë¡œ ë¶„ë¦¬
        words = clean_title.split()
        
        # 4. ë¶ˆìš©ì–´ ì œê±° ë° 2ê¸€ì ì´ìƒ í•„í„°ë§
        keywords = []
        for word in words:
            word = word.strip()
            if len(word) >= 2 and word not in stopwords:
                keywords.append(word)
        
        # 5. ì¹´í…Œê³ ë¦¬ë„ í‚¤ì›Œë“œë¡œ ì¶”ê°€
        if category and len(category) >= 2:
            keywords.append(category)
        
        # 6. 2ë‹¨ì–´ ì¡°í•© í‚¤ì›Œë“œ ì¶”ê°€ (ì˜ˆ: "ê²¨ìš¸ íŒ¨ë”©")
        if len(keywords) >= 2:
            combined = f"{keywords[0]} {keywords[1]}"
            keywords.insert(0, combined)
        
        # ì¤‘ë³µ ì œê±°
        seen = set()
        unique_keywords = []
        for kw in keywords:
            if kw not in seen:
                seen.add(kw)
                unique_keywords.append(kw)
        
        return unique_keywords[:5]  # ìµœëŒ€ 5ê°œ
    
    async def _predict_categories_with_xgboost(
        self, 
        category_groups: Dict[str, List[Dict]], 
        context: Dict[str, Any]
    ) -> Dict[str, Dict[str, float]]:
        """ì¹´í…Œê³ ë¦¬ë³„ XGBoost ë§¤ì¶œ ì˜ˆì¸¡"""
        
        category_scores = {}
        broadcast_dt = context["broadcast_dt"]
        
        for category, products in category_groups.items():
            if not products:
                continue
            
            try:
                # ëŒ€í‘œ ìƒí’ˆìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ë§¤ì¶œ ì˜ˆì¸¡
                representative_product = products[0]
                predicted_sales = await self._predict_product_sales(representative_product, context)
                
                # ì¹´í…Œê³ ë¦¬ ë‚´ ìƒí’ˆ ìˆ˜ë¡œ ë³´ì •
                adjusted_sales = predicted_sales * min(len(products) / 5, 2.0)
                
                category_scores[category] = {
                    "predicted_sales": adjusted_sales,
                    "product_count": len(products),
                    "avg_similarity": sum(p.get("similarity_score", 0) for p in products) / len(products)
                }
                
                print(f"  - ì¹´í…Œê³ ë¦¬ '{category}': {int(adjusted_sales/10000)}ë§Œì› (ìƒí’ˆ: {len(products)}ê°œ)")
                
            except Exception as e:
                logger.error(f"ì¹´í…Œê³ ë¦¬ '{category}' ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                category_scores[category] = {
                    "predicted_sales": 10000000,  # ê¸°ë³¸ê°’ 1000ë§Œì›
                    "product_count": len(products),
                    "avg_similarity": 0.4
                }
        
        return category_scores
    
    async def _rank_final_candidates(self, candidates: List[Dict[str, Any]], category_scores: Dict[str, Any], context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ìµœì¢… ë­í‚¹ ê³„ì‚° - ì‹œì¦Œ ì í•©ì„± + ì¹´í…Œê³ ë¦¬+ë¸Œëœë“œ ë‹¤ì–‘ì„± ì ìš©"""
        
        print(f"=== [DEBUG _rank_final_candidates] ì´ë¯¸ ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬ëœ {len(candidates)}ê°œ í›„ë³´ ìˆ˜ì‹  ===")
        
        # 0. ì‹œì¦Œ ì í•©ì„± í•„í„°ë§ (LLM ë°°ì¹˜ íŒë‹¨) - ìƒìœ„ 40ê°œ í›„ë³´ì— ëŒ€í•´
        top_candidates = candidates[:40]  # ì¶©ë¶„í•œ í›„ë³´êµ° ì¤€ë¹„ (ì‹œì¦Œ í•„í„° + ì¤‘ë³µ ì œê±° ê³ ë ¤)
        print(f"\n=== [ì‹œì¦Œ ì í•©ì„± ê²€ì‚¬] ìƒìœ„ {len(top_candidates)}ê°œ í›„ë³´ ê²€ì‚¬ ì‹œì‘ ===")
        
        season_filtered = await self._filter_by_season_suitability(top_candidates, context)
        print(f"=== [ì‹œì¦Œ ì í•©ì„± ê²€ì‚¬] {len(top_candidates)}ê°œ â†’ {len(season_filtered)}ê°œ (ë¶€ì í•© {len(top_candidates) - len(season_filtered)}ê°œ ì œê±°) ===\n")
        
        # 1. ì¹´í…Œê³ ë¦¬+ë¸Œëœë“œ ì¤‘ë³µ ì œê±° + ëŒ€ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ ì¿¼í„° ì œí•œ
        category_brand_seen = set()
        category_count = {}  # ëŒ€ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ë³„ ê°œìˆ˜
        filtered_candidates = []
        
        for candidate in season_filtered:
            product = candidate["product"]
            product_name = product.get("product_name", "")
            category = product.get("category_main", "Unknown")
            brand = product.get("brand", "Unknown")
            key = f"{category}_{brand}"
            
            # 1-1. ê°™ì€ ì¹´í…Œê³ ë¦¬+ë¸Œëœë“œ ì¡°í•©ì€ 1ê°œë§Œ í—ˆìš© (ë‹¤ì–‘ì„± ë³´ì¥)
            if key in category_brand_seen:
                print(f"  âš ï¸ ë¸Œëœë“œ ì¤‘ë³µ ì œê±°: {product_name[:30]} (ì¹´í…Œê³ ë¦¬: {category}, ë¸Œëœë“œ: {brand})")
                continue
            
            # 1-2. ê°™ì€ ëŒ€ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ëŠ” ìµœëŒ€ 4ê°œê¹Œì§€ë§Œ í—ˆìš©
            current_count = category_count.get(category, 0)
            if current_count >= 4:
                print(f"  âš ï¸ ì¹´í…Œê³ ë¦¬ ì¿¼í„° ì´ˆê³¼: {product_name[:30]} (ì¹´í…Œê³ ë¦¬: {category}, ì´ë¯¸ {current_count}ê°œ)")
                continue
            
            # í†µê³¼: í›„ë³´ì— ì¶”ê°€
            filtered_candidates.append(candidate)
            category_brand_seen.add(key)
            category_count[category] = current_count + 1
        
        print(f"=== [ë‹¤ì–‘ì„± í•„í„°ë§] {len(season_filtered)}ê°œ â†’ {len(filtered_candidates)}ê°œ (ì¤‘ë³µ {len(season_filtered) - len(filtered_candidates)}ê°œ ì œê±°) ===")
        print(f"=== [ì¹´í…Œê³ ë¦¬ ë¶„í¬] {category_count} ===")
        
        for i, candidate in enumerate(filtered_candidates[:5]):
            product = candidate['product']
            print(f"  {i+1}ìœ„: {product.get('product_name')[:25]} | {product.get('category_main', 'N/A')} | {product.get('brand', 'N/A')} (ì ìˆ˜: {candidate['final_score']:.3f})")
        
        return filtered_candidates
    
    async def _filter_by_season_suitability(self, candidates: List[Dict[str, Any]], context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ì‹œì¦Œ ì í•©ì„± í•„í„°ë§ - LLM ë°°ì¹˜ íŒë‹¨"""
        
        if not candidates:
            return []
        
        # í˜„ì¬ ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
        broadcast_dt = context.get("broadcast_dt")
        month = broadcast_dt.month if broadcast_dt else 11
        day = broadcast_dt.day if broadcast_dt else 19
        holiday_name = context.get("holiday_name")
        
        # ìƒí’ˆ ì •ë³´ ì¤€ë¹„ (ìƒí’ˆëª… + í…Œì´í”„ëª…)
        products_info = []
        for i, candidate in enumerate(candidates):
            product = candidate["product"]
            products_info.append({
                "index": i,
                "product_name": product.get("product_name", ""),
                "tape_name": product.get("tape_name", ""),
                "category": product.get("category_main", "")
            })
        
        # LLM í”„ë¡¬í”„íŠ¸
        season_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ 20ë…„ì°¨ í™ˆì‡¼í•‘ ë°©ì†¡ í¸ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
í˜„ì¬ ë‚ ì§œ/ê³„ì ˆì— ì–´ìš¸ë¦¬ì§€ ì•ŠëŠ” ìƒí’ˆì„ ì°¾ì•„ì£¼ì„¸ìš”.

**ì œì™¸ ê¸°ì¤€ (ìƒí’ˆì˜ ì‹¤ì œ íŠ¹ì„± ì¤‘ì‹¬):**

1. ëª…ì ˆ ë¶ˆì¼ì¹˜: íŠ¹ì • ëª…ì ˆ ìƒí’ˆì¸ë° í˜„ì¬ ëª…ì ˆê³¼ ë§ì§€ ì•ŠëŠ” ê²½ìš°
   - ì˜ˆ: 11ì›”ì— "ì‹ ë…„íŠ¹ì§‘", "ì„¤ë‚ ", "ì¶”ì„" í¬í•¨ ìƒí’ˆ
   - ì˜ˆ: 7ì›”ì— "í¬ë¦¬ìŠ¤ë§ˆìŠ¤" í¬í•¨ ìƒí’ˆ
   - ì„ í–‰ íŒë§¤ í—ˆìš©: 12ì›” ë§ ì‹ ë…„íŠ¹ì§‘ â­•, 12ì›” ì¤‘ìˆœ í¬ë¦¬ìŠ¤ë§ˆìŠ¤ â­•, 8ì›” ë§ ì¶”ì„ â­•

2. ê³„ì ˆ/ë‚ ì”¨ ë¶€ì í•© ìƒí’ˆ (ìƒí’ˆ íŠ¹ì„±ìœ¼ë¡œë§Œ íŒë‹¨):
   
   **ê²¨ìš¸ì² (11ì›”~2ì›”) - ì¶”ìš´ ë‚ ì”¨ì— ì œì™¸í•  ê²ƒ:**
   - ì—¬ë¦„ ëƒ‰ë°©: "ëƒ‰ê°", "ì¿¨ë§", "ì‹œì›í•œ", "ëƒ‰ë°©", "í”¼ì„œìš©", "ì—¬ë¦„ìš©"
   - ì—¬ë¦„ ì˜ë¥˜: "ë°˜íŒ”", "ë°˜ë°”ì§€", "ë¯¼ì†Œë§¤", "ìƒŒë“¤" (ì‹¤ë‚´ìš© ì œì™¸)
   - ì—¬ë¦„ ì¹¨êµ¬: "ëƒ‰ê° íŒ¨ë“œ", "ì¿¨ë§¤íŠ¸"
   - ì˜ˆ: "ì¿¨ë“œë¦¼ ëƒ‰ê°íŒ¨ë“œ", "ì—¬ë¦„ ë°˜íŒ”í‹°", "í”¼ì„œìš© ì„ í’ê¸°"
   
   **ê²¨ìš¸ì² (11ì›”~2ì›”) - ì¶”ìš´ ë‚ ì”¨ì— ì í•© (í—ˆìš©):**
   - ë‚œë°© ìƒí’ˆ: "ì „ê¸°ì¥íŒ", "ì „ê¸°ë‹´ìš”", "ì˜¨ì—´", "ë‚œë°©", "ë³´ì˜¨"
   - ê²¨ìš¸ ì˜ë¥˜: "íŒ¨ë”©", "ê¸°ëª¨", "ê²¨ìš¸", "ì½”íŠ¸", "ëª©ë„ë¦¬", "ì¥ê°‘", "ë‘êº¼ìš´"
   - ì˜ˆ: "ì „ê¸°ë§¤íŠ¸", "ì˜¨ì—´ë§ˆì‚¬ì§€ê¸°", "íŒ¨ë”©", "ê¸°ëª¨ë°”ì§€" â†’ ëª¨ë‘ OK!
   
   **ì—¬ë¦„ì² (6ì›”~8ì›”) - ë”ìš´ ë‚ ì”¨ì— ì œì™¸í•  ê²ƒ:**
   - ë‚œë°© ìƒí’ˆ: "ì „ê¸°ì¥íŒ", "ì „ê¸°ë‹´ìš”", "ì˜¨ì—´", "ë‚œë°©"
   - ê²¨ìš¸ ì˜ë¥˜: "íŒ¨ë”©", "ê¸°ëª¨", "ê²¨ìš¸", "ë‘êº¼ìš´ ì½”íŠ¸", "ëª©ë„ë¦¬"
   - ì˜ˆ: "ê²¨ìš¸ íŒ¨ë”©", "ê¸°ëª¨ ë°”ì§€", "ì „ê¸°ì¥íŒ"
   
   **ë´„/ê°€ì„(3~5ì›”, 9~10ì›”) - í™˜ì ˆê¸°:**
   - 3~5ì›”: ê²¨ìš¸ ë‚œë°© ìƒí’ˆ ì œì™¸, ì—¬ë¦„ ëƒ‰ë°© ìƒí’ˆ OK
   - 9~10ì›”: ì—¬ë¦„ ëƒ‰ë°© ìƒí’ˆ ì œì™¸, ê²¨ìš¸ ë‚œë°© ìƒí’ˆ OK

**ì¤‘ìš” - ì‹œì¦Œ ì½”ë“œ(SS/FW)ëŠ” ë¬´ì‹œí•˜ì„¸ìš”:**
- "25SS", "24FW" ê°™ì€ ì½”ë“œëŠ” ì°¸ê³ ë§Œ í•˜ê³ , ìƒí’ˆì˜ ì‹¤ì œ íŠ¹ì„±ìœ¼ë¡œ íŒë‹¨
- ì˜ˆ: "25SS ê¸°ëª¨ ë°”ì§€" â†’ ê¸°ëª¨ê°€ ìˆìœ¼ë©´ ê²¨ìš¸ì— OK
- ì˜ˆ: "24FW ë°˜íŒ”í‹°" â†’ ë°˜íŒ”ì´ë©´ ê²¨ìš¸ì— ì œì™¸
- ì˜ˆ: "23SS íŒ¨ë”©" â†’ íŒ¨ë”©ì´ë©´ ì—¬ë¦„ì— ì œì™¸

# ì„ í–‰ íŒë§¤ëŠ” í—ˆìš© (1~2ì£¼ ì „)
- 12ì›” ë§ ì‹ ë…„íŠ¹ì§‘ â­•
- 12ì›” ì¤‘ìˆœ í¬ë¦¬ìŠ¤ë§ˆìŠ¤ ì¼€ì´í¬ â­•
- 8ì›” ë§ ì¶”ì„ì„ ë¬¼ì„¸íŠ¸ â­•

**ì œì™¸í•˜ì§€ ë§ ê²ƒ:**
- ì‚¬ê³„ì ˆ ìƒí’ˆ: ê±´ê°•ì‹í’ˆ, ìƒí™œìš©í’ˆ, ì‹í’ˆ, ê°€ì „ ë“±
- ì‹œì¦Œ í‚¤ì›Œë“œê°€ ì—†ëŠ” ì¼ë°˜ ìƒí’ˆ

JSON í˜•ì‹ìœ¼ë¡œ ì œì™¸í•  ìƒí’ˆì˜ ì¸ë±ìŠ¤ ë°°ì—´ì„ ë°˜í™˜í•˜ì„¸ìš”:
{{
  "exclude_indices": [ì¸ë±ìŠ¤ ë°°ì—´],
  "reasons": {{
    "ì¸ë±ìŠ¤": "ì œì™¸ ì´ìœ "
  }}
}}"""),
            ("human", """í˜„ì¬ ì •ë³´:
- ë‚ ì§œ: {month}ì›” {day}ì¼
- ê³µíœ´ì¼: {holiday_name}

ìƒí’ˆ ëª©ë¡:
{products_list}

ìœ„ ìƒí’ˆ ì¤‘ í˜„ì¬ ë‚ ì§œ/ì‹œì¦Œì— ì í•©í•˜ì§€ ì•Šì€ ìƒí’ˆì˜ ì¸ë±ìŠ¤ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”.
ì˜ˆ: 11ì›” ì¤‘ìˆœì´ë©´ ê²¨ìš¸ ìƒí’ˆì€ OK, ì¶”ì„/ì„¤ë‚  ìƒí’ˆì€ ì œì™¸""")
        ])
        
        # ìƒí’ˆ ëª©ë¡ ë¬¸ìì—´ ìƒì„± (ìƒí’ˆëª… + í…Œì´í”„ëª…)
        products_list_str = "\n".join([
            f"{p['index']}. {p['product_name']}\n   í…Œì´í”„ëª…: {p['tape_name']}\n   ì¹´í…Œê³ ë¦¬: {p['category']}"
            for p in products_info
        ])
        
        chain = season_prompt | self.llm | JsonOutputParser()
        
        try:
            result = await chain.ainvoke({
                "month": month,
                "day": day,
                "holiday_name": holiday_name if holiday_name else "ì—†ìŒ",
                "products_list": products_list_str
            })
            
            exclude_indices = set(result.get("exclude_indices", []))
            reasons = result.get("reasons", {})
            
            # ì œì™¸ëœ ìƒí’ˆ ë¡œê·¸
            for idx in exclude_indices:
                if idx < len(candidates):
                    product_name = candidates[idx]["product"].get("product_name", "")[:40]
                    reason = reasons.get(str(idx), "ì‹œì¦Œ ë¶€ì í•©")
                    print(f"  âŒ ì œì™¸: {product_name} - {reason}")
            
            # í•„í„°ë§
            filtered = [c for i, c in enumerate(candidates) if i not in exclude_indices]
            return filtered
            
        except Exception as e:
            logger.error(f"ì‹œì¦Œ ì í•©ì„± íŒë‹¨ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(f"ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
            # ì˜¤ë¥˜ ì‹œ ëª¨ë“  í›„ë³´ ë°˜í™˜ (ì•ˆì „ì¥ì¹˜)
            return candidates
    
    def _calculate_competition_penalty(self, product: Dict[str, Any], all_candidates: List[Dict[str, Any]]) -> float:
        """ê²½ìŸ í˜ë„í‹° ì ìˆ˜ ê³„ì‚°"""
        category = product.get("category_main", "")
        same_category_count = sum(1 for c in all_candidates if c["product"].get("category_main") == category)
        
        # ê°™ì€ ì¹´í…Œê³ ë¦¬ ìƒí’ˆì´ ë§ì„ìˆ˜ë¡ í˜ë„í‹°
        if same_category_count <= 2:
            return 0.0
        elif same_category_count <= 4:
            return 0.1
        else:
            return 0.2
    
    async def _format_response(self, ranked_products: List[Dict[str, Any]], context: Dict[str, Any] = None) -> BroadcastResponse:
        """API ì‘ë‹µ ìƒì„± (ë¹„ë™ê¸°)"""
        print(f"=== [DEBUG _format_response] context keys: {context.keys() if context else 'None'} ===")
        if context:
            print(f"=== [DEBUG _format_response] generated_keywords: {context.get('generated_keywords', [])} ===")
        
        # 1. í…Œì´í”„ ì½”ë“œ ëª©ë¡ ì¶”ì¶œ
        tape_codes = [p["product"].get("tape_code") for p in ranked_products if p["product"].get("tape_code")]
        
        # 2. ìµœê·¼ ë°©ì†¡ ì‹¤ì  ë°°ì¹˜ ì¡°íšŒ (Netezza)
        broadcast_history_map = {}
        if tape_codes:
            logger.info(f" {len(tape_codes)}ê°œ í…Œì´í”„ì˜ ìµœê·¼ ë°©ì†¡ ì‹¤ì  ì¡°íšŒ ì¤‘...")
            broadcast_history_map = self.broadcast_history_service.get_latest_broadcasts_batch(tape_codes)
            logger.info(f" {sum(1 for v in broadcast_history_map.values() if v is not None)}ê°œ í…Œì´í”„ì˜ ì‹¤ì  ì¡°íšŒ ì„±ê³µ")
        
        recommendations = []
        
        # ìˆœìœ„ ì •ë³´ ì¶”ê°€ (ë°°ì¹˜ ì²˜ë¦¬ ì „)
        for i, candidate in enumerate(ranked_products):
            candidate["rank"] = i + 1
            candidate["total_count"] = len(ranked_products)
        
        # [5-1ë‹¨ê³„] ì½”ë”© ë°©ì‹ìœ¼ë¡œ ì¶”ì²œ ê·¼ê±° ìƒì„± (LLM ë¯¸ì‚¬ìš© - ì†ë„/ë¹„ìš© ìµœì í™”)
        step_5_1_start = time.time()
        print("\n" + "=" * 80)
        print(f"[5-1ë‹¨ê³„] ì½”ë”© ë°©ì‹ - {len(ranked_products)}ê°œ ìƒí’ˆì˜ ì¶”ì²œ ê·¼ê±° ìƒì„±")
        print("=" * 80)
        
        reasoning_results = []
        for candidate in ranked_products:
            result = self._generate_reasoning_by_code(candidate, context or {})
            reasoning_results.append(result)
        
        reasoning_list = [r["reasoning"] for r in reasoning_results]
        print(f"â±ï¸  [5-1ë‹¨ê³„] ì¶”ì²œ ê·¼ê±° ìƒì„±: {time.time() - step_5_1_start:.2f}ì´ˆ (LLM ë¯¸ì‚¬ìš©)")
        
        for i, candidate in enumerate(ranked_products):
            product = candidate["product"]
            reasoning_summary = reasoning_list[i] if i < len(reasoning_list) else f"{product.get('category_main', 'ìƒí’ˆ')} ì¶”ì²œ"
            
            # ìµœê·¼ ë°©ì†¡ ì‹¤ì  ì¡°íšŒ
            tape_code = product.get("tape_code")
            last_broadcast_data = broadcast_history_map.get(tape_code) if tape_code else None
            last_broadcast = None
            
            if last_broadcast_data:
                try:
                    last_broadcast = LastBroadcastMetrics(**last_broadcast_data)
                    logger.debug(f"âœ… í…Œì´í”„ {tape_code}ì˜ ìµœê·¼ ë°©ì†¡ ì‹¤ì  ì¶”ê°€")
                except Exception as e:
                    logger.warning(f"âš ï¸ í…Œì´í”„ {tape_code}ì˜ ì‹¤ì  ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {e}")
            
            # ì¶”ì²œ ì¶œì²˜ ì •ë³´ëŠ” ë‚´ë¶€ ë¡œê·¸ìš©ìœ¼ë¡œë§Œ ì‚¬ìš© (API ì‘ë‹µì—ëŠ” í¬í•¨í•˜ì§€ ì•ŠìŒ)
            sources_for_log = candidate.get("recommendation_sources", [])
            
            recommendation = BroadcastRecommendation(
                rank=i+1,
                productInfo=ProductInfo(
                    productId=product.get("product_code", "Unknown"),
                    productName=product.get("product_name", "Unknown"),
                    category=product.get("category_main", "Unknown"),
                    categoryMiddle=product.get("category_middle"),
                    categorySub=product.get("category_sub"),
                    brand=product.get("brand"),
                    price=product.get("price"),
                    tapeCode=product.get("tape_code"),
                    tapeName=product.get("tape_name")
                ),
                reasoning=reasoning_summary,
                businessMetrics=BusinessMetrics(
                    aiPredictedSales=f"{round(candidate['predicted_sales']/10000, 1):,.1f}ë§Œì›",  # AI ì˜ˆì¸¡ ë§¤ì¶œ (XGBoost, ì†Œìˆ˜ì  1ìë¦¬)
                    lastBroadcast=last_broadcast  # ìµœê·¼ ë°©ì†¡ ì‹¤ì  ì¶”ê°€
                )
                # sources í•„ë“œ ì œê±° - ì¶”ì²œ ê·¼ê±° ìƒì„±ì—ë§Œ ë‚´ë¶€ì ìœ¼ë¡œ ì‚¬ìš©
            )

            # ì¶”ì²œ ê²°ê³¼ ìš”ì•½ ë¡œê·¸ (ì‹œì—°/ë¶„ì„ìš©) - ì¶œì²˜ ì •ë³´ í¬í•¨
            try:
                # ì¶œì²˜ ìš”ì•½ ìƒì„± (ë”•ì…”ë„ˆë¦¬ í˜•íƒœë¡œ ì²˜ë¦¬)
                source_summary = []
                for src in sources_for_log:
                    src_type = src.get("source_type", "")
                    if src_type == "rag_match":
                        origin = src.get("keyword_origin", "unknown")
                        source_summary.append(f"í‚¤ì›Œë“œ({origin}): {src.get('matched_keyword', '')}")
                    elif src_type == "news_trend":
                        source_summary.append(f"ë‰´ìŠ¤: {src.get('news_keyword', '')}")
                    elif src_type == "ai_trend":
                        source_summary.append(f"AIíŠ¸ë Œë“œ: {src.get('ai_keyword', '')}")
                    elif src_type == "xgboost_sales":
                        pred_sales = src.get("predicted_sales", 0)
                        source_summary.append(f"ë§¤ì¶œì˜ˆì¸¡: {int(pred_sales/10000):,}ë§Œì›")
                
                print("=" * 100)
                print(
                    f"[ìµœì¢… ì¶”ì²œ #{recommendation.rank}] "
                    f"{recommendation.productInfo.productName[:40]}"
                )
                print(f"  [ì¹´í…Œê³ ë¦¬] {recommendation.productInfo.category}")
                print(f"  [ì˜ˆì¸¡ë§¤ì¶œ] {recommendation.businessMetrics.aiPredictedSales}")
                print(f"  [ì ìˆ˜] {candidate.get('final_score', 0.0):.3f}")
                print(f"  [ì¶œì²˜] {' | '.join(source_summary)}")
                print(f"  [ì¶”ì²œê·¼ê±°] {recommendation.reasoning[:80]}...")
            except Exception as e:
                print(f"[ë¡œê·¸ ì˜¤ë¥˜] {e}")

            recommendations.append(recommendation)
        
        # [5-2ë‹¨ê³„] ë„¤ì´ë²„/íƒ€ì‚¬ í¸ì„± ì¡°íšŒ (LLM ì—†ì´ ë‹¨ìˆœ ì¡°íšŒ)
        step_5_2_start = time.time()
        print("\n" + "=" * 80)
        print(f"[5-2ë‹¨ê³„] ë„¤ì´ë²„/íƒ€ì‚¬ í¸ì„± ì¡°íšŒ")
        print("=" * 80)
        
        # ë„¤ì´ë²„ ë² ìŠ¤íŠ¸ ìƒí’ˆ ì¡°íšŒ (ìƒìœ„ 3ê°œë§Œ)
        naver_products_data = self.external_products_service.get_latest_best_products(limit=3)
        naver_products = [NaverProduct(**product) for product in naver_products_data]
        logger.info(f"âœ… ë„¤ì´ë²„ ìƒí’ˆ ìƒìœ„ {len(naver_products)}ê°œ ìˆ˜ì§‘")
        print(f"âœ… ë„¤ì´ë²„ ìƒí’ˆ ìƒìœ„ {len(naver_products)}ê°œ ìˆ˜ì§‘")
        
        # íƒ€ í™ˆì‡¼í•‘ì‚¬ í¸ì„± ìƒí’ˆ ì¡°íšŒ - Netezzaì—ì„œ ì‹¤ì‹œê°„ ì¡°íšŒ (ì „ì²´)
        competitor_products = []
        try:
            broadcast_time_str = context.get("broadcast_time") if context else None
            if broadcast_time_str:
                competitor_data = await netezza_conn.get_competitor_schedules(broadcast_time_str)
                competitor_products = [CompetitorProduct(**comp) for comp in competitor_data]
                logger.info(f"âœ… íƒ€ì‚¬ í¸ì„± ì „ì²´ {len(competitor_products)}ê°œ ìˆ˜ì§‘")
                print(f"âœ… íƒ€ì‚¬ í¸ì„± ì „ì²´ {len(competitor_products)}ê°œ ìˆ˜ì§‘")
            else:
                logger.warning(f"âš ï¸ broadcast_timeì´ contextì— ì—†ìŒ")
        except Exception as e:
            logger.warning(f"âš ï¸ íƒ€ì‚¬ í¸ì„± ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
        
        # ë„¤ì´ë²„ ìƒìœ„ 3ê°œ + íƒ€ì‚¬ í¸ì„± ì „ì²´ í†µí•© (LLM ì„ íƒ ì—†ì´)
        selected_competitor_products = []
        
        # 1. íƒ€ì‚¬ í¸ì„± ì „ì²´ ì¶”ê°€
        selected_competitor_products.extend(competitor_products)
        
        # 2. ë„¤ì´ë²„ ìƒìœ„ 3ê°œë¥¼ CompetitorProduct í˜•ì‹ìœ¼ë¡œ ë³€í™˜í•˜ì—¬ ì¶”ê°€
        for idx, naver in enumerate(naver_products[:3]):
            selected_competitor_products.append(self._convert_naver_to_competitor(naver, idx))
        
        print(f"â±ï¸  [5-2ë‹¨ê³„] ë„¤ì´ë²„/íƒ€ì‚¬ ì¡°íšŒ ì™„ë£Œ: {time.time() - step_5_2_start:.2f}ì´ˆ")
        print(f"  - íƒ€ì‚¬ í¸ì„±: {len(competitor_products)}ê°œ")
        print(f"  - ë„¤ì´ë²„ ìƒìœ„: {len(naver_products)}ê°œ")
        
        return BroadcastResponse(
            requestTime="",  # ë©”ì¸ì—ì„œ ì„¤ì •
            recommendations=recommendations,
            competitorProducts=selected_competitor_products
        )
    
    async def _select_and_merge_top_10(
        self,
        naver_products: List[NaverProduct],
        competitor_products: List[CompetitorProduct],
        broadcast_time: str,
        context: Dict[str, Any] = None
    ) -> List[CompetitorProduct]:
        """
        AIë¥¼ í™œìš©í•˜ì—¬ ë„¤ì´ë²„/íƒ€ì‚¬ í¸ì„± ì¤‘ 10ê°œë¥¼ ì„ íƒí•˜ê³  í†µí•©
        ë„¤ì´ë²„:íƒ€ì‚¬ = 5:5 ë¹„ìœ¨ ìœ ì§€ (í•œìª½ì´ ë¶€ì¡±í•˜ë©´ ë‹¤ë¥¸ìª½ìœ¼ë¡œ ì±„ì›€)
        """
        try:
            # 1. ë„¤ì´ë²„ ìƒí’ˆì„ íƒ€ì‚¬ í¸ì„± í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            naver_as_competitor = [
                self._convert_naver_to_competitor(naver, idx)
                for idx, naver in enumerate(naver_products)
            ]
            
            # 2. AIì—ê²Œ 10ê°œ ì„ íƒ ìš”ì²­
            selected_indices = await self._ai_select_top_10(
                naver_products=naver_products,
                competitor_products=competitor_products,
                broadcast_time=broadcast_time,
                context=context
            )
            
            # 3. ì„ íƒëœ í•­ëª© ì¶”ì¶œ (íƒ€ì‚¬ í¸ì„± ë¨¼ì €, ë„¤ì´ë²„ ë‚˜ì¤‘)
            result = []
            
            # íƒ€ì‚¬ ì„ íƒ í•­ëª© (ìš°ì„  ë°°ì¹˜)
            for idx in selected_indices.get("competitor_indices", []):
                if 0 <= idx < len(competitor_products):
                    result.append(competitor_products[idx])
            
            # ë„¤ì´ë²„ ì„ íƒ í•­ëª© (ë’¤ì— ë°°ì¹˜)
            for idx in selected_indices.get("naver_indices", []):
                if 0 <= idx < len(naver_as_competitor):
                    result.append(naver_as_competitor[idx])
            
            logger.info(f"âœ… AI ì„ íƒ ì™„ë£Œ: ë„¤ì´ë²„ {len(selected_indices.get('naver_indices', []))}ê°œ + íƒ€ì‚¬ {len(selected_indices.get('competitor_indices', []))}ê°œ = ì´ {len(result)}ê°œ")
            
            return result[:10]  # ìµœëŒ€ 10ê°œ
            
        except Exception as e:
            logger.error(f"âš ï¸ AI ì„ íƒ ì‹¤íŒ¨, í´ë°± ë¡œì§ ì‚¬ìš©: {str(e)}")
            # í´ë°±: ë„¤ì´ë²„ 5ê°œ + íƒ€ì‚¬ 5ê°œ ë‹¨ìˆœ ì„ íƒ
            return self._fallback_select_top_10(naver_products, competitor_products)
    
    def _convert_naver_to_competitor(self, naver: NaverProduct, index: int) -> CompetitorProduct:
        """ë„¤ì´ë²„ ìƒí’ˆì„ íƒ€ì‚¬ í¸ì„± í˜•ì‹(CompetitorProduct)ìœ¼ë¡œ ë³€í™˜"""
        return CompetitorProduct(
            company_name="ë„¤ì´ë²„ ìŠ¤í† ì–´",
            broadcast_title=f"[ë„¤ì´ë²„ ì¸ê¸° {index + 1}ìœ„] {naver.name[:50]}",
            start_time="",  # ë¹ˆì¹¸
            end_time="",    # ë¹ˆì¹¸
            duration_minutes=None,
            category_main=""  # ë„¤ì´ë²„ ìƒí’ˆì—ëŠ” ì¹´í…Œê³ ë¦¬ ì •ë³´ ì—†ìŒ
        )
    
    async def _ai_select_top_10(
        self,
        naver_products: List[NaverProduct],
        competitor_products: List[CompetitorProduct],
        broadcast_time: str,
        context: Dict[str, Any] = None
    ) -> Dict[str, List[int]]:
        """
        AIë¥¼ í™œìš©í•˜ì—¬ ë„¤ì´ë²„/íƒ€ì‚¬ í¸ì„± ì¤‘ 10ê°œì˜ ì¸ë±ìŠ¤ë¥¼ ì„ íƒ
        """
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ í™ˆì‡¼í•‘ ë°©ì†¡ í¸ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

# ë°ì´í„° ì´í•´
- **ë„¤ì´ë²„ ì¸ê¸° ìƒí’ˆ**: í˜„ì¬ ì‹œì ì˜ ì‹œì¥ íŠ¸ë Œë“œë¥¼ ë°˜ì˜í•œ ì‹¤ì‹œê°„ ë² ìŠ¤íŠ¸ ìƒí’ˆ (ì‹œê°„ ë¬´ê´€)
- **íƒ€ì‚¬ í™ˆì‡¼í•‘ í¸ì„±**: íŠ¹ì • ë°©ì†¡ ì‹œê°„ëŒ€ì˜ ì‹¤ì œ í¸ì„± ì •ë³´ (ì‹œê°„ ê¸°ë°˜)

# ì„ íƒ ê¸°ì¤€
1. **ë¹„ìœ¨**: ë„¤ì´ë²„:íƒ€ì‚¬ = 5:5ë¥¼ ìµœëŒ€í•œ ìœ ì§€ (í•œìª½ ë¶€ì¡± ì‹œ ë‹¤ë¥¸ìª½ìœ¼ë¡œ ì±„ì›€)
2. **ì‹œê°„ ì í•©ì„±**: ìš”ì²­ëœ ë°©ì†¡ ì‹œê°„ëŒ€ì— ì í•©í•œ ìƒí’ˆ/í¸ì„± ì„ íƒ
3. **íŠ¸ë Œë“œ ë°˜ì˜**: ë„¤ì´ë²„ ì¸ê¸° ìƒí’ˆì„ í†µí•´ í˜„ì¬ ì‹œì¥ íŠ¸ë Œë“œ íŒŒì•…
4. **ì¹´í…Œê³ ë¦¬ ê· í˜•**: ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬ë¡œ ì‹œì²­ì ì„ íƒí­ í™•ëŒ€
5. **ê²½ìŸ ë¶„ì„**: íƒ€ì‚¬ í¸ì„±ì„ ì°¸ê³ í•˜ì—¬ ì°¨ë³„í™” ë˜ëŠ” ë²¤ì¹˜ë§ˆí‚¹

# ì„ íƒ ì „ëµ
- ë„¤ì´ë²„ ì¸ê¸° ìƒí’ˆ ì¤‘ ë°©ì†¡ ì‹œê°„ëŒ€ì™€ ì–´ìš¸ë¦¬ëŠ” íŠ¸ë Œë“œ ìƒí’ˆ ì„ íƒ
- íƒ€ì‚¬ í¸ì„± ì¤‘ í•´ë‹¹ ì‹œê°„ëŒ€ì— ê²€ì¦ëœ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ ì°¸ê³ 
- í˜„ì¬ íŠ¸ë Œë“œ(ë„¤ì´ë²„)ì™€ ì‹¤ì œ í¸ì„±(íƒ€ì‚¬)ì˜ ê· í˜• ìœ ì§€

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
  "naver_indices": [ì¸ë±ìŠ¤ ë°°ì—´],
  "competitor_indices": [ì¸ë±ìŠ¤ ë°°ì—´],
  "selection_summary": {{
    "time_match": "ì‹œê°„ëŒ€ ì í•©ì„± íŒë‹¨",
    "diversity": "ì„ íƒí•œ ìƒí’ˆë“¤ì˜ ë‹¤ì–‘ì„± ì„¤ëª…",
    "trend_analysis": "íŠ¸ë Œë“œ ë°˜ì˜ ë°©ì‹"
  }},
  "selection_reason": "ì „ì²´ ì„ íƒ ê·¼ê±° 2-3ë¬¸ì¥"
}}"""),
            ("user", """ë°©ì†¡ ì‹œê°„: {broadcast_time}

ë„¤ì´ë²„ ì¸ê¸° ìƒí’ˆ ({naver_count}ê°œ):
{naver_summary}

íƒ€ì‚¬ í™ˆì‡¼í•‘ í¸ì„± ({competitor_count}ê°œ):
{competitor_summary}

ìœ„ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ë°©ì†¡ ì‹œê°„({broadcast_time})ì— ìµœì í™”ëœ 10ê°œë¥¼ ì„ íƒí•˜ì„¸ìš”.""")
        ])
        
        # ë„¤ì´ë²„ ìƒí’ˆ ìš”ì•½
        naver_summary = "\n".join([
            f"[{i}] {p.name[:40]} | ê°€ê²©: {p.sale_price:,}ì› | í• ì¸: {p.discount_ratio}% | íŒë§¤ëŸ‰: {p.cumulation_sale_count}"
            for i, p in enumerate(naver_products[:20])  # ìµœëŒ€ 20ê°œë§Œ ì „ë‹¬
        ])
        
        # íƒ€ì‚¬ í¸ì„± ìš”ì•½
        competitor_summary = "\n".join([
            f"[{i}] {c.company_name} | {c.broadcast_title[:40]} | {c.start_time} ~ {c.end_time} | {c.category_main or 'ë¯¸ë¶„ë¥˜'}"
            for i, c in enumerate(competitor_products[:20])  # ìµœëŒ€ 20ê°œë§Œ ì „ë‹¬
        ])
        
        # LLM í˜¸ì¶œ
        chain = prompt_template | self.llm | JsonOutputParser()
        
        result = await chain.ainvoke({
            "broadcast_time": broadcast_time or "ë¯¸ì§€ì •",
            "naver_count": len(naver_products),
            "competitor_count": len(competitor_products),
            "naver_summary": naver_summary or "ì—†ìŒ",
            "competitor_summary": competitor_summary or "ì—†ìŒ"
        })
        
        logger.info(f"AI ì„ íƒ ê·¼ê±°: {result.get('selection_reason', 'ì—†ìŒ')}")
        
        return result
    
    def _fallback_select_top_10(
        self,
        naver_products: List[NaverProduct],
        competitor_products: List[CompetitorProduct]
    ) -> List[CompetitorProduct]:
        """AI ì‹¤íŒ¨ ì‹œ í´ë°±: ë‹¨ìˆœ 5:5 ì„ íƒ (íƒ€ì‚¬ ë¨¼ì €, ë„¤ì´ë²„ ë‚˜ì¤‘)"""
        result = []
        
        # íƒ€ì‚¬ 5ê°œ (ë˜ëŠ” ê°€ëŠ¥í•œ ë§Œí¼) - ìš°ì„  ë°°ì¹˜
        competitor_count = min(5, len(competitor_products))
        for i in range(competitor_count):
            result.append(competitor_products[i])
        
        # ë„¤ì´ë²„ 5ê°œ (ë˜ëŠ” ê°€ëŠ¥í•œ ë§Œí¼) - ë’¤ì— ë°°ì¹˜
        naver_count = min(5, len(naver_products))
        for i in range(naver_count):
            result.append(self._convert_naver_to_competitor(naver_products[i], i))
        
        # 10ê°œ ë¯¸ë§Œì´ë©´ ë‚˜ë¨¸ì§€ë¡œ ì±„ì›€
        if len(result) < 10:
            remaining = 10 - len(result)
            if competitor_count < len(competitor_products):
                for i in range(competitor_count, min(competitor_count + remaining, len(competitor_products))):
                    result.append(competitor_products[i])
            elif naver_count < len(naver_products):
                for i in range(naver_count, min(naver_count + remaining, len(naver_products))):
                    result.append(self._convert_naver_to_competitor(naver_products[i], i))
        
        logger.info(f"í´ë°± ì„ íƒ: íƒ€ì‚¬ ìš°ì„ , ì´ {len(result)}ê°œ")
        return result[:10]
    
    # ì¶œì²˜ ìœ í˜•ë³„ í¬ë§· í…œí”Œë¦¿ (ë” ìƒì„¸í•˜ê²Œ)
    SOURCE_TEMPLATES = {
        "news_trend": "[ë‰´ìŠ¤] '{keyword}' | {title} | URL: {url}",
        "ai_trend": "[AIíŠ¸ë Œë“œ] '{keyword}' - {reason}",
        "rag_match": "[í‚¤ì›Œë“œë§¤ì¹­] '{keyword}' ({origin})",
        "xgboost_sales": "[ë§¤ì¶œì˜ˆì¸¡] {sales}ë§Œì› ({rank}ìœ„)",
        "context": "[ì»¨í…ìŠ¤íŠ¸] {factor}",
        "competitor": "[ê²½ìŸì‚¬] {name} {time} í¸ì„± ì¤‘"
    }
    
    # í‚¤ì›Œë“œ ì¶œì²˜ ë§¤í•‘
    KEYWORD_ORIGIN_MAP = {
        "news": "ë‰´ìŠ¤ íŠ¸ë Œë“œ",
        "ai": "AI ìƒì„±",
        "context": "ì»¨í…ìŠ¤íŠ¸",
        "unknown": "ê²€ìƒ‰"
    }
    
    def _format_source_description(self, src: Dict[str, Any]) -> str:
        """ì¶œì²˜ ì •ë³´ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        src_type = src.get("source_type", "")
        
        if src_type == "news_trend":
            keyword = src.get("news_keyword", "")
            title = src.get("news_title", "")[:50] if src.get("news_title") else "ìµœê·¼ ê¸°ì‚¬"
            url = src.get("news_url", "") or "ì—†ìŒ"
            return self.SOURCE_TEMPLATES["news_trend"].format(keyword=keyword, title=title, url=url) if keyword else ""
        
        elif src_type == "ai_trend":
            keyword = src.get("ai_keyword", "")
            reason = src.get("ai_reason", "ì‹œì¦Œ íŠ¸ë Œë“œ")
            return self.SOURCE_TEMPLATES["ai_trend"].format(keyword=keyword, reason=reason) if keyword else ""
        
        elif src_type == "rag_match":
            keyword = src.get("matched_keyword", "")
            origin = self.KEYWORD_ORIGIN_MAP.get(src.get("keyword_origin", "unknown"), "ê²€ìƒ‰")
            return self.SOURCE_TEMPLATES["rag_match"].format(keyword=keyword, origin=origin) if keyword else ""
        
        elif src_type == "xgboost_sales":
            sales = int(src.get("predicted_sales", 0) / 10000)
            rank = src.get("xgboost_rank", 0)
            return self.SOURCE_TEMPLATES["xgboost_sales"].format(sales=f"{sales:,}", rank=rank)
        
        elif src_type == "context":
            factor = src.get("context_factor", "")
            return self.SOURCE_TEMPLATES["context"].format(factor=factor) if factor else ""
        
        elif src_type == "competitor":
            name = src.get("competitor_name", "")
            time = src.get("competitor_time", "")
            return self.SOURCE_TEMPLATES["competitor"].format(name=name, time=time) if name else ""
        
        return ""
    
    def _format_product_info(self, rank: int, name: str, category: str, sales: int, sources: List[str]) -> str:
        """ìƒí’ˆ ì •ë³´ë¥¼ í”„ë¡¬í”„íŠ¸ìš© í…ìŠ¤íŠ¸ë¡œ ë³€í™˜"""
        sources_str = " / ".join(sources) if sources else "ì¶œì²˜ ì—†ìŒ"
        return f"{rank}. {name[:50]} | {category} | {sales:,}ë§Œì›\n   ì¶œì²˜: {sources_str}"
    
    def _calculate_keyword_rankings(self, candidates: List[Dict[str, Any]]) -> Dict[str, int]:
        """í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ê¸°ì¤€ ìˆœìœ„ ê³„ì‚°"""
        # í‚¤ì›Œë“œ ë§¤ì¹­ ì ìˆ˜ ì¶”ì¶œ (similarity_score ê¸°ì¤€)
        scores = []
        for c in candidates:
            product_code = c.get("product", {}).get("product_code", "")
            similarity = c.get("similarity", 0)
            # recommendation_sourcesì—ì„œ rag_matchì˜ similarity_scoreë„ í™•ì¸
            for src in c.get("recommendation_sources", []):
                if src.get("source_type") == "rag_match":
                    similarity = max(similarity, src.get("similarity_score", 0))
            scores.append((product_code, similarity))
        
        # ì ìˆ˜ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ í›„ ìˆœìœ„ ë¶€ì—¬
        scores.sort(key=lambda x: x[1], reverse=True)
        return {code: rank + 1 for rank, (code, _) in enumerate(scores)}
    
    def _calculate_sales_rankings(self, candidates: List[Dict[str, Any]]) -> Dict[str, int]:
        """ë§¤ì¶œ ì˜ˆì¸¡ ê¸°ì¤€ ìˆœìœ„ ê³„ì‚°"""
        scores = []
        for c in candidates:
            product_code = c.get("product", {}).get("product_code", "")
            predicted_sales = c.get("predicted_sales", 0)
            scores.append((product_code, predicted_sales))
        
        # ë§¤ì¶œ ë‚´ë¦¼ì°¨ìˆœ ì •ë ¬ í›„ ìˆœìœ„ ë¶€ì—¬
        scores.sort(key=lambda x: x[1], reverse=True)
        return {code: rank + 1 for rank, (code, _) in enumerate(scores)}
    
    def _format_sources_with_rankings(self, sources: List[Dict], keyword_rank: int, sales_rank: int, total: int) -> List[str]:
        """ì¶œì²˜ ì •ë³´ë¥¼ ìˆœìœ„ì™€ í•¨ê»˜ í¬ë§·íŒ… (ìƒìœ„ê¶Œë§Œ ìˆœìœ„ í‘œì‹œ)"""
        result = []
        top_threshold = 10  # ìƒìœ„ 10ìœ„ê¹Œì§€ ìˆœìœ„ í‘œì‹œ
        
        for src in sources:
            src_type = src.get("source_type", "")
            
            if src_type == "news_trend":
                keyword = src.get("news_keyword", "")
                title = src.get("news_title", "")[:50] if src.get("news_title") else "ìµœê·¼ ê¸°ì‚¬"
                url = src.get("news_url", "") or "ì—†ìŒ"
                if keyword:
                    result.append(f"[ë‰´ìŠ¤] '{keyword}' | {title} | URL: {url}")
            
            elif src_type == "ai_trend":
                keyword = src.get("ai_keyword", "")
                reason = src.get("ai_reason", "ì‹œì¦Œ íŠ¸ë Œë“œ")
                if keyword:
                    # ìƒìœ„ê¶Œì¼ ë•Œë§Œ ìˆœìœ„ í‘œì‹œ
                    if keyword_rank <= top_threshold:
                        result.append(f"[AIíŠ¸ë Œë“œ] '{keyword}' - {reason} (í‚¤ì›Œë“œ {keyword_rank}ìœ„)")
                    else:
                        result.append(f"[AIíŠ¸ë Œë“œ] '{keyword}' - {reason}")
            
            elif src_type == "rag_match":
                keyword = src.get("matched_keyword", "")
                origin = self.KEYWORD_ORIGIN_MAP.get(src.get("keyword_origin", "unknown"), "ê²€ìƒ‰")
                if keyword:
                    if keyword_rank <= top_threshold:
                        result.append(f"[í‚¤ì›Œë“œë§¤ì¹­] '{keyword}' ({origin}) - í‚¤ì›Œë“œ {keyword_rank}ìœ„")
                    else:
                        result.append(f"[í‚¤ì›Œë“œë§¤ì¹­] '{keyword}' ({origin})")
            
            elif src_type == "xgboost_sales":
                sales = int(src.get("predicted_sales", 0) / 10000)
                # ìƒìœ„ê¶Œì¼ ë•Œë§Œ ìˆœìœ„ í‘œì‹œ
                if sales_rank <= top_threshold:
                    result.append(f"[ë§¤ì¶œì˜ˆì¸¡] {sales:,}ë§Œì› (ë§¤ì¶œ {sales_rank}ìœ„)")
                else:
                    result.append(f"[ë§¤ì¶œì˜ˆì¸¡] {sales:,}ë§Œì›")
            
            elif src_type == "context":
                factor = src.get("context_factor", "")
                if factor:
                    result.append(f"[ì»¨í…ìŠ¤íŠ¸] {factor}")
            
            elif src_type == "competitor":
                name = src.get("competitor_name", "")
                time = src.get("competitor_time", "")
                if name:
                    result.append(f"[ê²½ìŸì‚¬] {name} {time} í¸ì„± ì¤‘")
            
            elif src_type == "sales_top":
                # Track B: ë§¤ì¶œ ì˜ˆì¸¡ ìƒìœ„ (í‚¤ì›Œë“œ ë¬´ê´€)
                if sales_rank <= top_threshold:
                    result.append(f"[ë§¤ì¶œìƒìœ„] í‚¤ì›Œë“œ ë¬´ê´€ ë§¤ì¶œ ì˜ˆì¸¡ ìƒìœ„ (ë§¤ì¶œ {sales_rank}ìœ„)")
                else:
                    result.append(f"[ë§¤ì¶œìƒìœ„] í‚¤ì›Œë“œ ë¬´ê´€ ë§¤ì¶œ ì˜ˆì¸¡ ìƒìœ„")
        
        return result
    
    def _validate_reasons_response(self, result: Any, expected_count: int) -> List[str]:
        """LLM ì‘ë‹µì˜ reasons í•„ë“œ ê²€ì¦"""
        # resultê°€ Noneì¸ ê²½ìš°
        if result is None:
            logger.warning("[ê²€ì¦] LLM ì‘ë‹µì´ None")
            return []
        
        # resultê°€ dictê°€ ì•„ë‹Œ ê²½ìš°
        if not isinstance(result, dict):
            logger.warning(f"[ê²€ì¦] LLM ì‘ë‹µì´ dictê°€ ì•„ë‹˜: {type(result)}")
            return []
        
        # reasons í•„ë“œê°€ ì—†ëŠ” ê²½ìš°
        reasons = result.get("reasons")
        if reasons is None:
            logger.warning("[ê²€ì¦] reasons í•„ë“œ ì—†ìŒ")
            return []
        
        # reasonsê°€ listê°€ ì•„ë‹Œ ê²½ìš°
        if not isinstance(reasons, list):
            logger.warning(f"[ê²€ì¦] reasonsê°€ listê°€ ì•„ë‹˜: {type(reasons)}")
            return []
        
        # ê° í•­ëª©ì´ ë¬¸ìì—´ì¸ì§€ ê²€ì¦
        validated = []
        for i, reason in enumerate(reasons):
            if isinstance(reason, str) and reason.strip():
                validated.append(reason.strip())
            else:
                logger.warning(f"[ê²€ì¦] reasons[{i}]ê°€ ìœ íš¨í•˜ì§€ ì•ŠìŒ: {type(reason)}")
        
        # ê°œìˆ˜ ê²€ì¦
        if len(validated) != expected_count:
            logger.info(f"[ê²€ì¦] ê°œìˆ˜ ë¶ˆì¼ì¹˜: ê¸°ëŒ€ {expected_count}, ì‹¤ì œ {len(validated)}")
        
        return validated
    
    async def _generate_batch_reasons_with_langchain(self, candidates: List[Dict[str, Any]], context: Dict[str, Any] = None) -> List[str]:
        """ë°°ì¹˜ë¡œ ì—¬ëŸ¬ ìƒí’ˆì˜ ì¶”ì²œ ê·¼ê±°ë¥¼ í•œ ë²ˆì— ìƒì„±"""
        try:
            time_slot = context.get("time_slot", "") if context else ""
            weather = context.get("weather", {}).get("weather", "") if context else ""
            holiday_name = context.get("holiday_name") if context else None
            
            # í‚¤ì›Œë“œ ìˆœìœ„ì™€ ë§¤ì¶œ ìˆœìœ„ë¥¼ ë”°ë¡œ ê³„ì‚°
            keyword_rankings = self._calculate_keyword_rankings(candidates)
            sales_rankings = self._calculate_sales_rankings(candidates)
            
            # ìƒí’ˆë³„ ì¶œì²˜ ì •ë³´ í¬ë§·íŒ… (ìˆœìœ„ ì •ë³´ í¬í•¨)
            products_with_sources = []
            for candidate in candidates:
                product = candidate["product"]
                product_code = product.get("product_code", "")
                
                # ìˆœìœ„ ì •ë³´ ê°€ì ¸ì˜¤ê¸°
                keyword_rank = keyword_rankings.get(product_code, 0)
                sales_rank = sales_rankings.get(product_code, 0)
                
                # ì¶œì²˜ ì •ë³´ í¬ë§·íŒ… (ìˆœìœ„ ì •ë³´ í¬í•¨)
                sources = self._format_sources_with_rankings(
                    candidate.get("recommendation_sources", []),
                    keyword_rank, sales_rank, len(candidates)
                )
                
                products_with_sources.append({
                    "rank": candidate.get("rank", 0),
                    "product_name": product.get("product_name", ""),
                    "category": product.get("category_main", ""),
                    "predicted_sales": int(candidate.get("predicted_sales", 0) / 10000),
                    "keyword_rank": keyword_rank,
                    "sales_rank": sales_rank,
                    "sources": sources
                })
            
            # í”„ë¡¬í”„íŠ¸ - ìì—°ìŠ¤ëŸ½ê³  êµ¬ì²´ì ì¸ ì¶”ì²œ ê·¼ê±° (ìˆœìœ„ ì •ë³´ í¬í•¨)
            batch_prompt = ChatPromptTemplate.from_messages([
                ("system", """í™ˆì‡¼í•‘ ë°©ì†¡ í¸ì„± ì „ë¬¸ê°€ë¡œì„œ ìì—°ìŠ¤ëŸ¬ìš´ ì¶”ì²œ ê·¼ê±°ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

**ì˜¤ëŠ˜ ë‚ ì§œ: {today_date}**

ì¶œì²˜ ìœ í˜•ë³„ ì‘ì„± ë°©ë²•:

1. **[ë‰´ìŠ¤] ì¶œì²˜:**
   "ìµœê·¼ ë‰´ìŠ¤ì— ë”°ë¥´ë©´ 'í”„ë¦¬ë¯¸ì—„ ì—¬í–‰ìƒí’ˆ' ê´€ë ¨ ê¸°ì‚¬ê°€ ë³´ë„ë˜ì—ˆìŠµë‹ˆë‹¤. ì´ì— í•´ë‹¹ ìƒí’ˆì˜ í¸ì„±ì„ ì¶”ì²œí•©ë‹ˆë‹¤. (ì¶œì²˜: URL)"

2. **[AIíŠ¸ë Œë“œ] ì¶œì²˜:**
   "{today_date} íŠ¸ë Œë“œ í‚¤ì›Œë“œ ë¶„ì„ ê²°ê³¼ 'ê²¨ìš¸ ì˜ë¥˜' í‚¤ì›Œë“œ 1ìœ„ë¡œ ì í•©í•œ ìƒí’ˆì…ë‹ˆë‹¤."

3. **[ë§¤ì¶œì˜ˆì¸¡] ì¶œì²˜:**
   "AI ë§¤ì¶œ ì˜ˆì¸¡ ê²°ê³¼ 1,135ë§Œì›ìœ¼ë¡œ ë§¤ì¶œ 1ìœ„ë¥¼ ê¸°ë¡í–ˆìŠµë‹ˆë‹¤."

4. **[ê²½ìŸì‚¬] ì¶œì²˜:**
   "{today_date} {time_slot} ê²½ìŸì‚¬ ë¡¯ë°í™ˆì‡¼í•‘ì—ì„œ ìœ ì‚¬ ìƒí’ˆ íŒë§¤ ì¤‘ìœ¼ë¡œ, í•´ë‹¹ ì‹œê°„ëŒ€ í¸ì„±ì„ ì¶”ì²œí•©ë‹ˆë‹¤."

5. **[ë§¤ì¶œìƒìœ„] ì¶œì²˜:**
   "íŠ¸ë Œë“œ í‚¤ì›Œë“œì™€ ë¬´ê´€í•˜ê²Œ AI ë§¤ì¶œ ì˜ˆì¸¡ ìƒìœ„ ìƒí’ˆìœ¼ë¡œ, ì•ˆì •ì ì¸ ë§¤ì¶œì´ ê¸°ëŒ€ë©ë‹ˆë‹¤."

ê·œì¹™:
- ê° ìƒí’ˆ 100-150ì
- ì¶œì²˜ì— ìˆœìœ„ê°€ í‘œì‹œëœ ê²½ìš°ì—ë§Œ ìˆœìœ„ ì–¸ê¸‰ (ìƒìœ„ 10ìœ„ê¹Œì§€ë§Œ ìˆœìœ„ í‘œì‹œë¨)
- "[ë‰´ìŠ¤]", "[AI]" íƒœê·¸ë¥¼ ìì—°ì–´ë¡œ ë³€í™˜
- ë‰´ìŠ¤ URLì´ ìˆìœ¼ë©´ "(ì¶œì²˜: URL)" í˜•íƒœë¡œ ëì— ì¶”ê°€

JSON: {{"reasons": ["ê·¼ê±°1", "ê·¼ê±°2", ...]}}"""),
                ("human", """ì‹œê°„ëŒ€: {time_slot} | ë‚ ì”¨: {weather}

{products_info}

{count}ê°œ ìƒí’ˆì˜ ì¶”ì²œ ê·¼ê±°ë¥¼ ìì—°ìŠ¤ëŸ½ê²Œ ì‘ì„±í•˜ì„¸ìš”.""")
            ])
            
            # ìƒí’ˆ ì •ë³´ í¬ë§·íŒ…
            products_info = "\n".join([
                self._format_product_info(p["rank"], p["product_name"], p["category"], p["predicted_sales"], p["sources"])
                for p in products_with_sources
            ])
            
            # ë””ë²„ê·¸: ì‹¤ì œ ì „ë‹¬ë˜ëŠ” ìƒí’ˆ ì •ë³´ ì¶œë ¥
            print(f"\n[DEBUG] LLMì— ì „ë‹¬ë˜ëŠ” ìƒí’ˆ ì •ë³´:\n{products_info[:500]}...")
            
            chain = batch_prompt | self.llm | JsonOutputParser()
            
            # ì˜¤ëŠ˜ ë‚ ì§œ ìƒì„±
            from datetime import datetime
            today_date = datetime.now().strftime("%mì›” %dì¼")
            
            result = await chain.ainvoke({
                "time_slot": time_slot or "ë¯¸ì§€ì •",
                "weather": weather or "ë³´í†µ",
                "today_date": today_date,
                "products_info": products_info,
                "count": len(candidates)
            })
            
            # JSON íŒŒì‹± ê²€ì¦
            reasons = self._validate_reasons_response(result, len(candidates))
            print(f"[ë°°ì¹˜ ì²˜ë¦¬] {len(reasons)}ê°œ ê·¼ê±° ìƒì„± ì™„ë£Œ")
            
            # ê°œìˆ˜ê°€ ë¶€ì¡±í•˜ë©´ ì¶œì²˜ ê¸°ë°˜ ê¸°ë³¸ ë©”ì‹œì§€ë¡œ ì±„ì›€
            while len(reasons) < len(candidates):
                idx = len(reasons)
                candidate = candidates[idx]
                sources = candidate.get("recommendation_sources", [])
                
                # ì¶œì²˜ ê¸°ë°˜ í´ë°± ë©”ì‹œì§€ ìƒì„±
                fallback_reason = self._generate_fallback_reason(candidate, sources)
                reasons.append(fallback_reason)
            
            return reasons[:len(candidates)]
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ê·¼ê±° ìƒì„± ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            # í´ë°±: ì¶œì²˜ ê¸°ë°˜ ê¸°ë³¸ ë©”ì‹œì§€
            print("âš ï¸ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨, ì¶œì²˜ ê¸°ë°˜ í´ë°±...")
            return [self._generate_fallback_reason(c, c.get("recommendation_sources", [])) for c in candidates]
    
    def _generate_reasoning_by_code(self, candidate: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """
        ì½”ë”© ë°©ì‹ìœ¼ë¡œ ì¶”ì²œ ê·¼ê±° ìƒì„± (LLM ë¯¸ì‚¬ìš©)
        
        Returns:
            {
                "reasoning": "ì¶”ì²œ ê·¼ê±° í…ìŠ¤íŠ¸",
                "scores": {
                    "total": 0.85,
                    "keyword_score": 0.70,
                    "sales_score": 0.15,
                    "historical_score": 0.00
                },
                "keyword_source": {
                    "type": "news" | "ai" | "context" | "historical",
                    "keyword": "í‚¤ì›Œë“œ",
                    "news_url": "URL (ë‰´ìŠ¤ì¸ ê²½ìš°)"
                }
            }
        """
        product = candidate.get("product", {})
        sources = candidate.get("recommendation_sources", [])
        predicted_sales = candidate.get("predicted_sales", 0)
        similarity_score = candidate.get("similarity_score", 0)
        final_score = candidate.get("final_score", 0)
        candidate_source = candidate.get("source", "")  # Track ì¶œì²˜ (competitor, sales_top, historical, news_trend, ai_trend)
        
        # ì¶œì²˜ ì •ë³´ íŒŒì‹±
        news_info = None
        ai_info = None
        rag_info = None
        historical_info = None
        sales_top_info = None
        competitor_info = None
        context_info = None
        
        # candidate.source ê¸°ë°˜ìœ¼ë¡œ ì¶œì²˜ ì •ë³´ ì„¤ì •
        if candidate_source == "sales_top":
            sales_top_info = {"reason": "í‚¤ì›Œë“œ ë¬´ê´€ ë§¤ì¶œ ì˜ˆì¸¡ ìƒìœ„ ìƒí’ˆ"}
        elif candidate_source == "historical":
            historical_info = {
                "avg_profit": product.get("historical_avg_profit", 0),
                "broadcast_count": product.get("historical_broadcast_count", 0),
                "reason": "ê³¼ê±° ìœ ì‚¬ ì‹œê°„ëŒ€ ì‹¤ì  ìƒìœ„"
            }
        elif candidate_source == "competitor":
            comp_info = product.get("competitor_info", {})
            competitor_info = {
                "company": comp_info.get("company", "ê²½ìŸì‚¬"),
                "title": comp_info.get("title", ""),
                "keyword": product.get("matched_keyword", "")
            }
        
        for src in sources:
            src_type = src.get("source_type", "")
            if src_type == "news_trend" and not news_info:
                news_info = {
                    "keyword": src.get("news_keyword", ""),
                    "title": src.get("news_title", ""),
                    "url": src.get("news_url", "")
                }
            elif src_type == "ai_trend" and not ai_info:
                ai_info = {"keyword": src.get("ai_keyword", ""), "reason": src.get("ai_reason", "")}
            elif src_type == "rag_match" and not rag_info:
                rag_info = {
                    "keyword": src.get("matched_keyword", ""),
                    "similarity": src.get("similarity_score", 0),
                    "origin": src.get("keyword_origin", "unknown"),
                    "origin_detail": src.get("keyword_origin_detail", "")
                }
            elif src_type == "historical" and not historical_info:
                historical_info = {
                    "avg_profit": src.get("historical_avg_profit", 0),
                    "broadcast_count": src.get("historical_broadcast_count", 0),
                    "reason": src.get("reason", "")
                }
            elif src_type == "sales_top" and not sales_top_info:
                sales_top_info = {"reason": src.get("reason", "")}
            elif src_type == "competitor" and not competitor_info:
                competitor_info = {
                    "company": src.get("competitor_company", ""),
                    "title": src.get("competitor_title", ""),
                    "keyword": src.get("competitor_keyword", "")
                }
            elif src_type == "context" and not context_info:
                context_info = {"factor": src.get("context_factor", "")}
        
        # ì ìˆ˜ ê³„ì‚° (ì„¸ë¶„í™”)
        keyword_score = similarity_score * 0.7 if similarity_score >= 0.7 else similarity_score * 0.3
        sales_score = (predicted_sales / 100000000) * (0.3 if similarity_score >= 0.7 else 0.7)
        historical_score = 0.0
        if historical_info and historical_info["avg_profit"] > 0:
            historical_score = min(historical_info["avg_profit"] / 50000000, 0.2)  # ìµœëŒ€ 0.2
        
        scores = {
            "total": round(final_score, 3),
            "keyword_score": round(keyword_score, 3),
            "sales_score": round(sales_score, 3),
            "historical_score": round(historical_score, 3)
        }
        
        # í‚¤ì›Œë“œ ì¶œì²˜ ì •ë³´
        keyword_source = {"type": "unknown", "keyword": "", "news_url": None}
        
        if rag_info and rag_info["keyword"]:
            keyword_source["keyword"] = rag_info["keyword"]
            if rag_info["origin"] == "news" or news_info:
                keyword_source["type"] = "news"
                if news_info and news_info.get("url"):
                    keyword_source["news_url"] = news_info["url"]
            elif rag_info["origin"] == "ai" or ai_info:
                keyword_source["type"] = "ai"
            elif rag_info["origin"] == "context":
                keyword_source["type"] = "context"
            else:
                keyword_source["type"] = "ai"  # ê¸°ë³¸ê°’
        
        if historical_info:
            keyword_source["type"] = "historical"
        
        # ì¶”ì²œ ê·¼ê±° í…ìŠ¤íŠ¸ ìƒì„±
        parts = []
        
        # 0. ì—¬ëŸ¬ ì¶œì²˜ í‘œì‹œ (source_labels í™œìš©)
        source_labels = product.get("source_labels", [])
        if source_labels:
            source_tag = "|".join(source_labels)
            parts.append(f"[{source_tag}]")
        
        # 1. í‚¤ì›Œë“œ ì¶œì²˜ ìƒì„¸ (ë‰´ìŠ¤ URL ë“±) - ë³µí•© ì¶œì²˜ë©´ ëª¨ë‘ í‘œì‹œ
        # ë‹¨, ê²½ìŸì‚¬ ìƒí’ˆì€ ê²½ìŸì‚¬ ì •ë³´ì—ì„œ í‚¤ì›Œë“œê°€ í‘œì‹œë˜ë¯€ë¡œ AI íŠ¸ë Œë“œ ìƒëµ
        product_comp_info_check = product.get("competitor_info", {})
        is_competitor_only = product_comp_info_check and product_comp_info_check.get("company")
        
        if news_info and news_info["keyword"]:
            keyword_part = f"[ë‰´ìŠ¤] '{news_info['keyword']}' íŠ¸ë Œë“œ"
            if news_info.get("url"):
                keyword_part += f" (ì¶œì²˜: {news_info['url'][:50]}...)"
            parts.append(keyword_part)
        elif ai_info and ai_info["keyword"] and not is_competitor_only:
            # ê²½ìŸì‚¬ ìƒí’ˆì´ ì•„ë‹Œ ê²½ìš°ì—ë§Œ AI íŠ¸ë Œë“œ í‘œì‹œ
            parts.append(f"[AI] '{ai_info['keyword']}' ì‹œì¦Œ íŠ¸ë Œë“œ")
        elif rag_info and rag_info["keyword"]:
            parts.append(f"[RAG] '{rag_info['keyword']}' ë§¤ì¹­ (ìœ ì‚¬ë„: {rag_info['similarity']:.0%})")
        
        # 2. Track C (ê³¼ê±° ì‹¤ì ) ìƒì„¸ - ìµœê·¼ ë°©ì†¡ì¼ì/ë§¤ì¶œ í¬í•¨
        if historical_info and historical_info["avg_profit"] > 0:
            # ìµœê·¼ ë°©ì†¡ ì •ë³´ (1ê±´)
            last_broadcast_time = product.get("last_broadcast_time", "")
            last_profit = product.get("last_profit", 0)
            
            if last_broadcast_time and last_profit > 0:
                # ìµœê·¼ ë°©ì†¡ì¼ìì™€ ë§¤ì¶œ í‘œì‹œ
                last_date = str(last_broadcast_time)[:10]  # YYYY-MM-DD
                last_time = str(last_broadcast_time)[11:16] if len(str(last_broadcast_time)) > 11 else ""  # HH:MM
                last_profit_str = f"{int(last_profit/10000):,}ë§Œì›"
                hist_detail = f"[ê³¼ê±°ì‹¤ì ] ìµœê·¼ {last_date} {last_time} ë°©ì†¡ ë§¤ì¶œ {last_profit_str}"
                # í‰ê·  ì •ë³´ë„ ì¶”ê°€
                avg_profit_str = f"{int(historical_info['avg_profit']/10000):,}ë§Œì›"
                hist_detail += f" (í‰ê·  {avg_profit_str}, {historical_info['broadcast_count']}íšŒ)"
            else:
                # ìµœê·¼ ì •ë³´ê°€ ì—†ìœ¼ë©´ í‰ê· ë§Œ í‘œì‹œ
                avg_profit_str = f"{int(historical_info['avg_profit']/10000):,}ë§Œì›"
                hist_detail = f"[ê³¼ê±°ì‹¤ì ] ìœ ì‚¬ ì‹œê°„ëŒ€ í‰ê·  {avg_profit_str} ({historical_info['broadcast_count']}íšŒ ë°©ì†¡)"
            
            parts.append(hist_detail)
        
        # 3. Track D (ê²½ìŸì‚¬ ëŒ€ì‘) ìƒì„¸ - ë°©ì†¡ì‚¬, í¸ì„±ì œëª©, ì‹œê°„ í¬í•¨
        product_comp_info = product.get("competitor_info", {})
        if product_comp_info and product_comp_info.get("company"):
            comp_company = product_comp_info.get("company", "")
            comp_title = product_comp_info.get("title", "")
            comp_time = product_comp_info.get("start_time", "")
            comp_keyword = product_comp_info.get("keyword", "")  # ê²½ìŸì‚¬ í¸ì„±ì—ì„œ ì¶”ì¶œí•œ í‚¤ì›Œë“œ
            
            # ìƒì„¸ ê²½ìŸì‚¬ ì •ë³´: "GSí™ˆì‡¼í•‘ 'ë¡œë´‡ì²­ì†Œê¸° íŠ¹ê°€' (14:00) í‚¤ì›Œë“œ:'ë¡œë´‡ì²­ì†Œê¸°'"
            comp_part = f"[ê²½ìŸì‚¬ëŒ€ì‘] {comp_company}"
            if comp_title:
                comp_part += f" '{comp_title[:30]}'"
            if comp_time:
                time_str = comp_time[11:16] if len(comp_time) > 11 else comp_time
                comp_part += f" ({time_str})"
            if comp_keyword:
                comp_part += f" í‚¤ì›Œë“œ:'{comp_keyword}'"
            parts.append(comp_part)
        elif competitor_info and competitor_info.get("company"):
            comp_part = f"[ê²½ìŸì‚¬ëŒ€ì‘] {competitor_info['company']}"
            if competitor_info.get("keyword"):
                comp_part += f" '{competitor_info['keyword']}' í¸ì„±"
            parts.append(comp_part)
        
        # 4. ë§¤ì¶œ ì˜ˆì¸¡
        sales_str = f"{int(predicted_sales/10000):,}ë§Œì›"
        parts.append(f"[ì˜ˆì¸¡ë§¤ì¶œ] {sales_str}")
        
        # 6. ìµœì¢… ì ìˆ˜
        parts.append(f"[ì ìˆ˜] ì´ì  {final_score:.3f} (í‚¤ì›Œë“œ {keyword_score:.3f} + ë§¤ì¶œ {sales_score:.3f})")
        
        reasoning = " | ".join(parts)
        
        return {
            "reasoning": reasoning,
            "scores": scores,
            "keyword_source": keyword_source
        }
    
    def _generate_fallback_reason(self, candidate: Dict[str, Any], sources: List[Dict]) -> str:
        """ì¶œì²˜ ê¸°ë°˜ í´ë°± ì¶”ì²œ ê·¼ê±° ìƒì„± - ì—¬ëŸ¬ ì¶œì²˜ ì¡°í•© (í•˜ìœ„ í˜¸í™˜ìš©)"""
        result = self._generate_reasoning_by_code(candidate, {})
        return result["reasoning"]
    
    def _prepare_features_for_product(self, product: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """1ê°œ ìƒí’ˆì˜ XGBoost feature ì¤€ë¹„ (ì˜ˆì¸¡ì€ ì•ˆ í•¨)
        
        2024-12-15 ìˆ˜ì •: ì‹œê°„ëŒ€/ì›” í”¼ì²˜ ê°•í™”, ë‚ ì”¨/ê°€ê²© í”¼ì²˜ ì œê±°
        - ì‹œê°„: 9ì‹œì— íŒ”ë¦° ìƒí’ˆ â†’ 8~10ì‹œì— ì¶”ì²œ
        - ì›”: 11ì›”ì— íŒ”ë¦° ìƒí’ˆ â†’ 10~12ì›”ì— ì¶”ì²œ
        """
        broadcast_dt = context["broadcast_dt"]
        
        print(f"=== [_prepare_features_for_product] í˜¸ì¶œë¨: {product.get('product_name', 'Unknown')[:30]} ===")
        
        category_main = product.get("category_main", product.get("category", "Unknown"))
        time_slot = context["time_slot"]
        
        # ì‹œê°„ í”¼ì²˜: ì‚¬ì¸/ì½”ì‚¬ì¸ ë³€í™˜ (ì£¼ê¸°ì„± ë°˜ì˜)
        hour = broadcast_dt.hour
        hour_sin = np.sin(2 * np.pi * hour / 24)
        hour_cos = np.cos(2 * np.pi * hour / 24)
        
        # ì›” í”¼ì²˜: ì‚¬ì¸/ì½”ì‚¬ì¸ ë³€í™˜ (ì£¼ê¸°ì„± ë°˜ì˜)
        month = broadcast_dt.month
        month_sin = np.sin(2 * np.pi * month / 12)
        month_cos = np.cos(2 * np.pi * month / 12)
        
        return {
            # Numeric features - ì‹œê°„ëŒ€/ì›” ê°•í™” (ë‚ ì”¨/ê°€ê²© ì œê±°)
            "hour": hour,
            "hour_sin": hour_sin,
            "hour_cos": hour_cos,
            "month": month,
            "month_sin": month_sin,
            "month_cos": month_cos,
            
            # Categorical features (ë‚ ì”¨/ê³„ì ˆ ì œê±°)
            "product_lgroup": category_main,
            "product_mgroup": product.get("category_middle", "Unknown"),
            "product_sgroup": product.get("category_sub", "Unknown"),
            "brand": product.get("brand", "Unknown"),
            "product_type": product.get("product_type", "ìœ í˜•"),
            "time_slot": time_slot,  # í•µì‹¬ í”¼ì²˜
            "day_of_week": ["ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ", "í† ", "ì¼"][broadcast_dt.weekday()],  # í•µì‹¬ í”¼ì²˜
            # "season" ì œê±°: month í”¼ì²˜ë¡œ ëŒ€ì²´
            
            # Boolean features - í•µì‹¬ í”¼ì²˜
            "is_weekend": 1 if broadcast_dt.weekday() >= 5 else 0,
            "is_holiday": 0
        }
    
    async def _predict_product_sales(self, product: Dict[str, Any], context: Dict[str, Any]) -> float:
        """ê°œë³„ ìƒí’ˆ XGBoost ë§¤ì¶œ ì˜ˆì¸¡"""
        try:
            import pandas as pd
            
            # Feature ì¤€ë¹„
            features = self._prepare_features_for_product(product, context)
            product_data = pd.DataFrame([features])
            
            logger.info(f"=== XGBoost ë§¤ì¶œ ì˜ˆì¸¡ ì…ë ¥ ë°ì´í„° ===")
            logger.info(f"ìƒí’ˆ: {product.get('product_name', 'Unknown')}")
            logger.info(f"ì¹´í…Œê³ ë¦¬: {product.get('category_main', 'Unknown')}")
            logger.info(f"ê°€ê²©: {product.get('product_price', 100000):,}ì›")
            logger.info(f"ê³¼ê±° í‰ê·  ë§¤ì¶œ: {product.get('avg_sales', 30000000):,}ì›")
            logger.info(f"ë°©ì†¡ ì‹œê°„: {context['broadcast_dt'].hour}ì‹œ")
            logger.info(f"ë‚ ì”¨: {context['weather'].get('weather', 'Clear')}, {context['weather'].get('temperature', 20)}Â°C")
            
            # XGBoost íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì˜ˆì¸¡ (ì „ì²˜ë¦¬ í¬í•¨)
            predicted_sales_log = self.model.predict(product_data)[0]
            # ë¡œê·¸ ì—­ë³€í™˜ (í•™ìŠµ ì‹œ log1p ì‚¬ìš©)
            predicted_sales = np.expm1(predicted_sales_log)
            logger.info(f"=== XGBoost ì˜ˆì¸¡ ê²°ê³¼ ===")
            logger.info(f"ì˜ˆì¸¡ ë§¤ì¶œ: {predicted_sales:,.0f}ì› ({predicted_sales/100000000:.2f}ì–µ)")
            
            return float(predicted_sales)
            
        except Exception as e:
            logger.error(f"ìƒí’ˆ ë§¤ì¶œ ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
            logger.error(f"ìƒí’ˆ ì •ë³´: {product.get('product_name', 'Unknown')}")
            import traceback
            logger.error(f"ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
            return 30000000  # ê¸°ë³¸ê°’ (0.3ì–µ)
    
    async def _predict_products_sales_batch(self, products: List[Dict[str, Any]], context: Dict[str, Any]) -> List[float]:
        """ì—¬ëŸ¬ ìƒí’ˆ XGBoost ë§¤ì¶œ ì˜ˆì¸¡ (ë°°ì¹˜ ì²˜ë¦¬) + ì‹ ìƒí’ˆ ë³´ì •"""
        try:
            import pandas as pd
            
            if not products:
                return []
            
            # ëª¨ë“  ìƒí’ˆì˜ featuresë¥¼ í•œ ë²ˆì— ì¤€ë¹„
            features_list = [
                self._prepare_features_for_product(product, context)
                for product in products
            ]
            
            batch_df = pd.DataFrame(features_list)
            
            print(f"=== [ë°°ì¹˜ ì˜ˆì¸¡] {len(products)}ê°œ ìƒí’ˆ ì¼ê´„ ì˜ˆì¸¡ ì‹œì‘ ===")
            
            # ì…ë ¥ í”¼ì²˜ ìƒ˜í”Œ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
            print(f"=== [ì…ë ¥ í”¼ì²˜ ìƒ˜í”Œ] ===")
            for i, (product, features) in enumerate(zip(products[:3], features_list[:3])):
                print(f"  ìƒí’ˆ {i+1}: {product.get('product_name', '')[:30]}")
                print(f"    - hour: {features['hour']}, month: {features['month']}")
                print(f"    - time_slot: {features['time_slot']}, day_of_week: {features['day_of_week']}")
                print(f"    - ì¹´í…Œê³ ë¦¬: {features['product_lgroup']}")
            
            # XGBoost ë°°ì¹˜ ì˜ˆì¸¡ (í•œ ë²ˆì— ì²˜ë¦¬)
            predicted_sales_log = self.model.predict(batch_df)
            # ë¡œê·¸ ì—­ë³€í™˜ (í•™ìŠµ ì‹œ log1p ì‚¬ìš©)
            predicted_sales_array = np.expm1(predicted_sales_log)
            
            # ========== ì‹ ìƒí’ˆ ë§¤ì¶œ ë³´ì • (2-A) ==========
            # ì¹´í…Œê³ ë¦¬ë³„ í‰ê·  ë§¤ì¶œ ì¡°íšŒ
            category_avg_sales = self.product_embedder.get_category_avg_sales()
            
            # ì „ì²´ í‰ê·  ê³„ì‚° (ì¹´í…Œê³ ë¦¬ í‰ê· ì´ ì—†ëŠ” ê²½ìš° ëŒ€ë¹„)
            overall_avg = sum(category_avg_sales.values()) / len(category_avg_sales) if category_avg_sales else 30000000
            
            # ì‹ ìƒí’ˆ ë³´ì •: ì˜ˆì¸¡ê°’ì´ ë„ˆë¬´ ë‚®ìœ¼ë©´ ì¹´í…Œê³ ë¦¬ í‰ê· ìœ¼ë¡œ ëŒ€ì²´
            MIN_SALES_THRESHOLD = 5000000  # 500ë§Œì› ë¯¸ë§Œì´ë©´ ì‹ ìƒí’ˆìœ¼ë¡œ ê°„ì£¼
            corrected_count = 0
            
            for i, (product, sales) in enumerate(zip(products, predicted_sales_array)):
                if sales < MIN_SALES_THRESHOLD:
                    category = product.get("category_main", "")
                    category_avg = category_avg_sales.get(category, overall_avg)
                    # ì¹´í…Œê³ ë¦¬ í‰ê· ì˜ 80%ë¡œ ë³´ì • (ë³´ìˆ˜ì  ì¶”ì •)
                    corrected_sales = category_avg * 0.8
                    predicted_sales_array[i] = corrected_sales
                    corrected_count += 1
                    print(f"  [ì‹ ìƒí’ˆ ë³´ì •] {product.get('product_name', '')[:25]} | {sales/10000:.0f}ë§Œì› â†’ {corrected_sales/10000:.0f}ë§Œì› (ì¹´í…Œê³ ë¦¬ í‰ê· )")
            
            if corrected_count > 0:
                print(f"=== [ì‹ ìƒí’ˆ ë³´ì •] {corrected_count}ê°œ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ í‰ê· ìœ¼ë¡œ ë³´ì •ë¨ ===")
            
            print(f"=== [ë°°ì¹˜ ì˜ˆì¸¡] ì™„ë£Œ ===")
            print(f"  í‰ê· : {predicted_sales_array.mean()/10000:.0f}ë§Œì›")
            print(f"  ìµœì†Œ: {predicted_sales_array.min()/10000:.0f}ë§Œì›")
            print(f"  ìµœëŒ€: {predicted_sales_array.max()/10000:.0f}ë§Œì›")
            print(f"  í‘œì¤€í¸ì°¨: {predicted_sales_array.std()/10000:.0f}ë§Œì›")
            
            # ì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ ì¶œë ¥
            print(f"=== [ì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ] ===")
            for i, (product, sales) in enumerate(zip(products[:5], predicted_sales_array[:5])):
                print(f"  {i+1}. {product.get('product_name', '')[:30]:30s} â†’ {sales/10000:.0f}ë§Œì›")
            
            return [float(sales) for sales in predicted_sales_array]
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ë§¤ì¶œ ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(f"ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return [30000000.0] * len(products)
    
    async def _get_all_categories_from_db(self) -> List[str]:
        """PostgreSQLì—ì„œ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì¡°íšŒ"""
        try:
            query = text("""
                SELECT DISTINCT category_main
                FROM broadcast_training_dataset
                WHERE category_main IS NOT NULL
                ORDER BY category_main
            """)
            
            with self.engine.connect() as conn:
                result = conn.execute(query).fetchall()
            
            categories = [row[0] for row in result]
            return categories
            
        except Exception as e:
            logger.error(f"ì „ì²´ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    async def _get_ace_products_from_category(self, category: str, limit: int = 5) -> List[Dict[str, Any]]:
        """ì¹´í…Œê³ ë¦¬ë³„ ì—ì´ìŠ¤ ìƒí’ˆ ì¡°íšŒ"""
        try:
            query = text("""
                SELECT product_code, product_name, category_main, category_middle, category_sub,
                       AVG(gross_profit) as avg_sales, COUNT(*) as broadcast_count,
                       tape_code, tape_name, MAX(price) as price, brand
                FROM broadcast_training_dataset 
                WHERE category_main = :category
                GROUP BY product_code, product_name, category_main, category_middle, category_sub,
                         tape_code, tape_name, brand
                ORDER BY avg_sales DESC 
                LIMIT :limit
            """)
            
            with self.engine.connect() as conn:
                result = conn.execute(query, {"category": category, "limit": limit}).fetchall()
                
            products = []
            for row in result:
                products.append({
                    "product_code": row[0],
                    "product_name": row[1],
                    "category_main": row[2],
                    "category_middle": row[3],
                    "category_sub": row[4],
                    "avg_sales": float(row[5]),
                    "broadcast_count": int(row[6]),
                    "tape_code": row[7],
                    "tape_name": row[8],
                    "price": float(row[9]) if row[9] else None,
                    "brand": row[10] if len(row) > 10 else None
                })
            
            return products
            
        except Exception as e:
            logger.error(f"ì—ì´ìŠ¤ ìƒí’ˆ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    def _get_time_slot(self, dt: datetime) -> str:
        """ì‹œê°„ëŒ€ ë¶„ë¥˜"""
        hour = dt.hour
        if 6 <= hour < 9:
            return "ì•„ì¹¨"
        elif 9 <= hour < 12:
            return "ì˜¤ì „"
        elif 12 <= hour < 14:
            return "ì ì‹¬"
        elif 14 <= hour < 18:
            return "ì˜¤í›„"
        elif 18 <= hour < 22:
            return "ì €ë…"
        else:
            return "ì•¼ê°„"
    
    def _get_season(self, month: int) -> str:
        """ê³„ì ˆ ë¶„ë¥˜"""
        if 3 <= month <= 5:
            return "ë´„"
        elif 6 <= month <= 8:
            return "ì—¬ë¦„"
        elif 9 <= month <= 11:
            return "ê°€ì„"
        else:
            return "ê²¨ìš¸"
