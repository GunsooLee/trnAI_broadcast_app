"""
ë°©ì†¡ í¸ì„± AI ì¶”ì²œ ì›Œí¬í”Œë¡œìš°
LangChain ê¸°ë°˜ 2ë‹¨ê³„ ì›Œí¬í”Œë¡œìš°: AI ë°©í–¥ íƒìƒ‰ + ê³ ì† ë­í‚¹
"""

import asyncio
import json
import logging
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
import pandas as pd
import numpy as np
from sqlalchemy import create_engine, text
import os

from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import ChatPromptTemplate
from .external_apis import ExternalAPIManager

from .dependencies import get_product_embedder
from . import broadcast_recommender as br
from .schemas import BroadcastResponse, BroadcastRecommendation, ProductInfo, BusinessMetrics, NaverProduct, CompetitorProduct, LastBroadcastMetrics
from .external_products_service import ExternalProductsService
from .services.broadcast_history_service import BroadcastHistoryService
from .netezza_config import netezza_conn

logger = logging.getLogger(__name__)

class BroadcastWorkflow:
    """ë°©ì†¡ í¸ì„± AI ì¶”ì²œ ì›Œí¬í”Œë¡œìš°"""
    
    def __init__(self, model):
        self.model = model  # XGBoost ëª¨ë¸
        self.product_embedder = get_product_embedder()
        
        # AI íŠ¸ë Œë“œ ìºì‹œ (ì‹œê°„ëŒ€ë³„)
        self._ai_trends_cache = {}
        self._cache_ttl = 3600  # 1ì‹œê°„ (ì´ˆ)
        
        # LangChain LLM ì´ˆê¸°í™”
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.5,
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        
        # DB ì—°ê²°
        self.engine = create_engine(os.getenv("POSTGRES_URI"))
        
        # ì™¸ë¶€ ìƒí’ˆ ì„œë¹„ìŠ¤
        self.external_products_service = ExternalProductsService()
        
        # ë°©ì†¡ ì´ë ¥ ì„œë¹„ìŠ¤ (Netezza)
        self.broadcast_history_service = BroadcastHistoryService()
    
    async def process_broadcast_recommendation(
        self, 
        broadcast_time: str, 
        recommendation_count: int = 5,
        trend_weight: float = 0.3,  # íŠ¸ë Œë“œ ê°€ì¤‘ì¹˜ (0.3 = 30%)
        selling_weight: float = 0.7   # ë§¤ì¶œ ì˜ˆì¸¡ ê°€ì¤‘ì¹˜ (0.7 = 70%)
    ) -> BroadcastResponse:
        """ë©”ì¸ ì›Œí¬í”Œë¡œìš°: ë°©ì†¡ ì‹œê°„ ê¸°ë°˜ ì¶”ì²œ
        
        Args:
            broadcast_time: ë°©ì†¡ ì‹œê°„
            recommendation_count: ì¶”ì²œ ê°œìˆ˜
            trend_weight: íŠ¸ë Œë“œ ê°€ì¤‘ì¹˜ (0.0~1.0, ê¸°ë³¸ 0.3)
            selling_weight: ë§¤ì¶œ ì˜ˆì¸¡ ê°€ì¤‘ì¹˜ (0.0~1.0, ê¸°ë³¸ 0.7)
                - ì˜ˆ: trend_weight=0.3, selling_weight=0.7 â†’ íŠ¸ë Œë“œ 30%, ë§¤ì¶œ 70%
                - ì˜ˆ: trend_weight=0.5, selling_weight=0.5 â†’ ê· í˜• (50:50)
        """
        
        import time
        workflow_start = time.time()
        
        print("=== [DEBUG] process_broadcast_recommendation ì‹œì‘ ===")
        request_time = datetime.now().isoformat()
        logger.info(f"ë°©ì†¡ ì¶”ì²œ ì›Œí¬í”Œë¡œìš° ì‹œì‘: {broadcast_time}")
        print(f"=== [DEBUG] broadcast_time: {broadcast_time}, recommendation_count: {recommendation_count} ===")
        
        try:
            # 1ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ë° í†µí•© í‚¤ì›Œë“œ ìƒì„±
            step_start = time.time()
            print("=== [DEBUG] _collect_context_and_keywords í˜¸ì¶œ ===")
            context = await self._collect_context_and_keywords(broadcast_time)
            print(f"â±ï¸  [1ë‹¨ê³„] ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘: {time.time() - step_start:.2f}ì´ˆ")
            print(f"=== [DEBUG] í†µí•© í‚¤ì›Œë“œ: {len(context.get('unified_keywords', []))}ê°œ ===")
            
            # 2. í†µí•© ê²€ìƒ‰ ì‹¤í–‰ (1íšŒ)
            step_start = time.time()
            print("=== [DEBUG] _execute_unified_search í˜¸ì¶œ ===")
            search_result = await self._execute_unified_search(context, context.get("unified_keywords", []))
            print(f"â±ï¸  [2ë‹¨ê³„] í†µí•© ê²€ìƒ‰: {time.time() - step_start:.2f}ì´ˆ")
            print(f"=== [DEBUG] ê²€ìƒ‰ ì™„ë£Œ - ì§ì ‘ë§¤ì¹­: {len(search_result['direct_products'])}ê°œ, ì¹´í…Œê³ ë¦¬: {len(search_result['category_groups'])}ê°œ ===")
            
            # ê²€ìƒ‰ì— ì‚¬ìš©ëœ í‚¤ì›Œë“œë¥¼ contextì— ì €ì¥
            context["search_keywords"] = search_result.get("search_keywords", [])
            
            # 3. í›„ë³´êµ° ìƒì„± (ê°€ì¤‘ì¹˜ ê¸°ë°˜ ë¹„ìœ¨ ì¡°ì •)
            step_start = time.time()
            print("=== [DEBUG] _generate_unified_candidates í˜¸ì¶œ ===")
            max_trend = max(1, int(recommendation_count * trend_weight))  # ìµœì†Œ 1ê°œ
            max_sales = recommendation_count - max_trend + 3  # ì—¬ìœ ë¶„ ì¶”ê°€
            print(f"=== [DEBUG] ê°€ì¤‘ì¹˜ ì ìš©: íŠ¸ë Œë“œ {max_trend}ê°œ ({trend_weight:.0%}), ë§¤ì¶œ {max_sales}ê°œ ({selling_weight:.0%}) ===")
            
            candidate_products, category_scores = await self._generate_unified_candidates(
                search_result,
                context,
                max_trend_match=max_trend,
                max_sales_prediction=max_sales
            )
            print(f"â±ï¸  [3ë‹¨ê³„] í›„ë³´êµ° ìƒì„±: {time.time() - step_start:.2f}ì´ˆ")
            print(f"=== [DEBUG] í›„ë³´êµ° ìƒì„± ì™„ë£Œ: {len(candidate_products)}ê°œ ===")
            
            # 4. ìµœì¢… ë­í‚¹ ê³„ì‚°
            step_start = time.time()
            ranked_products = await self._rank_final_candidates(
                candidate_products,
                category_scores=category_scores,
                context=context
            )
            print(f"â±ï¸  [4ë‹¨ê³„] ìµœì¢… ë­í‚¹: {time.time() - step_start:.2f}ì´ˆ")
            
            # 5. API ì‘ë‹µ ìƒì„±
            step_start = time.time()
            response = await self._format_response(ranked_products[:recommendation_count], context)
            response.requestTime = request_time
            step_time = time.time() - step_start
            print(f"â±ï¸  [5ë‹¨ê³„] ì‘ë‹µ ìƒì„± ì´: {step_time:.2f}ì´ˆ")
            
            total_time = time.time() - workflow_start
            print(f"â±ï¸  ===== ì›Œí¬í”Œë¡œìš° ì´ ì‹œê°„: {total_time:.2f}ì´ˆ =====")
            
            logger.info(f"ë°©ì†¡ ì¶”ì²œ ì™„ë£Œ: {len(ranked_products)}ê°œ ì¶”ì²œ")
            return response
            
        except Exception as e:
            print(f"=== [DEBUG] ì˜ˆì™¸ ë°œìƒ: {type(e).__name__}: {e} ===")
            import traceback
            traceback.print_exc()
            logger.error(f"ë°©ì†¡ ì¶”ì²œ ì›Œí¬í”Œë¡œìš° ì˜¤ë¥˜: {e}")
            # OpenAI API ê´€ë ¨ ì˜¤ë¥˜ëŠ” ìƒìœ„ë¡œ ì „íŒŒ (503 ì—ëŸ¬ ë°˜í™˜ìš©)
            if "AI ì„œë¹„ìŠ¤" in str(e) or "OpenAI" in str(e) or "í• ë‹¹ëŸ‰" in str(e):
                raise e
            # ê¸°íƒ€ ë‚´ë¶€ ì˜¤ë¥˜ëŠ” 500 ì—ëŸ¬ë¡œ ì²˜ë¦¬
            raise Exception(f"ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜: {e}")
    
    async def _collect_context_and_keywords(self, broadcast_time: str) -> Dict[str, Any]:
        """ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ë° í†µí•© í‚¤ì›Œë“œ ìƒì„± (ê°œì„ ëœ ë²„ì „)"""
        
        # ë°©ì†¡ ì‹œê°„ íŒŒì‹±
        broadcast_dt = datetime.fromisoformat(broadcast_time.replace('Z', '+00:00'))
        
        # DBì—ì„œ ê³µíœ´ì¼ ì •ë³´ ì¡°íšŒ
        holiday_name = await self._get_holiday_from_db(broadcast_dt.date())
        
        context = {
            "broadcast_time": broadcast_time,
            "broadcast_dt": broadcast_dt,
            "hour": broadcast_dt.hour,
            "weekday": broadcast_dt.weekday(),
            "season": self._get_season(broadcast_dt.month),
            "holiday_name": holiday_name  # ê³µíœ´ì¼ ì •ë³´ ì¶”ê°€
        }
        
        # ë‚ ì”¨ ì •ë³´ ìˆ˜ì§‘
        weather_info = br.get_weather_by_date(broadcast_dt.date())
        context["weather"] = weather_info

        # ì‹œê°„ëŒ€ ì •ë³´
        time_slot = self._get_time_slot(broadcast_dt)
        day_type = "ì£¼ë§" if broadcast_dt.weekday() >= 5 else "í‰ì¼"
        context["time_slot"] = time_slot
        context["day_type"] = day_type

        # AI ê¸°ë°˜ íŠ¸ë Œë“œ ìƒì„± (LLM API) - ìºì‹± ì ìš©
        cache_key = f"{broadcast_dt.hour}_{weather_info.get('weather', 'Clear')}"
        current_time = datetime.now().timestamp()
        
        # ìºì‹œ í™•ì¸
        if cache_key in self._ai_trends_cache:
            cached_data, cached_time = self._ai_trends_cache[cache_key]
            if current_time - cached_time < self._cache_ttl:
                context["ai_trends"] = cached_data
                logger.info(f"âœ… AI íŠ¸ë Œë“œ ìºì‹œ íˆíŠ¸ ({cache_key}): {len(cached_data)}ê°œ í‚¤ì›Œë“œ")
            else:
                # ìºì‹œ ë§Œë£Œ
                del self._ai_trends_cache[cache_key]
                logger.info(f"â° AI íŠ¸ë Œë“œ ìºì‹œ ë§Œë£Œ ({cache_key})")
                context["ai_trends"] = None
        else:
            context["ai_trends"] = None
        
        # ìºì‹œ ë¯¸ìŠ¤ ì‹œ API í˜¸ì¶œ
        if context["ai_trends"] is None:
            api_manager = ExternalAPIManager()
            if api_manager.llm_trend_api:
                try:
                    import time
                    api_start = time.time()
                    # ë°©ì†¡ ì‹œê°„ê³¼ ë‚ ì”¨ ì •ë³´ë¥¼ ì „ë‹¬í•˜ì—¬ ë§¥ë½ ê¸°ë°˜ íŠ¸ë Œë“œ ìƒì„±
                    llm_trends = await api_manager.llm_trend_api.get_trending_searches(
                        hour=broadcast_dt.hour,
                        weather_info=weather_info,
                        broadcast_date=broadcast_dt  # ë°©ì†¡ ë‚ ì§œ ì „ë‹¬
                    )
                    api_time = time.time() - api_start
                    # AIê°€ ìƒì„±í•œ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì¶”ê°€
                    context["ai_trends"] = [t["keyword"] for t in llm_trends]
                    # ìºì‹œ ì €ì¥
                    self._ai_trends_cache[cache_key] = (context["ai_trends"], current_time)
                    logger.info(f"ğŸ”¥ AI íŠ¸ë Œë“œ ìƒì„± ì™„ë£Œ ({broadcast_dt.hour}ì‹œ, {weather_info.get('weather', 'N/A')}): {len(llm_trends)}ê°œ í‚¤ì›Œë“œ (ì†Œìš”: {api_time:.2f}ì´ˆ)")
                    logger.info(f"AI íŠ¸ë Œë“œ: {context['ai_trends'][:5]}...")  # ìƒìœ„ 5ê°œë§Œ ë¡œê·¸
                except Exception as e:
                    logger.error(f"AI íŠ¸ë Œë“œ ìƒì„± ì‹¤íŒ¨: {e}")
                    context["ai_trends"] = []
            else:
                logger.warning("OpenAI API í‚¤ ì—†ìŒ - AI íŠ¸ë Œë“œ ìƒì„± ê±´ë„ˆëœ€")
                context["ai_trends"] = []

        # ì»¨í…ìŠ¤íŠ¸ ë¡œê·¸ ì¶œë ¥
        logger.info(f"ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ - ê³„ì ˆ: {context['season']}, ì‹œê°„ëŒ€: {time_slot}, ìš”ì¼: {day_type}")
        if holiday_name:
            logger.info(f"ğŸ‰ ê³µíœ´ì¼: {holiday_name}")
        logger.info(f"ë‚ ì”¨: {weather_info.get('weather', 'N/A')}")
        
        # í†µí•© í‚¤ì›Œë“œ ìƒì„± (ì»¨í…ìŠ¤íŠ¸ ìš°ì„ , ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ë³´ì¡°)
        unified_keywords = []
        
        # 1. ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í‚¤ì›Œë“œ ìƒì„± (ë‚ ì§œ/ì‹œê°„/ë‚ ì”¨ ê¸°ë°˜ - ìš°ì„ ìˆœìœ„ ë†’ìŒ)
        context_keywords = await self._generate_context_keywords(context)
        if context_keywords:
            unified_keywords.extend(context_keywords)
            logger.info(f"[ìš°ì„ ìˆœìœ„ 1] ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ {len(context_keywords)}ê°œ ì¶”ê°€")
        
        # 2. AI íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì¶”ê°€ (1ë‹¨ê³„ LLM - ì‹œê°„ëŒ€/ë‚ ì”¨ ê¸°ë°˜ ìƒí’ˆ í‚¤ì›Œë“œ)
        if context.get("ai_trends"):
            ai_trend_limit = 10  # 3ê°œ â†’ 10ê°œë¡œ ì¦ê°€ (ê²¨ìš¸ ì‹œì¦Œ ìƒí’ˆ ë°˜ì˜)
            ai_keywords = context["ai_trends"][:ai_trend_limit]
            unified_keywords.extend(ai_keywords)
            print(f"[ìš°ì„ ìˆœìœ„ 2] AI íŠ¸ë Œë“œ í‚¤ì›Œë“œ {len(ai_keywords)}ê°œ ì¶”ê°€: {ai_keywords}")
            logger.info(f"[ìš°ì„ ìˆœìœ„ 2] AI íŠ¸ë Œë“œ í‚¤ì›Œë“œ {len(ai_keywords)}ê°œ ì¶”ê°€: {ai_keywords}")
        
        # 3. ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ íŠ¸ë Œë“œ ì¶”ê°€ (2ë‹¨ê³„ LLM - Web Search)
        print("=" * 80)
        print("[í†µí•© í‚¤ì›Œë“œ ìƒì„±] 2ë‹¨ê³„: ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ íŠ¸ë Œë“œ")
        print("=" * 80)
        try:
            realtime_keywords = await self._get_realtime_trend_keywords()
            if realtime_keywords:
                unified_keywords.extend(realtime_keywords)
                context["realtime_trends"] = realtime_keywords  # ì»¨í…ìŠ¤íŠ¸ì—ë„ ì €ì¥
                print(f"[2ë‹¨ê³„ ì™„ë£Œ] ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ í‚¤ì›Œë“œ {len(realtime_keywords)}ê°œ: {realtime_keywords}")
                logger.info(f"[ìš°ì„ ìˆœìœ„ 3] ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ í‚¤ì›Œë“œ {len(realtime_keywords)}ê°œ ì¶”ê°€: {realtime_keywords}")
        except Exception as e:
            print(f"[2ë‹¨ê³„ ì‹¤íŒ¨] {e}")
            logger.warning(f"ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰ íŠ¸ë Œë“œ ìˆ˜ì§‘ ì‹¤íŒ¨ (ë¬´ì‹œ): {e}")
            context["realtime_trends"] = []
        
        # 4. ì¤‘ë³µ ì œê±° ë° ì €ì¥
        context["unified_keywords"] = list(dict.fromkeys(unified_keywords))  # ìˆœì„œ ìœ ì§€ ì¤‘ë³µ ì œê±°
        
        # í†µí•© í‚¤ì›Œë“œ ë¡œê·¸ ì¶œë ¥
        print("=" * 80)
        print(f"[í‚¤ì›Œë“œ í†µí•© ì™„ë£Œ] ì´ {len(context['unified_keywords'])}ê°œ í‚¤ì›Œë“œ")
        print("=" * 80)
        print(f"[í†µí•© í‚¤ì›Œë“œ ì „ì²´]: {context['unified_keywords']}")
        print("=" * 80)
        logger.info(f"í†µí•© í‚¤ì›Œë“œ ìƒì„± ì™„ë£Œ: ì´ {len(context['unified_keywords'])}ê°œ")
        logger.info(f"í†µí•© í‚¤ì›Œë“œ (ìš°ì„ ìˆœìœ„ìˆœ): {context['unified_keywords']}")

        return context
    
    async def _get_holiday_from_db(self, target_date) -> Optional[str]:
        """DBì—ì„œ ê³µíœ´ì¼ ì •ë³´ ì¡°íšŒ"""
        try:
            with self.engine.connect() as conn:
                query = text("""
                    SELECT holiday_name 
                    FROM TAIHOLIDAYS 
                    WHERE holiday_date = :target_date
                """)
                result = conn.execute(query, {"target_date": target_date})
                row = result.fetchone()
                
                if row:
                    holiday_name = row[0]
                    logger.info(f"ê³µíœ´ì¼ ì¡°íšŒ ì„±ê³µ: {target_date} -> {holiday_name}")
                    return holiday_name
                else:
                    return None
        except Exception as e:
            logger.error(f"ê³µíœ´ì¼ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return None
    
    def _get_season(self, month: int) -> str:
        """ê³„ì ˆ ì •ë³´ ë°˜í™˜"""
        if month in [12, 1, 2]:
            return "ê²¨ìš¸"
        elif month in [3, 4, 5]:
            return "ë´„"
        elif month in [6, 7, 8]:
            return "ì—¬ë¦„"
        else:
            return "ê°€ì„"
    
    def _get_time_slot(self, dt: datetime) -> str:
        """ì‹œê°„ëŒ€ ì •ë³´ ë°˜í™˜"""
        hour = dt.hour
        if 6 <= hour < 12:
            return "ì˜¤ì „"
        elif 12 <= hour < 18:
            return "ì˜¤í›„"
        elif 18 <= hour < 24:
            return "ì €ë…"
        else:
            return "ìƒˆë²½"

    # _classify_keywords_with_langchain í•¨ìˆ˜ ì œê±°ë¨
    # ì´ì œ _generate_base_context_keywordsì—ì„œ í‚¤ì›Œë“œ ìƒì„±ê³¼ í™•ì¥ì„ í†µí•© ì²˜ë¦¬
    
    async def _execute_unified_search(self, context: Dict[str, Any], unified_keywords: List[str]) -> Dict[str, Any]:
        """ë‹¤ë‹¨ê³„ Qdrant ê²€ìƒ‰: í‚¤ì›Œë“œë¥¼ ê·¸ë£¹ë³„ë¡œ ë‚˜ëˆ ì„œ ê²€ìƒ‰í•˜ì—¬ ì„ë² ë”© í¬ì„ ë°©ì§€"""
        
        print(f"=== [DEBUG Multi-Stage Search] ì‹œì‘, keywords: {len(unified_keywords)}ê°œ ===")
        
        if not unified_keywords:
            logger.warning("í†µí•© í‚¤ì›Œë“œ ì—†ìŒ - ë¹ˆ ê²°ê³¼ ë°˜í™˜")
            return {"direct_products": [], "category_groups": {}}
        
        try:
            # ëª¨ë“  í‚¤ì›Œë“œë¥¼ ê°œë³„ì ìœ¼ë¡œ ê²€ìƒ‰ (í‚¤ì›Œë“œë³„ ë‹¤ì–‘ì„± í™•ë³´)
            all_results = []
            seen_products = set()
            keyword_results = {}  # í‚¤ì›Œë“œë³„ ê²€ìƒ‰ ê²°ê³¼ ì¶”ì 
            
            print(f"=== [ê°œë³„ í‚¤ì›Œë“œ ê²€ìƒ‰] ì´ {len(unified_keywords)}ê°œ í‚¤ì›Œë“œ ===")
            
            for keyword in unified_keywords:
                results = self.product_embedder.search_products(
                    trend_keywords=[keyword],
                    top_k=5,  # í‚¤ì›Œë“œë‹¹ 5ê°œì”©
                    score_threshold=0.3,
                    only_ready_products=True
                )
                
                new_count = 0
                for r in results:
                    code = r.get("product_code")
                    if code not in seen_products:
                        all_results.append(r)
                        seen_products.add(code)
                        new_count += 1
                
                if new_count > 0:
                    keyword_results[keyword] = new_count
            
            # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½ (ì „ì²´ ì¶œë ¥)
            print(f"=== [í‚¤ì›Œë“œë³„ ê²€ìƒ‰ ê²°ê³¼] ì´ {len(keyword_results)}ê°œ í‚¤ì›Œë“œ ===")
            for kw, count in keyword_results.items():
                print(f"  - {kw}: {count}ê°œ")
            
            # ìœ ì‚¬ë„ ê¸°ì¤€ ì •ë ¬
            all_results.sort(key=lambda x: x.get("similarity_score", 0), reverse=True)
            
            print(f"=== [ê²€ìƒ‰ ì™„ë£Œ] ì´ {len(all_results)}ê°œ ìƒí’ˆ (ìœ ì‚¬ë„ìˆœ ì •ë ¬) ===")
            
            # ìœ ì‚¬ë„ ë¶„í¬ í™•ì¸ (ë””ë²„ê¹…)
            if all_results:
                similarities = [p.get("similarity_score", 0) for p in all_results]
                print(f"[ìœ ì‚¬ë„ ë¶„í¬] ìµœê³ : {max(similarities):.3f}, í‰ê· : {sum(similarities)/len(similarities):.3f}, ìµœì €: {min(similarities):.3f}")
                print(f"[ìƒìœ„ 5ê°œ ìœ ì‚¬ë„]")
                for i, p in enumerate(all_results[:5], 1):
                    sim = p.get("similarity_score", 0)
                    name = p.get("product_name", "")[:40]
                    tape = "ğŸ“¼" if (p.get("tape_code") and p.get("tape_name")) else "âŒ"
                    print(f"  {i}. {name} | ìœ ì‚¬ë„: {sim:.3f} | í…Œì´í”„: {tape}")
            
            # ìœ ì‚¬ë„ ê¸°ë°˜ ë¶„ë¥˜
            direct_products = []      # ê³ ìœ ì‚¬ë„: ì§ì ‘ ì¶”ì²œ
            category_groups = {}      # ì¤‘ìœ ì‚¬ë„: ì¹´í…Œê³ ë¦¬ë³„ ê·¸ë£¹
            
            # ìœ ì‚¬ë„ ì„ê³„ê°’
            HIGH_SIMILARITY_THRESHOLD = 0.45  # ì‹¤ì œ ìœ ì‚¬ë„ ë¶„í¬(ìµœê³  0.498)ì— ë§ì¶¤
            
            for product in all_results:
                similarity = product.get("similarity_score", 0)
                category = product.get("category_main", "ê¸°íƒ€")
                
                # ê³ ìœ ì‚¬ë„ ìƒí’ˆ: ì§ì ‘ ë§¤ì¹­
                if similarity >= HIGH_SIMILARITY_THRESHOLD:
                    if product.get("tape_code") and product.get("tape_name"):
                        direct_products.append({
                            **product,
                            "source": "direct_match",
                            "similarity_score": similarity
                        })
                        print(f"  âœ… ì§ì ‘ë§¤ì¹­: {product.get('product_name')[:30]} (ìœ ì‚¬ë„: {similarity:.2f})")
                
                # ì¤‘ìœ ì‚¬ë„: ì¹´í…Œê³ ë¦¬ ê·¸ë£¹í•‘
                if category not in category_groups:
                    category_groups[category] = []
                category_groups[category].append(product)
            
            print(f"=== [ë¶„ë¥˜ ì™„ë£Œ] ì§ì ‘ë§¤ì¹­: {len(direct_products)}ê°œ, ì¹´í…Œê³ ë¦¬: {len(category_groups)}ê°œ ===")
            
            return {
                "direct_products": direct_products,
                "category_groups": category_groups,
                "search_keywords": unified_keywords[:5]
            }
            
        except Exception as e:
            logger.error(f"ë‹¤ë‹¨ê³„ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(f"ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
            return {"direct_products": [], "category_groups": {}}
    
    async def _get_realtime_trend_keywords(self) -> List[str]:
        """ì‹¤ì‹œê°„ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ìˆ˜ì§‘ (OpenAI Web Search)"""
        from openai import OpenAI
        from datetime import datetime
        
        try:
            client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            
            
            # 1. [ë‚ ì§œ ë™ì  ê³„ì‚°]
            now = datetime.now()
            current_date_str = now.strftime("%Yë…„ %mì›” %dì¼")
            
            # 2. [ê²€ìƒ‰ íƒ€ê²Ÿ ì„¤ì •] íŠ¸ë Œë“œ ì •í™•ë„ë¥¼ ìœ„í•´ 'ì´ë²ˆ ë‹¬'ê³¼ 'ì§€ë‚œ ë‹¬'ë„ êµ¬í•¨.
            current_month_str = now.strftime("%Yë…„ %mì›”")
            last_month_date = now.replace(day=1) - timedelta(days=1)
            last_month_str = last_month_date.strftime("%Yë…„ %mì›”")
            
            target_period_str = f"{last_month_str} ~ {current_month_str}"
            
            prompt = f"""ë‹¹ì‹ ì€ ëŒ€í•œë¯¼êµ­ 20ë…„ì°¨ ìœ í†µ ì „ë¬¸ ê¸°ìì´ì í™ˆì‡¼í•‘ MDì…ë‹ˆë‹¤.

**í˜„ì¬ ë‚ ì§œ: {current_date_str}**
ìš°ë¦¬ëŠ” '{current_date_str}' ë°©ì†¡ì„ ìœ„í•œ ì•„ì´ë””ì–´ íšŒì˜ ì¤‘ì…ë‹ˆë‹¤.

**[í•µì‹¬ ê³¼ì œ]**
ê°€ìƒì˜ ë¯¸ë˜ê°€ ì•„ë‹Œ, **í˜„ì‹¤ ì„¸ê³„ì˜ '{last_month_str}' í™ˆì‡¼í•‘, ìƒí’ˆ, ìœ í†µ ê´€ë ¨ ë‰´ìŠ¤ ê¸°ì‚¬**ë¥¼ ê²€ìƒ‰í•˜ì—¬, 
ì–¸ë¡ ì—ì„œ ë³´ë„ëœ **'ì‹¤ì œ íˆíŠ¸ ìƒí’ˆ'** 5ê°€ì§€ë¥¼ ì°¾ì•„ë‚´ì„¸ìš”.

**[ê²€ìƒ‰ ì§€ì¹¨ - ì‹ ë¢° ê°€ëŠ¥í•œ ë‰´ìŠ¤ ì‚¬ì´íŠ¸ í•œì •]**
ë°˜ë“œì‹œ ì•„ë˜ 'ë‰´ìŠ¤ ì‚¬ì´íŠ¸'ì—ì„œë§Œ '{last_month_str}' ê¸°ì¤€ìœ¼ë¡œ 3ê°œì›” ì´ë‚´ì˜ ì •ë³´ë§Œ ê²€ìƒ‰í•˜ì„¸ìš”:
- ê²½ì œì§€: ë¨¸ë‹ˆíˆ¬ë°ì´(mt.co.kr), í•œêµ­ê²½ì œ(hankyung.com), ë§¤ì¼ê²½ì œ(mk.co.kr)
- ìœ í†µ ì „ë¬¸ì§€: íŒ¨ì…˜ë¹„ì¦ˆ(fashionbiz.co.kr), ì–´íŒ¨ëŸ´ë‰´ìŠ¤(apparelnews.co.kr), ë¦¬í…Œì¼ë§¤ê±°ì§„
- ì¢…í•©ì§€: ì¡°ì„ ì¼ë³´(chosun.com), ì´íˆ¬ë°ì´(etoday.co.kr)

**ê²€ìƒ‰ í‚¤ì›Œë“œ (ê¸°ì‚¬ ê²€ìƒ‰ìš©):**
1. "{last_month_str} ì‹ ìƒ íˆíŠ¸ ìƒí’ˆ"
2. "{last_month_str} ìœ í†µì—…ê³„ ê²°ì‚° ë§¤ì¶œ ê¸‰ì¦ ì•„ì´í…œ"
3. "{last_month_str} í™ˆì‡¼í•‘ ë§¤ì§„ ìƒí’ˆ"
4. "{last_month_str} ëŒ€ë€í…œ"

**[ì •ë‹µ í•„í„°ë§ - ê¸°ì‚¬ ê²€ì¦]**
1. **í•„ìˆ˜:** ê°œì¸ ë¸”ë¡œê·¸ë‚˜ SNSê°€ ì•„ë‹Œ, **'ë‰´ìŠ¤ ê¸°ì‚¬', 'ê²½ì œ ì‹ ë¬¸', 'ê³µì‹ ë³´ë„ìë£Œ'**ì— ì–¸ê¸‰ëœ ìƒí’ˆ.
2. **ëŒ€ìƒ:** êµ¬ì²´ì ì¸ **ì¹´í…Œê³ ë¦¬"" ë˜ëŠ” ""ìƒí’ˆëª…**ë¥¼ í•œ ë‹¨ì–´ í‚¤ì›Œë“œë¡œ í‘œí˜„
3. **ê²€ì¦ í‚¤ì›Œë“œ:** ê¸°ì‚¬ ì œëª©ì— 'ì¸ê¸°', 'í’ˆì ˆ', 'ì˜¤í”ˆëŸ°', 'ë§¤ì¶œ ìƒìŠ¹', 'ì™„íŒ', 'ëŒ€ë€' ì¤‘ í•˜ë‚˜ ì´ìƒ í¬í•¨.
4. **ë¬¼ë¦¬ì  ìƒí’ˆë§Œ:**
5. **ì œì™¸ ëŒ€ìƒ:**
   - ë¹„ì‹¤ë¬¼ ìƒí’ˆ (ì•±, ë©¤ë²„ì‹­, ë¶€ë™ì‚°)



**[ê²½ê³ ]**
- ê¸°ì‚¬ì˜ ë‚ ì§œëŠ” ë¬´ì¡°ê±´ ì§€ì¼œì•¼í•˜ëŠ” ì œ 1ì›ì¹™ ì…ë‹ˆë‹¤.
- ë‚ ì§œê°€ {last_month_str} ì´ ì•„ë‹Œ ë‹¤ë¥¸ ë…„ë„, ì›” ê¸°ì‚¬ëŠ” ì‚¬ìš©í•˜ì§€ ë§ˆì„¸ìš”.
- ì¶œì²˜ URLì´ ì—†ìœ¼ë©´ ë‹µë³€í•˜ì§€ ë§ˆì„¸ìš”.
- ë‰´ìŠ¤ ì‚¬ì´íŠ¸ ì¶œì²˜ê°€ ì•„ë‹ˆë©´ ë¬´ì‹œí•˜ì„¸ìš”.


**[ì¶œë ¥ í˜•ì‹]**
```json
["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", "í‚¤ì›Œë“œ3", "í‚¤ì›Œë“œ4", "í‚¤ì›Œë“œ5"]
```

**ê° ìƒí’ˆì˜ ì¶œì²˜:**
- í‚¤ì›Œë“œ1: ê¸°ì‚¬ ì œëª© ìš”ì•½ (ë‰´ìŠ¤ì‚¬ëª…, URL, ë‚ ì§œ)
- í‚¤ì›Œë“œ2: ê¸°ì‚¬ ì œëª© ìš”ì•½ (ë‰´ìŠ¤ì‚¬ëª…, URL, ë‚ ì§œ)
- í‚¤ì›Œë“œ3: ê¸°ì‚¬ ì œëª© ìš”ì•½ (ë‰´ìŠ¤ì‚¬ëª…, URL, ë‚ ì§œ)
- í‚¤ì›Œë“œ4: ê¸°ì‚¬ ì œëª© ìš”ì•½ (ë‰´ìŠ¤ì‚¬ëª…, URL, ë‚ ì§œ)
- í‚¤ì›Œë“œ5: ê¸°ì‚¬ ì œëª© ìš”ì•½ (ë‰´ìŠ¤ì‚¬ëª…, URL, ë‚ ì§œ)

"""
            
            print("=" * 80)
            print("[2ë‹¨ê³„ - OpenAI Web Search] ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ìˆ˜ì§‘ ì‹œì‘")
            print("=" * 80)
            print(f"[í”„ë¡¬í”„íŠ¸]\n{prompt}")
            print("=" * 80)
            logger.info(f"[2ë‹¨ê³„] ì‹¤ì‹œê°„ íŠ¸ë Œë“œ í”„ë¡¬í”„íŠ¸: {prompt[:200]}...")
            
            response = client.responses.create(
                model="gpt-4o-mini",
                tools=[{
                    "type": "web_search",
                    "search_context_size": "high",  # medium â†’ highë¡œ ë³€ê²½
                    "user_location": {
                        "type": "approximate",
                        "country": "KR",
                        "timezone": "Asia/Seoul"
                    }
                }],
                tool_choice="required",  # ì›¹ ê²€ìƒ‰ ë„êµ¬ ì‚¬ìš© ê°•ì œ
                input=prompt,
                max_output_tokens=1500
            )
            
            result_text = response.output_text
            print("=" * 80)
            print(f"[2ë‹¨ê³„ - ì‘ë‹µ (ì „ì²´)]")
            print("-" * 80)
            print(result_text)
            print("-" * 80)
            logger.info(f"[2ë‹¨ê³„] ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ì‘ë‹µ: {result_text}")
            
            # JSON ë°°ì—´ ì¶”ì¶œ (```json ì½”ë“œë¸”ë¡ ë‚´ë¶€ ìš°ì„ )
            import json
            import re
            
            # 1ì°¨: ```json ì½”ë“œë¸”ë¡ ë‚´ë¶€ì—ì„œ ë°°ì—´ ì¶”ì¶œ
            code_block_match = re.search(r'```json\s*(\[.*?\])\s*```', result_text, re.DOTALL)
            if code_block_match:
                json_str = code_block_match.group(1)
            else:
                # 2ì°¨: ì²« ë²ˆì§¸ JSON ë°°ì—´ë§Œ ì¶”ì¶œ (ì¤„ë°”ê¿ˆ ì „ê¹Œì§€)
                # ["a", "b", "c"] í˜•íƒœë§Œ ë§¤ì¹­
                json_match = re.search(r'\["[^"]*"(?:\s*,\s*"[^"]*")*\]', result_text)
                json_str = json_match.group() if json_match else None
            
            if json_str:
                keywords = json.loads(json_str)
                # ì¤‘ë³µ ì œê±°
                unique_keywords = list(dict.fromkeys(keywords))
                if len(unique_keywords) != len(keywords):
                    print(f"[2ë‹¨ê³„ - ì¤‘ë³µ ì œê±°] {len(keywords)}ê°œ â†’ {len(unique_keywords)}ê°œ")
                print(f"[2ë‹¨ê³„ - ì¶”ì¶œ ì„±ê³µ] í‚¤ì›Œë“œ: {unique_keywords}")
                logger.info(f"[2ë‹¨ê³„] ì‹¤ì‹œê°„ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì¶”ì¶œ ì„±ê³µ: {unique_keywords}")
                return unique_keywords[:5]  # ìµœëŒ€ 5ê°œë§Œ
            else:
                print("[2ë‹¨ê³„ - ì‹¤íŒ¨] JSON ë°°ì—´ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                logger.warning("[2ë‹¨ê³„] ì‹¤ì‹œê°„ íŠ¸ë Œë“œì—ì„œ JSON ë°°ì—´ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return []
                
        except Exception as e:
            print("=" * 80)
            print(f"[2ë‹¨ê³„ - ì˜¤ë¥˜] {e}")
            print("=" * 80)
            logger.error(f"[2ë‹¨ê³„] ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []
    
    async def _generate_base_context_keywords(self, context: Dict[str, Any]) -> List[str]:
        """ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ LangChainìœ¼ë¡œ ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„±"""
        
        # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ì¶œ (ì•ˆì „í•˜ê²Œ)
        weather_info = context.get("weather", {})
        logger.info(f"weather_info type: {type(weather_info)}, value: {weather_info}")
        
        if isinstance(weather_info, dict):
            weather = weather_info.get("weather", "ë§‘ìŒ")
            temperature = weather_info.get("temperature", 20)
        else:
            logger.warning(f"weather_info is not dict: {weather_info}")
            weather = "ë§‘ìŒ"
            temperature = 20
        
        time_slot = context.get("time_slot", "ì €ë…")
        day_type = context.get("day_type", "í‰ì¼")
        holiday_name = context.get("holiday_name")  # ê³µíœ´ì¼ ì •ë³´
        
        # ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
        broadcast_dt = context.get("broadcast_dt")
        month = broadcast_dt.month if broadcast_dt else 11
        day = broadcast_dt.day if broadcast_dt else 19
        
        logger.info(f"ì¶”ì¶œëœ ì •ë³´ - weather: {weather}, temp: {temperature}, time_slot: {time_slot}, month: {month}, day: {day}, day_type: {day_type}, holiday: {holiday_name}")
        
        # LangChain í”„ë¡¬í”„íŠ¸ (í‚¤ì›Œë“œ ìƒì„± + í™•ì¥ í†µí•©)
        keyword_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ 20ë…„ì°¨ í™ˆì‡¼í•‘ ìƒí’ˆ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì£¼ì–´ì§„ ìƒí™©ì— ë§ëŠ” **êµ¬ì²´ì ì¸ ìƒí’ˆëª… í‚¤ì›Œë“œ**ë¥¼ ìƒì„±í•˜ê³ , ì¶”ìƒì ì¸ í‚¤ì›Œë“œëŠ” í™•ì¥í•´ì£¼ì„¸ìš”.

**2ë‹¨ê³„ ì‘ì—…:**
1. ìƒí™©ì— ë§ëŠ” í‚¤ì›Œë“œ 10-15ê°œ ìƒì„±
2. ì¶”ìƒì  í‚¤ì›Œë“œë¥¼ êµ¬ì²´ì  ìƒí’ˆëª…ìœ¼ë¡œ í™•ì¥

**í•µì‹¬ ì›ì¹™: ì‹¤ì œ ìƒí’ˆëª…ì²˜ëŸ¼ êµ¬ì²´ì ìœ¼ë¡œ!**

âŒ ë‚˜ìœ ì˜ˆ (ì¶”ìƒì ):
- "ê²¨ìš¸ì¤€ë¹„", "ê±´ê°•ê´€ë¦¬", "ê°€ì¡±ëª¨ì„", "ë”°ëœ»í•œ", "í¸ë¦¬í•œ"

âœ… ì¢‹ì€ ì˜ˆ (êµ¬ì²´ì ):
- "íŒ¨ë”©", "ê¸°ëª¨ë°”ì§€", "ë‹´ìš”", "ì˜¤ë©”ê°€3", "ë½í† í•", "ì˜¨ì—´ê¸°", "ì „ê¸°ì¥íŒ"

**ì‹œì¦Œë³„ êµ¬ì²´ì  í‚¤ì›Œë“œ ì˜ˆì‹œ:**

11ì›”-12ì›” (ê²¨ìš¸):
- ì˜ë¥˜: "íŒ¨ë”©", "ê¸°ëª¨", "ëª©ë„ë¦¬", "ì¥ê°‘", "ê²¨ìš¸ì½”íŠ¸"
- ê°€ì „: "ì˜¨ì—´ê¸°", "ì „ê¸°ì¥íŒ", "íˆí„°", "ê°€ìŠµê¸°"
- ê±´ê°•: "ì˜¤ë©”ê°€3", "ìœ ì‚°ê· ", "í™ì‚¼", "ë¹„íƒ€ë¯¼", "ë©´ì—­"
- ì‹í’ˆ: "êµ°ê³ êµ¬ë§ˆ", "í˜¸ë¹µ", "ì–´ë¬µ", "í•«íŒ©"

7-8ì›” (ì—¬ë¦„):
- ì˜ë¥˜: "ë°˜íŒ”", "ë°˜ë°”ì§€", "ì›í”¼ìŠ¤", "ìƒŒë“¤"
- ê°€ì „: "ì„ í’ê¸°", "ì—ì–´ì»¨", "ì œìŠµê¸°", "ëƒ‰í’ê¸°"
- ê±´ê°•: "ìˆ˜ë¶„í¬ë¦¼", "ìì™¸ì„ ì°¨ë‹¨ì œ", "ë¹„íƒ€ë¯¼C"
- ì‹í’ˆ: "ì•„ì´ìŠ¤í¬ë¦¼", "ëƒ‰ë©´", "ìˆ˜ë°•", "ìŒë£Œ"

**í™•ì¥ ê·œì¹™:**
- "ìˆ˜ëŠ¥ ê°„ì‹" â†’ ["ì´ˆì½œë¦¿", "ê²¬ê³¼ë¥˜", "ì—ë„ˆì§€ë°”", "í™ì‚¼"]
- "ë¸”ë™í”„ë¼ì´ë°ì´" â†’ ["í• ì¸", "íŠ¹ê°€", "ì„¸ì¼"]
- "ê¹€ì¥ ì¬ë£Œ" â†’ ["ê¹€ì¹˜ëƒ‰ì¥ê³ ", "ì ˆì„ë°°ì¶”", "ê³ ì¶§ê°€ë£¨"]
- "ê²¨ìš¸ íŒ¨ì…˜" â†’ ["íŒ¨ë”©", "ê¸°ëª¨", "ì½”íŠ¸", "ëª©ë„ë¦¬"]
- ì´ë¯¸ êµ¬ì²´ì ì´ë©´ í™•ì¥ ë¶ˆí•„ìš”

**ì¤‘ìš” ì§€ì¹¨:**
1. ë¸Œëœë“œëª…ë„ í¬í•¨ ê°€ëŠ¥: "ë½í† í•", "ì¢…ê·¼ë‹¹", "ì¿ ì¿ ", "í•´í”¼ì½œ"
2. ìƒí’ˆ ì¹´í…Œê³ ë¦¬ëª…: "ê±´ê°•ì‹í’ˆ", "ìƒí™œê°€ì „", "ì˜ë¥˜", "ì‹í’ˆ"
3. ì‹œì¦Œ íŠ¹í™” ìƒí’ˆ: 11-12ì›”ì´ë©´ "í¬ë¦¬ìŠ¤ë§ˆìŠ¤", "ì—°ë§ì„ ë¬¼", "ìˆ˜ëŠ¥ê°„ì‹"
4. ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬ í¬í•¨ (ìµœì†Œ 3ê°œ ì´ìƒ ì¹´í…Œê³ ë¦¬)

**ì‹œê°„ëŒ€ë³„ ì¹´í…Œê³ ë¦¬ ìš°ì„ ìˆœìœ„ ë° ê°€ì¤‘ì¹˜ (ì¤‘ìš”!):**

ğŸŒ… ì•„ì¹¨ (06:00-09:59):
- ë§¤ìš° ì í•© (1.2): ê±´ê°•ì‹í’ˆ, ì¼ë°˜ì‹í’ˆ, ì£¼ë°©ìš©í’ˆ
- ë³´í†µ (0.9): ì˜ë¥˜, ê°€ì „
- ë¶€ì í•© (0.8): íŒ¨ì…˜ì¡í™”, ì‹ ë°œ
- ì˜ˆ: "ì˜¤ë©”ê°€3"(1.2), "ìœ ì‚°ê· "(1.2), "ì»¤í”¼"(1.2), "íŒ¨ë”©"(0.9)

ğŸŒ ì ì‹¬ (10:00-13:59):
- ë§¤ìš° ì í•© (1.2): ì¼ë°˜ì‹í’ˆ, ì£¼ë°©ìš©í’ˆ, ìƒí™œìš©í’ˆ
- ë³´í†µ (0.9): ì˜ë¥˜, ê°€ì „, ê±´ê°•ì‹í’ˆ
- ë¶€ì í•© (0.8): íŒ¨ì…˜ì¡í™”, ì‹ ë°œ
- ì˜ˆ: "ê°„í¸ì‹"(1.2), "ë„ì‹œë½"(1.2), "ì²­ì†Œìš©í’ˆ"(1.2), "íŒ¨ë”©"(0.9)

ğŸŒ¤ï¸ ì˜¤í›„ (14:00-17:59):
- ë§¤ìš° ì í•© (1.2): ê°€êµ¬/ì¹¨êµ¬, ìƒí™œìš©í’ˆ, ê°€ì „
- ë³´í†µ (1.0): ê±´ê°•ì‹í’ˆ, ì˜ë¥˜, ì‹í’ˆ
- ì˜ˆ: "ì¹¨ëŒ€"(1.2), "ë§¤íŠ¸ë¦¬ìŠ¤"(1.2), "ì²­ì†Œê¸°"(1.2), "íŒ¨ë”©"(1.0)

ğŸŒ™ ì €ë…/ë°¤ (18:00-05:59):
- ë§¤ìš° ì í•© (1.2): ì˜ë¥˜, íŒ¨ì…˜ì¡í™”/ë³´ì„, ì‹ ë°œ, í™”ì¥í’ˆ/ë·°í‹°
- ì í•© (1.1): ê±´ê°•ì‹í’ˆ, ê°€ì „, ê°€êµ¬/ì¹¨êµ¬
- ë³´í†µ (0.9): ì‹í’ˆ, ì£¼ë°©ìš©í’ˆ
- ì˜ˆ: "íŒ¨ë”©"(1.2), "ê¸°ëª¨"(1.2), "ëª©ë„ë¦¬"(1.2), "ìŠ¤í‚¨ì¼€ì–´"(1.2), "í™”ì¥í’ˆ"(1.2), "í™ì‚¼"(1.1)

**ê°€ì¤‘ì¹˜ ê·œì¹™:**
- 1.2: í•´ë‹¹ ì‹œê°„ëŒ€ì— ë§¤ìš° ì í•©í•œ ì¹´í…Œê³ ë¦¬
- 1.1: ì í•©í•œ ì¹´í…Œê³ ë¦¬
- 1.0: ë³´í†µ (ê¸°ë³¸ê°’)
- 0.9: ë‹¤ì†Œ ë¶€ì í•©
- 0.8: ë¶€ì í•©

JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜ (ê° í‚¤ì›Œë“œì— ê°€ì¤‘ì¹˜ í¬í•¨):
{{
  "keywords": [
    {{"keyword": "í‚¤ì›Œë“œ1", "weight": 1.2}},
    {{"keyword": "í‚¤ì›Œë“œ2", "weight": 1.0}},
    {{"keyword": "í‚¤ì›Œë“œ3", "weight": 0.9}}
  ],
  "expanded": {{
    "ì¶”ìƒí‚¤ì›Œë“œ1": ["êµ¬ì²´1", "êµ¬ì²´2", "êµ¬ì²´3"],
    "ì¶”ìƒí‚¤ì›Œë“œ2": ["êµ¬ì²´1", "êµ¬ì²´2"]
  }}
}}"""),
            ("human", """ë‚ ì§œ: {month}ì›” {day}ì¼
ë‚ ì”¨: {weather}
ê¸°ì˜¨: {temperature}ë„
ì‹œê°„ëŒ€: {time_slot}
ìš”ì¼ íƒ€ì…: {day_type}
ê³µíœ´ì¼: {holiday_name}

ìœ„ ìƒí™©ì— ì í•©í•œ ìƒí’ˆ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”. 
**íŠ¹íˆ ì‹œê°„ëŒ€({time_slot})ë¥¼ ê³ ë ¤í•´ì„œ í•´ë‹¹ ì‹œê°„ëŒ€ì— ì í•©í•œ ì¹´í…Œê³ ë¦¬ì˜ í‚¤ì›Œë“œë¥¼ ìš°ì„ ì ìœ¼ë¡œ ìƒì„±í•˜ì„¸ìš”!**""")
        ])
        
        chain = keyword_prompt | self.llm | JsonOutputParser()
        
        try:
            # í”„ë¡¬í”„íŠ¸ ë¡œê¹… (ëˆˆì— ë„ê²Œ)
            prompt_vars = {
                "month": month,
                "day": day,
                "weather": weather,
                "temperature": temperature,
                "time_slot": time_slot,
                "day_type": day_type,
                "holiday_name": holiday_name if holiday_name else "ì—†ìŒ"
            }
            print("=" * 80)
            print("[1ë‹¨ê³„ - LangChain í”„ë¡¬í”„íŠ¸] ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ ìƒì„±")
            print("=" * 80)
            print(f"ì…ë ¥ ë³€ìˆ˜:")
            for key, value in prompt_vars.items():
                print(f"  - {key}: {value}")
            print("=" * 80)
            logger.info(f"[1ë‹¨ê³„] ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜: {prompt_vars}")
            
            result = await chain.ainvoke({
                "month": month,
                "day": day,
                "weather": weather,
                "temperature": temperature,
                "time_slot": time_slot,
                "day_type": day_type,
                "holiday_name": holiday_name if holiday_name else "ì—†ìŒ"
            })
            
            # ê²°ê³¼ íŒŒì‹±
            keyword_weights = {}  # í‚¤ì›Œë“œë³„ ê°€ì¤‘ì¹˜ ì €ì¥
            
            if isinstance(result, dict):
                keywords_data = result.get("keywords", [])
                expansion_map = result.get("expanded", {})
                
                # í‚¤ì›Œë“œì™€ ê°€ì¤‘ì¹˜ ë¶„ë¦¬
                keywords = []
                for item in keywords_data:
                    if isinstance(item, dict):
                        kw = item.get("keyword", "")
                        weight = item.get("weight", 1.0)
                        keywords.append(kw)
                        keyword_weights[kw] = weight
                    else:
                        # í´ë°±: ë¬¸ìì—´ë¡œ ì˜¨ ê²½ìš°
                        keywords.append(item)
                        keyword_weights[item] = 1.0
            else:
                # í´ë°±: ë¦¬ìŠ¤íŠ¸ë¡œ ì˜¨ ê²½ìš°
                keywords = result if isinstance(result, list) else []
                expansion_map = {}
                for kw in keywords:
                    keyword_weights[kw] = 1.0
            
            print("=" * 80)
            print(f"[1ë‹¨ê³„ - ì‘ë‹µ] LLM ìƒì„± í‚¤ì›Œë“œ (ê°€ì¤‘ì¹˜ í¬í•¨):")
            for kw in keywords[:10]:
                weight = keyword_weights.get(kw, 1.0)
                print(f"  - {kw}: {weight}x")
            print(f"[1ë‹¨ê³„ - ê²°ê³¼] ì´ {len(keywords)}ê°œ í‚¤ì›Œë“œ")
            print("=" * 80)
            
            # í™•ì¥ í‚¤ì›Œë“œ ì²˜ë¦¬ ë° ë§¤í•‘ ìƒì„±
            expanded_keywords = []
            keyword_mapping = {}
            
            print(f"[1ë‹¨ê³„ - í™•ì¥] LLM í™•ì¥ ê²°ê³¼:")
            for original_kw in keywords:
                # ì›ë³¸ í‚¤ì›Œë“œ ì¶”ê°€ (ê°€ì¤‘ì¹˜ ìœ ì§€)
                expanded_keywords.append(original_kw)
                keyword_mapping[original_kw] = original_kw
                
                # í™•ì¥ëœ í‚¤ì›Œë“œ ì¶”ê°€
                if original_kw in expansion_map:
                    expanded_list = expansion_map[original_kw]
                    print(f"  ğŸ”„ '{original_kw}' â†’ {expanded_list}")
                    expanded_keywords.extend(expanded_list)
                    
                    # ë§¤í•‘ ì €ì¥
                    for exp_kw in expanded_list:
                        keyword_mapping[exp_kw] = original_kw
            
            # ì¤‘ë³µ ì œê±°
            expanded_keywords = list(dict.fromkeys(expanded_keywords))
            
            print("=" * 80)
            print(f"[1ë‹¨ê³„ - LLM í™•ì¥ ì™„ë£Œ] ì›ë³¸ {len(keywords)}ê°œ â†’ í™•ì¥ {len(expanded_keywords)}ê°œ")
            print(f"[1ë‹¨ê³„ - í™•ì¥ í‚¤ì›Œë“œ] {expanded_keywords}")
            print("=" * 80)
            
            # RAG ë°©ì‹: ì‹¤ì œ DB ìƒí’ˆëª… ê¸°ë°˜ í‚¤ì›Œë“œ ì¬í™•ì¥
            rag_keywords = await self._extract_keywords_from_actual_products(expanded_keywords)
            
            # RAG í‚¤ì›Œë“œë„ ë§¤í•‘ì— ì¶”ê°€ (ì›ë³¸ í‚¤ì›Œë“œë¡œ ì—­ì¶”ì )
            for rag_kw in rag_keywords:
                if rag_kw not in keyword_mapping:
                    # RAGë¡œ ì¶”ì¶œëœ í‚¤ì›Œë“œëŠ” ê°€ì¥ ê´€ë ¨ ìˆëŠ” ì›ë³¸ í‚¤ì›Œë“œë¡œ ë§¤í•‘
                    # ê°„ë‹¨í•˜ê²Œ ì²« ë²ˆì§¸ ì›ë³¸ í‚¤ì›Œë“œë¡œ ë§¤í•‘ (ê°œì„  ê°€ëŠ¥)
                    keyword_mapping[rag_kw] = keywords[0] if keywords else rag_kw
            
            # ìµœì¢… í‚¤ì›Œë“œ: ëª¨ë“  í‚¤ì›Œë“œ í†µí•© (ì¤‘ë³µ ì œê±°)
            final_keywords = []
            
            # 1ìˆœìœ„: ì›ë³¸ í‚¤ì›Œë“œ (LLM ìƒì„±)
            final_keywords.extend(keywords)
            
            # 2ìˆœìœ„: LLM í™•ì¥ í‚¤ì›Œë“œ
            for exp_kw in expanded_keywords:
                if exp_kw not in final_keywords:
                    final_keywords.append(exp_kw)
            
            # 3ìˆœìœ„: RAG í‚¤ì›Œë“œ (ëª¨ë‘ í¬í•¨)
            for rag_kw in rag_keywords:
                if rag_kw not in final_keywords:
                    final_keywords.append(rag_kw)
            
            # contextì— ë§¤í•‘ ì •ë³´ ë° ê°€ì¤‘ì¹˜ ì €ì¥
            context["keyword_mapping"] = keyword_mapping
            context["original_keywords"] = keywords
            context["keyword_weights"] = keyword_weights  # ì‹œê°„ëŒ€ë³„ ê°€ì¤‘ì¹˜
            
            print("=" * 80)
            print(f"[1ë‹¨ê³„ - ìµœì¢… ì™„ë£Œ] ì›ë³¸ {len(keywords)}ê°œ + í™•ì¥ {len(expanded_keywords)}ê°œ + RAG {len(rag_keywords)}ê°œ â†’ ìµœì¢… {len(final_keywords)}ê°œ")
            print(f"[í‚¤ì›Œë“œ í†µí•© ì™„ë£Œ]")
            print(f"  - ì›ë³¸: {keywords[:5]}...")
            print(f"  - í™•ì¥: {expanded_keywords[:5]}...")
            print(f"  - RAG: {rag_keywords[:5]}...")
            print(f"[1ë‹¨ê³„ - ìµœì¢… í‚¤ì›Œë“œ ìˆœì„œ] {final_keywords[:20]}...")
            print(f"[1ë‹¨ê³„ - ë§¤í•‘] {len(keyword_mapping)}ê°œ ë§¤í•‘ ì €ì¥")
            print("=" * 80)
            
            logger.info(f"[1ë‹¨ê³„] ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í‚¤ì›Œë“œ ìƒì„± ì™„ë£Œ: {keywords}")
            logger.info(f"[1ë‹¨ê³„] LLM í™•ì¥: {expanded_keywords}")
            logger.info(f"[1ë‹¨ê³„] RAG ì¶”ì¶œ: {rag_keywords[:10]}")
            logger.info(f"[1ë‹¨ê³„] ìµœì¢… í‚¤ì›Œë“œ: {final_keywords[:15]}")
            logger.info(f"[1ë‹¨ê³„] í‚¤ì›Œë“œ ë§¤í•‘: {len(keyword_mapping)}ê°œ")
            return final_keywords
        except Exception as e:
            logger.error(f"ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ ìƒì„± ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(f"ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
            # í´ë°±: ì‹œê°„ëŒ€/ê³„ì ˆ ê¸°ë°˜ ì‹¤ìš©ì  í‚¤ì›Œë“œ
            fallback_keywords = []
            
            # ì‹œê°„ëŒ€ë³„ í‚¤ì›Œë“œ
            if time_slot == "ì €ë…":
                fallback_keywords.extend(["ì €ë…ì‹ì‚¬", "ì‹¤ë‚´í™œë™", "íœ´ì‹", "ê°€ì¡±ì‹œê°„"])
            elif time_slot == "ì˜¤ì „":
                fallback_keywords.extend(["ì•„ì¹¨", "ì¶œê·¼", "í™œë ¥", "ê±´ê°•"])
            elif time_slot == "ì˜¤í›„":
                fallback_keywords.extend(["ì ì‹¬", "ì•¼ì™¸í™œë™", "ìš´ë™", "ì‡¼í•‘"])
            else:
                fallback_keywords.extend(["ë°¤", "ìˆ˜ë©´", "íœ´ì‹"])
            
            # ê³„ì ˆë³„ í‚¤ì›Œë“œ
            if season == "ê²¨ìš¸":
                fallback_keywords.extend(["ë”°ëœ»í•œ", "ë³´ì˜¨", "ë‚œë°©"])
            elif season == "ì—¬ë¦„":
                fallback_keywords.extend(["ì‹œì›í•œ", "ëƒ‰ë°©", "íœ´ê°€"])
            elif season == "ë´„":
                fallback_keywords.extend(["ì‹ ì„ í•œ", "ì•¼ì™¸", "ê½ƒ"])
            else:
                fallback_keywords.extend(["ê°€ì„", "ê±´ê°•", "í™˜ì ˆê¸°"])
            
            print(f"[1ë‹¨ê³„ - í´ë°±] í´ë°± í‚¤ì›Œë“œ ì‚¬ìš©: {fallback_keywords}")
            logger.info(f"[1ë‹¨ê³„] í´ë°± í‚¤ì›Œë“œ ì‚¬ìš©: {fallback_keywords}")
            logger.info(f"[1ë‹¨ê³„] í´ë°± í‚¤ì›Œë“œ ê°œìˆ˜: {len(fallback_keywords)}")
            return fallback_keywords
    
    # ì£¼ì„: _expand_keywords_to_product_terms í•¨ìˆ˜ëŠ” ì œê±°ë¨
    # ì´ì œ _generate_base_context_keywordsì—ì„œ í‚¤ì›Œë“œ ìƒì„±ê³¼ í™•ì¥ì„ í•œ ë²ˆì— ì²˜ë¦¬
    
    async def _extract_keywords_from_actual_products(self, trend_keywords: List[str]) -> List[str]:
        """
        RAG ë°©ì‹: ì‹¤ì œ DB ìƒí’ˆëª… ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        1. íŠ¸ë Œë“œ í‚¤ì›Œë“œë¡œ ëŠìŠ¨í•˜ê²Œ ê²€ìƒ‰
        2. ê²€ìƒ‰ëœ ì‹¤ì œ ìƒí’ˆëª… ë¶„ì„
        3. LLMìœ¼ë¡œ ìœ ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Returns:
            ì‹¤ì œ DBì— ì¡´ì¬í•˜ëŠ” ìƒí’ˆ ê¸°ë°˜ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        
        print("=" * 80)
        print("[RAG í‚¤ì›Œë“œ ì¶”ì¶œ] ì‹¤ì œ ìƒí’ˆëª… ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘")
        print("=" * 80)
        
        try:
            # 1ë‹¨ê³„: ëŠìŠ¨í•œ ê²€ìƒ‰ (ìƒìœ„ 5ê°œ í‚¤ì›Œë“œë§Œ ì‚¬ìš©)
            query = " ".join(trend_keywords[:5])
            print(f"[1ë‹¨ê³„] ëŠìŠ¨í•œ ê²€ìƒ‰ ì¿¼ë¦¬: {query}")
            
            search_results = self.product_embedder.search_products(
                trend_keywords=[query],
                top_k=30,  # ì¶©ë¶„í•œ ìƒ˜í”Œ
                score_threshold=0.25,  # ë§¤ìš° ë‚®ì€ threshold
                only_ready_products=True
            )
            
            if not search_results:
                print("[RAG] ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ - ì›ë³¸ í‚¤ì›Œë“œ ë°˜í™˜")
                return trend_keywords
            
            # 2ë‹¨ê³„: ì‹¤ì œ ìƒí’ˆëª… ì¶”ì¶œ
            actual_product_names = [
                result.get("product_name", "")
                for result in search_results[:20]  # ìƒìœ„ 20ê°œë§Œ
            ]
            
            print(f"[2ë‹¨ê³„] ê²€ìƒ‰ëœ ìƒí’ˆ {len(actual_product_names)}ê°œ:")
            for i, name in enumerate(actual_product_names[:5], 1):
                print(f"  {i}. {name[:50]}")
            
            # 3ë‹¨ê³„: LLMìœ¼ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
            extraction_prompt = ChatPromptTemplate.from_messages([
                ("system", """ë‹¹ì‹ ì€ í™ˆì‡¼í•‘ ìƒí’ˆ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ì„ë¬´**: ì‹¤ì œ DB ìƒí’ˆëª…ë“¤ì„ ë¶„ì„í•´ì„œ ê²€ìƒ‰ì— ìœ ìš©í•œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

**ì¶”ì¶œ ê·œì¹™**:
1. ë¸Œëœë“œëª… ì¶”ì¶œ (ì˜ˆ: "ì¿ ì¿ ", "í•„ë¦½ìŠ¤", "ë½í† í•", "ì¢…ê·¼ë‹¹ê±´ê°•")
2. ìƒí’ˆ ì¹´í…Œê³ ë¦¬ (ì˜ˆ: "ì••ë ¥ì†¥", "ì—ì–´í”„ë¼ì´ì–´", "ìœ ì‚°ê· ", "ì˜¤ë©”ê°€3")
3. í•µì‹¬ í‚¤ì›Œë“œ (ì˜ˆ: "IH", "XXL", "í”„ë¡œë°”ì´ì˜¤í‹±ìŠ¤", "ì•Œí‹°ì§€")
4. ì¤‘ë³µ ì œê±°

**ì œì™¸ ê·œì¹™ (ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”)**:
- âŒ ìˆ«ì+ë‹¨ìœ„ ì¡°í•©: "12ê°œì›”", "15ê°œì›”", "3ë°•ìŠ¤", "6í†µ", "18ë°•ìŠ¤" ë“±
- âŒ ìˆœìˆ˜ ìˆ«ì: "12", "15", "3" ë“±
- âŒ ì ‘ë‘ì‚¬/ì ‘ë¯¸ì‚¬: "ì§_", "ë‹¨_", "ì„¸ì¼_", "[ì„¸ì¼]", "[í™˜ì›]" ë“±
- âŒ ì˜ë¯¸ì—†ëŠ” ë‹¨ì–´: "ê°œì›”", "ë°•ìŠ¤", "í†µ", "ê°œì›”ë¶„" ë“±

**ì˜ˆì‹œ**:
ìƒí’ˆëª…: "ì¢…ê·¼ë‹¹ê±´ê°• í”„ë¡œë©”ê°€ ì•Œí‹°ì§€ë¹„íƒ€ë¯¼D 12ê°œì›”"
ì¶”ì¶œ: ["ì¢…ê·¼ë‹¹ê±´ê°•", "í”„ë¡œë©”ê°€", "ì•Œí‹°ì§€", "ë¹„íƒ€ë¯¼D", "ì˜¤ë©”ê°€3"]
(âŒ "12ê°œì›”" ì œì™¸)

ìƒí’ˆëª…: "[ì„¸ì¼]ì•ˆêµ­ê±´ê°• ì´ˆì„ê³„ ì•Œí‹°ì§€ì˜¤ë©”ê°€3 12ê°œì›”"
ì¶”ì¶œ: ["ì•ˆêµ­ê±´ê°•", "ì´ˆì„ê³„", "ì•Œí‹°ì§€", "ì˜¤ë©”ê°€3"]
(âŒ "[ì„¸ì¼]", "12ê°œì›”" ì œì™¸)

JSON í˜•ì‹:
{{"keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", ...]}}""")
,
                ("human", """íŠ¸ë Œë“œ í‚¤ì›Œë“œ: {trend_keywords}

ìš°ë¦¬ DBì—ì„œ ê²€ìƒ‰ëœ ì‹¤ì œ ìƒí’ˆëª…ë“¤:
{product_names}

ìœ„ ìƒí’ˆëª…ë“¤ì„ ë¶„ì„í•´ì„œ ê²€ìƒ‰ì— ìœ ìš©í•œ í‚¤ì›Œë“œ 15-20ê°œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.""")
            ])
            
            chain = extraction_prompt | self.llm | JsonOutputParser()
            
            result = await chain.ainvoke({
                "trend_keywords": ", ".join(trend_keywords[:5]),
                "product_names": "\n".join([f"{i+1}. {name}" for i, name in enumerate(actual_product_names)])
            })
            
            extracted_keywords = result.get("keywords", [])
            
            # í›„ì²˜ë¦¬: ì˜ë¯¸ì—†ëŠ” í‚¤ì›Œë“œ í•„í„°ë§
            import re
            invalid_patterns = [
                r'^\d+ê°œì›”ë¶„?$',      # "12ê°œì›”", "15ê°œì›”ë¶„"
                r'^\d+ë°•ìŠ¤$',         # "3ë°•ìŠ¤", "18ë°•ìŠ¤"
                r'^\d+í†µ$',           # "6í†µ"
                r'^\d+$',             # ìˆœìˆ˜ ìˆ«ì
                r'^[\[\(].*[\]\)]$',  # "[ì„¸ì¼]", "(í™˜ì›)" ë“±
                r'^ì§_',              # "ì§_" ì ‘ë‘ì‚¬
                r'^ë‹¨_',              # "ë‹¨_" ì ‘ë‘ì‚¬
                r'^ì„¸ì¼_',            # "ì„¸ì¼_" ì ‘ë‘ì‚¬
            ]
            
            filtered_keywords = []
            removed_keywords = []
            for kw in extracted_keywords:
                is_invalid = False
                for pattern in invalid_patterns:
                    if re.match(pattern, kw):
                        is_invalid = True
                        removed_keywords.append(kw)
                        break
                if not is_invalid and len(kw) >= 2:  # ìµœì†Œ 2ê¸€ì
                    filtered_keywords.append(kw)
            
            if removed_keywords:
                print(f"[3ë‹¨ê³„ - í•„í„°ë§] ì œê±°ëœ í‚¤ì›Œë“œ: {removed_keywords}")
            
            print("=" * 80)
            print(f"[3ë‹¨ê³„] LLM ì¶”ì¶œ ì™„ë£Œ: {len(extracted_keywords)}ê°œ â†’ í•„í„°ë§ í›„ {len(filtered_keywords)}ê°œ")
            print(f"[ì¶”ì¶œ í‚¤ì›Œë“œ] {filtered_keywords[:10]}...")
            print("=" * 80)
            
            return filtered_keywords
            
        except Exception as e:
            logger.error(f"RAG í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(f"ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
            
            # í´ë°±: ì›ë³¸ í‚¤ì›Œë“œ ë°˜í™˜
            print(f"[RAG ì‹¤íŒ¨] ì›ë³¸ í‚¤ì›Œë“œ ì‚¬ìš©: {trend_keywords}")
            return trend_keywords
    
    async def _generate_context_keywords(self, context: Dict[str, Any]) -> List[str]:
        """ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í‚¤ì›Œë“œ ìƒì„± (1ë‹¨ê³„ë§Œ - 2ë‹¨ê³„ëŠ” ìƒìœ„ì—ì„œ ë³„ë„ í˜¸ì¶œ)"""
        
        print("=" * 80)
        print("[í†µí•© í‚¤ì›Œë“œ ìƒì„±] 1ë‹¨ê³„: ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ")
        print("=" * 80)
        
        # 1ë‹¨ê³„: ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ (ë‚ ì”¨, ì‹œê°„ëŒ€, ê³„ì ˆ, ê³µíœ´ì¼)
        base_keywords = await self._generate_base_context_keywords(context)
        logger.info(f"1ë‹¨ê³„ ê¸°ë³¸ í‚¤ì›Œë“œ: {base_keywords}")
        
        # 2ë‹¨ê³„(ì‹¤ì‹œê°„ ì›¹ ê²€ìƒ‰)ëŠ” _collect_context_and_keywords()ì—ì„œ ë³„ë„ í˜¸ì¶œë¨
        # ì—¬ê¸°ì„œëŠ” 1ë‹¨ê³„ ê²°ê³¼ë§Œ ë°˜í™˜
        
        print("=" * 80)
        print(f"[1ë‹¨ê³„ ì™„ë£Œ] ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ {len(base_keywords)}ê°œ")
        print(f"  í‚¤ì›Œë“œ: {base_keywords[:10]}...")
        print("=" * 80)
        logger.info(f"ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ ìƒì„± ì™„ë£Œ: {base_keywords[:20]}")
        
        return base_keywords
    
    async def _generate_unified_candidates(
        self,
        search_result: Dict[str, Any],
        context: Dict[str, Any],
        max_trend_match: int = 8,  # ìœ ì‚¬ë„ ê¸°ë°˜ ìµœëŒ€ ê°œìˆ˜ (ì˜ë¥˜ í¸ì¤‘ ë°©ì§€)
        max_sales_prediction: int = 32  # ë§¤ì¶œì˜ˆì¸¡ ê¸°ë°˜ ìµœëŒ€ ê°œìˆ˜ (ë‹¤ì–‘ì„± í™•ë³´)
    ) -> tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """í†µí•© í›„ë³´êµ° ìƒì„± - ëª¨ë“  ìƒí’ˆ XGBoost ì˜ˆì¸¡ í›„ ê°€ì¤‘ì¹˜ ì¡°ì •"""
        
        candidates = []
        seen_products = set()
        
        print(f"=== [DEBUG Unified Candidates] í›„ë³´êµ° ìƒì„± ì‹œì‘ (ëª©í‘œ: ìµœëŒ€ {max_trend_match + max_sales_prediction}ê°œ) ===")
        
        # 1. ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ë¥¼ í•˜ë‚˜ë¡œ í†µí•©
        all_products = []
        all_products.extend(search_result["direct_products"])  # ê³ ìœ ì‚¬ë„ ìƒí’ˆ
        
        # ì¹´í…Œê³ ë¦¬ ê·¸ë£¹ì˜ ëª¨ë“  ìƒí’ˆë„ ì¶”ê°€
        for category, products in search_result["category_groups"].items():
            all_products.extend(products)
        
        print(f"=== [DEBUG] í†µí•©ëœ ìƒí’ˆ ìˆ˜: {len(all_products)}ê°œ ===")
        
        # 2. ì¤‘ë³µ ì œê±° (ìƒí’ˆì½”ë“œ + ì†Œë¶„ë¥˜ + ë¸Œëœë“œ)
        unique_products = {}
        seen_category_brand_pairs = set()  # (ì†Œë¶„ë¥˜, ë¸Œëœë“œ) ì¡°í•©
        
        for product in all_products:
            product_code = product.get("product_code")
            category_sub = product.get("category_sub", "")
            brand = product.get("brand", "")
            
            # ìƒí’ˆì½”ë“œ ì¤‘ë³µ ì²´í¬
            if product_code in unique_products:
                continue
            
            # ì†Œë¶„ë¥˜ + ë¸Œëœë“œ ì¡°í•© ì¤‘ë³µ ì²´í¬ (ë‹¤ì–‘ì„± ë³´ì¥)
            category_brand_key = (category_sub, brand)
            if category_sub and brand and category_brand_key in seen_category_brand_pairs:
                logger.info(f"ì†Œë¶„ë¥˜+ë¸Œëœë“œ ì¤‘ë³µ ì œì™¸: {product.get('product_name', '')[:30]} (ì†Œë¶„ë¥˜: {category_sub}, ë¸Œëœë“œ: {brand})")
                continue
            
            # í†µê³¼í•œ ê²½ìš° ì¶”ê°€
            unique_products[product_code] = product
            if category_sub and brand:
                seen_category_brand_pairs.add(category_brand_key)
        
        print(f"=== [DEBUG] ì¤‘ë³µ ì œê±° í›„: {len(unique_products)}ê°œ (ì†Œë¶„ë¥˜+ë¸Œëœë“œ ë‹¤ì–‘ì„± ë³´ì¥) ===")
        
        # 3. ë°°ì¹˜ ì˜ˆì¸¡ ì¤€ë¹„ (ìƒìœ„ 40ê°œ - ì˜ë¥˜ í¸ì¤‘ ë°©ì§€)
        products_list = list(unique_products.values())[:40]
        print(f"=== [DEBUG] ë°°ì¹˜ ì˜ˆì¸¡ ëŒ€ìƒ: {len(products_list)}ê°œ ===")
        
        # 4. ë°°ì¹˜ XGBoost ì˜ˆì¸¡ (í•œ ë²ˆì— ì²˜ë¦¬)
        predicted_sales_list = await self._predict_products_sales_batch(products_list, context)
        
        # 5. ì˜ˆì¸¡ ê²°ê³¼ì™€ ìƒí’ˆ ë§¤ì¹­ + ì ìˆ˜ ê³„ì‚°
        for i, product in enumerate(products_list):
            similarity = product.get("similarity_score", 0.5)
            predicted_sales = predicted_sales_list[i]
            
            # ì ìˆ˜ ê³„ì‚° (ìœ ì‚¬ë„ vs ë§¤ì¶œ ê°€ì¤‘ì¹˜ ì¡°ì •)
            if similarity >= 0.7:
                # ê³ ìœ ì‚¬ë„: ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜ ë†’ì„
                final_score = (
                    similarity * 0.7 +  # ìœ ì‚¬ë„ 70%
                    (predicted_sales / 100000000) * 0.3  # ë§¤ì¶œ 30% (ì •ê·œí™”: 1ì–µ ê¸°ì¤€)
                )
                source = "trend_match"
                print(f"  [ê³ ìœ ì‚¬ë„] {product.get('product_name')[:20]}: ìœ ì‚¬ë„={similarity:.2f}, ë§¤ì¶œ={predicted_sales/10000:.0f}ë§Œì›, ì ìˆ˜={final_score:.3f}")
            else:
                # ì €ìœ ì‚¬ë„: ë§¤ì¶œ ê°€ì¤‘ì¹˜ ë†’ì„
                final_score = (
                    similarity * 0.3 +  # ìœ ì‚¬ë„ 30%
                    (predicted_sales / 100000000) * 0.7  # ë§¤ì¶œ 70%
                )
                source = "sales_prediction"
                print(f"  [ì €ìœ ì‚¬ë„] {product.get('product_name')[:20]}: ìœ ì‚¬ë„={similarity:.2f}, ë§¤ì¶œ={predicted_sales/10000:.0f}ë§Œì›, ì ìˆ˜={final_score:.3f}")
            
            candidates.append({
                "product": product,
                "source": source,
                "similarity_score": similarity,
                "predicted_sales": predicted_sales,
                "final_score": final_score
            })
        
        # 4. ì ìˆ˜ìˆœ ì •ë ¬
        candidates.sort(key=lambda x: x["final_score"], reverse=True)
        
        print(f"=== [DEBUG] ì´ {len(candidates)}ê°œ í›„ë³´ ìƒì„± ì™„ë£Œ, ì ìˆ˜ìˆœ ì •ë ¬ë¨ ===")
        
        # 5. ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚° (ë‚´ë¶€ ì‚¬ìš©ìš©)
        category_scores = {}
        category_sales = {}
        for candidate in candidates:
            category = candidate["product"].get("category_main", "ê¸°íƒ€")
            if category == "ê¸°íƒ€" or not category:
                continue
            if category not in category_sales:
                category_sales[category] = []
            category_sales[category].append(candidate["predicted_sales"])
        
        for category, sales_list in category_sales.items():
            avg_sales = sum(sales_list) / len(sales_list)
            category_scores[category] = {"predicted_sales": avg_sales}
        
        return candidates, category_scores
    
    async def _predict_categories_with_xgboost(
        self, 
        category_groups: Dict[str, List[Dict]], 
        context: Dict[str, Any]
    ) -> Dict[str, Dict[str, float]]:
        """ì¹´í…Œê³ ë¦¬ë³„ XGBoost ë§¤ì¶œ ì˜ˆì¸¡"""
        
        category_scores = {}
        broadcast_dt = context["broadcast_dt"]
        
        for category, products in category_groups.items():
            if not products:
                continue
            
            try:
                # ëŒ€í‘œ ìƒí’ˆìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ë§¤ì¶œ ì˜ˆì¸¡
                representative_product = products[0]
                predicted_sales = await self._predict_product_sales(representative_product, context)
                
                # ì¹´í…Œê³ ë¦¬ ë‚´ ìƒí’ˆ ìˆ˜ë¡œ ë³´ì •
                adjusted_sales = predicted_sales * min(len(products) / 5, 2.0)
                
                category_scores[category] = {
                    "predicted_sales": adjusted_sales,
                    "product_count": len(products),
                    "avg_similarity": sum(p.get("similarity_score", 0) for p in products) / len(products)
                }
                
                print(f"  - ì¹´í…Œê³ ë¦¬ '{category}': {int(adjusted_sales/10000)}ë§Œì› (ìƒí’ˆ: {len(products)}ê°œ)")
                
            except Exception as e:
                logger.error(f"ì¹´í…Œê³ ë¦¬ '{category}' ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                category_scores[category] = {
                    "predicted_sales": 10000000,  # ê¸°ë³¸ê°’ 1000ë§Œì›
                    "product_count": len(products),
                    "avg_similarity": 0.4
                }
        
        return category_scores
    
    async def _rank_final_candidates(self, candidates: List[Dict[str, Any]], category_scores: Dict[str, Any], context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ìµœì¢… ë­í‚¹ ê³„ì‚° - ì‹œì¦Œ ì í•©ì„± + ì¹´í…Œê³ ë¦¬+ë¸Œëœë“œ ë‹¤ì–‘ì„± ì ìš©"""
        
        print(f"=== [DEBUG _rank_final_candidates] ì´ë¯¸ ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬ëœ {len(candidates)}ê°œ í›„ë³´ ìˆ˜ì‹  ===")
        
        # 0. ì‹œì¦Œ ì í•©ì„± í•„í„°ë§ (LLM ë°°ì¹˜ íŒë‹¨) - ìƒìœ„ 40ê°œ í›„ë³´ì— ëŒ€í•´
        top_candidates = candidates[:40]  # ì¶©ë¶„í•œ í›„ë³´êµ° ì¤€ë¹„ (ì‹œì¦Œ í•„í„° + ì¤‘ë³µ ì œê±° ê³ ë ¤)
        print(f"\n=== [ì‹œì¦Œ ì í•©ì„± ê²€ì‚¬] ìƒìœ„ {len(top_candidates)}ê°œ í›„ë³´ ê²€ì‚¬ ì‹œì‘ ===")
        
        season_filtered = await self._filter_by_season_suitability(top_candidates, context)
        print(f"=== [ì‹œì¦Œ ì í•©ì„± ê²€ì‚¬] {len(top_candidates)}ê°œ â†’ {len(season_filtered)}ê°œ (ë¶€ì í•© {len(top_candidates) - len(season_filtered)}ê°œ ì œê±°) ===\n")
        
        # 1. ì¹´í…Œê³ ë¦¬+ë¸Œëœë“œ ì¤‘ë³µ ì œê±° + ëŒ€ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ ì¿¼í„° ì œí•œ
        category_brand_seen = set()
        category_count = {}  # ëŒ€ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ë³„ ê°œìˆ˜
        filtered_candidates = []
        
        for candidate in season_filtered:
            product = candidate["product"]
            product_name = product.get("product_name", "")
            category = product.get("category_main", "Unknown")
            brand = product.get("brand", "Unknown")
            key = f"{category}_{brand}"
            
            # 1-1. ê°™ì€ ì¹´í…Œê³ ë¦¬+ë¸Œëœë“œ ì¡°í•©ì€ 1ê°œë§Œ í—ˆìš© (ë‹¤ì–‘ì„± ë³´ì¥)
            if key in category_brand_seen:
                print(f"  âš ï¸ ë¸Œëœë“œ ì¤‘ë³µ ì œê±°: {product_name[:30]} (ì¹´í…Œê³ ë¦¬: {category}, ë¸Œëœë“œ: {brand})")
                continue
            
            # 1-2. ê°™ì€ ëŒ€ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ëŠ” ìµœëŒ€ 4ê°œê¹Œì§€ë§Œ í—ˆìš©
            current_count = category_count.get(category, 0)
            if current_count >= 4:
                print(f"  âš ï¸ ì¹´í…Œê³ ë¦¬ ì¿¼í„° ì´ˆê³¼: {product_name[:30]} (ì¹´í…Œê³ ë¦¬: {category}, ì´ë¯¸ {current_count}ê°œ)")
                continue
            
            # í†µê³¼: í›„ë³´ì— ì¶”ê°€
            filtered_candidates.append(candidate)
            category_brand_seen.add(key)
            category_count[category] = current_count + 1
        
        print(f"=== [ë‹¤ì–‘ì„± í•„í„°ë§] {len(season_filtered)}ê°œ â†’ {len(filtered_candidates)}ê°œ (ì¤‘ë³µ {len(season_filtered) - len(filtered_candidates)}ê°œ ì œê±°) ===")
        print(f"=== [ì¹´í…Œê³ ë¦¬ ë¶„í¬] {category_count} ===")
        
        for i, candidate in enumerate(filtered_candidates[:5]):
            product = candidate['product']
            print(f"  {i+1}ìœ„: {product.get('product_name')[:25]} | {product.get('category_main', 'N/A')} | {product.get('brand', 'N/A')} (ì ìˆ˜: {candidate['final_score']:.3f})")
        
        return filtered_candidates
    
    async def _filter_by_season_suitability(self, candidates: List[Dict[str, Any]], context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ì‹œì¦Œ ì í•©ì„± í•„í„°ë§ - LLM ë°°ì¹˜ íŒë‹¨"""
        
        if not candidates:
            return []
        
        # í˜„ì¬ ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
        broadcast_dt = context.get("broadcast_dt")
        month = broadcast_dt.month if broadcast_dt else 11
        day = broadcast_dt.day if broadcast_dt else 19
        holiday_name = context.get("holiday_name")
        
        # ìƒí’ˆ ì •ë³´ ì¤€ë¹„ (ìƒí’ˆëª… + í…Œì´í”„ëª…)
        products_info = []
        for i, candidate in enumerate(candidates):
            product = candidate["product"]
            products_info.append({
                "index": i,
                "product_name": product.get("product_name", ""),
                "tape_name": product.get("tape_name", ""),
                "category": product.get("category_main", "")
            })
        
        # LLM í”„ë¡¬í”„íŠ¸
        season_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ 20ë…„ì°¨ í™ˆì‡¼í•‘ ë°©ì†¡ í¸ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
í˜„ì¬ ë‚ ì§œ/ê³„ì ˆì— ì–´ìš¸ë¦¬ì§€ ì•ŠëŠ” ìƒí’ˆì„ ì°¾ì•„ì£¼ì„¸ìš”.

**ì œì™¸ ê¸°ì¤€ (ìƒí’ˆì˜ ì‹¤ì œ íŠ¹ì„± ì¤‘ì‹¬):**

1. ëª…ì ˆ ë¶ˆì¼ì¹˜: íŠ¹ì • ëª…ì ˆ ìƒí’ˆì¸ë° í˜„ì¬ ëª…ì ˆê³¼ ë§ì§€ ì•ŠëŠ” ê²½ìš°
   - ì˜ˆ: 11ì›”ì— "ì‹ ë…„íŠ¹ì§‘", "ì„¤ë‚ ", "ì¶”ì„" í¬í•¨ ìƒí’ˆ
   - ì˜ˆ: 7ì›”ì— "í¬ë¦¬ìŠ¤ë§ˆìŠ¤" í¬í•¨ ìƒí’ˆ
   - ì„ í–‰ íŒë§¤ í—ˆìš©: 12ì›” ë§ ì‹ ë…„íŠ¹ì§‘ â­•, 12ì›” ì¤‘ìˆœ í¬ë¦¬ìŠ¤ë§ˆìŠ¤ â­•, 8ì›” ë§ ì¶”ì„ â­•

2. ê³„ì ˆ/ë‚ ì”¨ ë¶€ì í•© ìƒí’ˆ (ìƒí’ˆ íŠ¹ì„±ìœ¼ë¡œë§Œ íŒë‹¨):
   
   **ê²¨ìš¸ì² (11ì›”~2ì›”) - ì¶”ìš´ ë‚ ì”¨ì— ì œì™¸í•  ê²ƒ:**
   - ì—¬ë¦„ ëƒ‰ë°©: "ëƒ‰ê°", "ì¿¨ë§", "ì‹œì›í•œ", "ëƒ‰ë°©", "í”¼ì„œìš©", "ì—¬ë¦„ìš©"
   - ì—¬ë¦„ ì˜ë¥˜: "ë°˜íŒ”", "ë°˜ë°”ì§€", "ë¯¼ì†Œë§¤", "ìƒŒë“¤" (ì‹¤ë‚´ìš© ì œì™¸)
   - ì—¬ë¦„ ì¹¨êµ¬: "ëƒ‰ê° íŒ¨ë“œ", "ì¿¨ë§¤íŠ¸"
   - ì˜ˆ: "ì¿¨ë“œë¦¼ ëƒ‰ê°íŒ¨ë“œ", "ì—¬ë¦„ ë°˜íŒ”í‹°", "í”¼ì„œìš© ì„ í’ê¸°"
   
   **ê²¨ìš¸ì² (11ì›”~2ì›”) - ì¶”ìš´ ë‚ ì”¨ì— ì í•© (í—ˆìš©):**
   - ë‚œë°© ìƒí’ˆ: "ì „ê¸°ì¥íŒ", "ì „ê¸°ë‹´ìš”", "ì˜¨ì—´", "ë‚œë°©", "ë³´ì˜¨"
   - ê²¨ìš¸ ì˜ë¥˜: "íŒ¨ë”©", "ê¸°ëª¨", "ê²¨ìš¸", "ì½”íŠ¸", "ëª©ë„ë¦¬", "ì¥ê°‘", "ë‘êº¼ìš´"
   - ì˜ˆ: "ì „ê¸°ë§¤íŠ¸", "ì˜¨ì—´ë§ˆì‚¬ì§€ê¸°", "íŒ¨ë”©", "ê¸°ëª¨ë°”ì§€" â†’ ëª¨ë‘ OK!
   
   **ì—¬ë¦„ì² (6ì›”~8ì›”) - ë”ìš´ ë‚ ì”¨ì— ì œì™¸í•  ê²ƒ:**
   - ë‚œë°© ìƒí’ˆ: "ì „ê¸°ì¥íŒ", "ì „ê¸°ë‹´ìš”", "ì˜¨ì—´", "ë‚œë°©"
   - ê²¨ìš¸ ì˜ë¥˜: "íŒ¨ë”©", "ê¸°ëª¨", "ê²¨ìš¸", "ë‘êº¼ìš´ ì½”íŠ¸", "ëª©ë„ë¦¬"
   - ì˜ˆ: "ê²¨ìš¸ íŒ¨ë”©", "ê¸°ëª¨ ë°”ì§€", "ì „ê¸°ì¥íŒ"
   
   **ë´„/ê°€ì„(3~5ì›”, 9~10ì›”) - í™˜ì ˆê¸°:**
   - 3~5ì›”: ê²¨ìš¸ ë‚œë°© ìƒí’ˆ ì œì™¸, ì—¬ë¦„ ëƒ‰ë°© ìƒí’ˆ OK
   - 9~10ì›”: ì—¬ë¦„ ëƒ‰ë°© ìƒí’ˆ ì œì™¸, ê²¨ìš¸ ë‚œë°© ìƒí’ˆ OK

**ì¤‘ìš” - ì‹œì¦Œ ì½”ë“œ(SS/FW)ëŠ” ë¬´ì‹œí•˜ì„¸ìš”:**
- "25SS", "24FW" ê°™ì€ ì½”ë“œëŠ” ì°¸ê³ ë§Œ í•˜ê³ , ìƒí’ˆì˜ ì‹¤ì œ íŠ¹ì„±ìœ¼ë¡œ íŒë‹¨
- ì˜ˆ: "25SS ê¸°ëª¨ ë°”ì§€" â†’ ê¸°ëª¨ê°€ ìˆìœ¼ë©´ ê²¨ìš¸ì— OK
- ì˜ˆ: "24FW ë°˜íŒ”í‹°" â†’ ë°˜íŒ”ì´ë©´ ê²¨ìš¸ì— ì œì™¸
- ì˜ˆ: "23SS íŒ¨ë”©" â†’ íŒ¨ë”©ì´ë©´ ì—¬ë¦„ì— ì œì™¸

# ì„ í–‰ íŒë§¤ëŠ” í—ˆìš© (1~2ì£¼ ì „)
- 12ì›” ë§ ì‹ ë…„íŠ¹ì§‘ â­•
- 12ì›” ì¤‘ìˆœ í¬ë¦¬ìŠ¤ë§ˆìŠ¤ ì¼€ì´í¬ â­•
- 8ì›” ë§ ì¶”ì„ì„ ë¬¼ì„¸íŠ¸ â­•

**ì œì™¸í•˜ì§€ ë§ ê²ƒ:**
- ì‚¬ê³„ì ˆ ìƒí’ˆ: ê±´ê°•ì‹í’ˆ, ìƒí™œìš©í’ˆ, ì‹í’ˆ, ê°€ì „ ë“±
- ì‹œì¦Œ í‚¤ì›Œë“œê°€ ì—†ëŠ” ì¼ë°˜ ìƒí’ˆ

JSON í˜•ì‹ìœ¼ë¡œ ì œì™¸í•  ìƒí’ˆì˜ ì¸ë±ìŠ¤ ë°°ì—´ì„ ë°˜í™˜í•˜ì„¸ìš”:
{{
  "exclude_indices": [ì¸ë±ìŠ¤ ë°°ì—´],
  "reasons": {{
    "ì¸ë±ìŠ¤": "ì œì™¸ ì´ìœ "
  }}
}}"""),
            ("human", """í˜„ì¬ ì •ë³´:
- ë‚ ì§œ: {month}ì›” {day}ì¼
- ê³µíœ´ì¼: {holiday_name}

ìƒí’ˆ ëª©ë¡:
{products_list}

ìœ„ ìƒí’ˆ ì¤‘ í˜„ì¬ ë‚ ì§œ/ì‹œì¦Œì— ì í•©í•˜ì§€ ì•Šì€ ìƒí’ˆì˜ ì¸ë±ìŠ¤ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”.
ì˜ˆ: 11ì›” ì¤‘ìˆœì´ë©´ ê²¨ìš¸ ìƒí’ˆì€ OK, ì¶”ì„/ì„¤ë‚  ìƒí’ˆì€ ì œì™¸""")
        ])
        
        # ìƒí’ˆ ëª©ë¡ ë¬¸ìì—´ ìƒì„± (ìƒí’ˆëª… + í…Œì´í”„ëª…)
        products_list_str = "\n".join([
            f"{p['index']}. {p['product_name']}\n   í…Œì´í”„ëª…: {p['tape_name']}\n   ì¹´í…Œê³ ë¦¬: {p['category']}"
            for p in products_info
        ])
        
        chain = season_prompt | self.llm | JsonOutputParser()
        
        try:
            result = await chain.ainvoke({
                "month": month,
                "day": day,
                "holiday_name": holiday_name if holiday_name else "ì—†ìŒ",
                "products_list": products_list_str
            })
            
            exclude_indices = set(result.get("exclude_indices", []))
            reasons = result.get("reasons", {})
            
            # ì œì™¸ëœ ìƒí’ˆ ë¡œê·¸
            for idx in exclude_indices:
                if idx < len(candidates):
                    product_name = candidates[idx]["product"].get("product_name", "")[:40]
                    reason = reasons.get(str(idx), "ì‹œì¦Œ ë¶€ì í•©")
                    print(f"  âŒ ì œì™¸: {product_name} - {reason}")
            
            # í•„í„°ë§
            filtered = [c for i, c in enumerate(candidates) if i not in exclude_indices]
            return filtered
            
        except Exception as e:
            logger.error(f"ì‹œì¦Œ ì í•©ì„± íŒë‹¨ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(f"ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
            # ì˜¤ë¥˜ ì‹œ ëª¨ë“  í›„ë³´ ë°˜í™˜ (ì•ˆì „ì¥ì¹˜)
            return candidates
    
    def _calculate_competition_penalty(self, product: Dict[str, Any], all_candidates: List[Dict[str, Any]]) -> float:
        """ê²½ìŸ í˜ë„í‹° ì ìˆ˜ ê³„ì‚°"""
        category = product.get("category_main", "")
        same_category_count = sum(1 for c in all_candidates if c["product"].get("category_main") == category)
        
        # ê°™ì€ ì¹´í…Œê³ ë¦¬ ìƒí’ˆì´ ë§ì„ìˆ˜ë¡ í˜ë„í‹°
        if same_category_count <= 2:
            return 0.0
        elif same_category_count <= 4:
            return 0.1
        else:
            return 0.2
    
    async def _format_response(self, ranked_products: List[Dict[str, Any]], context: Dict[str, Any] = None) -> BroadcastResponse:
        """API ì‘ë‹µ ìƒì„± (ë¹„ë™ê¸°)"""
        print(f"=== [DEBUG _format_response] context keys: {context.keys() if context else 'None'} ===")
        if context:
            print(f"=== [DEBUG _format_response] generated_keywords: {context.get('generated_keywords', [])} ===")
        
        # 1. í…Œì´í”„ ì½”ë“œ ëª©ë¡ ì¶”ì¶œ
        tape_codes = [p["product"].get("tape_code") for p in ranked_products if p["product"].get("tape_code")]
        
        # 2. ìµœê·¼ ë°©ì†¡ ì‹¤ì  ë°°ì¹˜ ì¡°íšŒ (Netezza)
        broadcast_history_map = {}
        if tape_codes:
            logger.info(f" {len(tape_codes)}ê°œ í…Œì´í”„ì˜ ìµœê·¼ ë°©ì†¡ ì‹¤ì  ì¡°íšŒ ì¤‘...")
            broadcast_history_map = self.broadcast_history_service.get_latest_broadcasts_batch(tape_codes)
            logger.info(f" {sum(1 for v in broadcast_history_map.values() if v is not None)}ê°œ í…Œì´í”„ì˜ ì‹¤ì  ì¡°íšŒ ì„±ê³µ")
        
        recommendations = []
        
        # ìˆœìœ„ ì •ë³´ ì¶”ê°€ (ë°°ì¹˜ ì²˜ë¦¬ ì „)
        for i, candidate in enumerate(ranked_products):
            candidate["rank"] = i + 1
            candidate["total_count"] = len(ranked_products)
        
        # [5-1ë‹¨ê³„] ë°°ì¹˜ë¡œ ëª¨ë“  ìƒí’ˆì˜ ì¶”ì²œ ê·¼ê±° ìƒì„± (í•œ ë²ˆì˜ LLM í˜¸ì¶œ)
        step_5_1_start = time.time()
        print("\n" + "=" * 80)
        print(f"[5-1ë‹¨ê³„] LLM ë°°ì¹˜ ì²˜ë¦¬ - {len(ranked_products)}ê°œ ìƒí’ˆì˜ ì¶”ì²œ ê·¼ê±° ìƒì„±")
        print("=" * 80)
        reasoning_list = await self._generate_batch_reasons_with_langchain(
            ranked_products,
            context or {"time_slot": "ì €ë…", "weather": {"weather": "í­ì—¼"}}
        )
        print(f"â±ï¸  [5-1ë‹¨ê³„] ì¶”ì²œ ê·¼ê±° ìƒì„±: {time.time() - step_5_1_start:.2f}ì´ˆ")
        
        for i, candidate in enumerate(ranked_products):
            product = candidate["product"]
            reasoning_summary = reasoning_list[i] if i < len(reasoning_list) else f"{product.get('category_main', 'ìƒí’ˆ')} ì¶”ì²œ"
            
            # ìµœê·¼ ë°©ì†¡ ì‹¤ì  ì¡°íšŒ
            tape_code = product.get("tape_code")
            last_broadcast_data = broadcast_history_map.get(tape_code) if tape_code else None
            last_broadcast = None
            
            if last_broadcast_data:
                try:
                    last_broadcast = LastBroadcastMetrics(**last_broadcast_data)
                    logger.debug(f"âœ… í…Œì´í”„ {tape_code}ì˜ ìµœê·¼ ë°©ì†¡ ì‹¤ì  ì¶”ê°€")
                except Exception as e:
                    logger.warning(f"âš ï¸ í…Œì´í”„ {tape_code}ì˜ ì‹¤ì  ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {e}")
            
            recommendation = BroadcastRecommendation(
                rank=i+1,
                productInfo=ProductInfo(
                    productId=product.get("product_code", "Unknown"),
                    productName=product.get("product_name", "Unknown"),
                    category=product.get("category_main", "Unknown"),
                    categoryMiddle=product.get("category_middle"),
                    categorySub=product.get("category_sub"),
                    brand=product.get("brand"),
                    price=product.get("price"),
                    tapeCode=product.get("tape_code"),
                    tapeName=product.get("tape_name")
                ),
                reasoning=reasoning_summary,
                businessMetrics=BusinessMetrics(
                    aiPredictedSales=f"{round(candidate['predicted_sales']/10000, 1):,.1f}ë§Œì›",  # AI ì˜ˆì¸¡ ë§¤ì¶œ (XGBoost, ì†Œìˆ˜ì  1ìë¦¬)
                    lastBroadcast=last_broadcast  # ìµœê·¼ ë°©ì†¡ ì‹¤ì  ì¶”ê°€
                )
            )

            # ì¶”ì²œ ê²°ê³¼ ìš”ì•½ ë¡œê·¸ (ì‹œì—°/ë¶„ì„ìš©)
            try:
                print(
                    f"[RECOMMENDATION] #{recommendation.rank} "
                    f"{recommendation.productInfo.productName[:30]} | "
                    f"{recommendation.productInfo.category} | "
                    f"ë§¤ì¶œ: {recommendation.businessMetrics.aiPredictedSales} | "
                    f"ì ìˆ˜: {candidate.get('final_score', 0.0):.3f}"
                )
            except Exception:
                pass

            recommendations.append(recommendation)
        
        # [5-2ë‹¨ê³„] ë„¤ì´ë²„/íƒ€ì‚¬ í¸ì„± ì¡°íšŒ ë° AI ì„ íƒ
        step_5_2_start = time.time()
        print("\n" + "=" * 80)
        print(f"[5-2ë‹¨ê³„] ë„¤ì´ë²„/íƒ€ì‚¬ í¸ì„± ì¡°íšŒ ë° AI ì„ íƒ")
        print("=" * 80)
        
        # ë„¤ì´ë²„ ë² ìŠ¤íŠ¸ ìƒí’ˆ ì¡°íšŒ
        naver_products_data = self.external_products_service.get_latest_best_products(limit=10)
        naver_products = [NaverProduct(**product) for product in naver_products_data]
        logger.info(f"âœ… ë„¤ì´ë²„ ìƒí’ˆ {len(naver_products)}ê°œ ìˆ˜ì§‘")
        print(f"âœ… ë„¤ì´ë²„ ìƒí’ˆ {len(naver_products)}ê°œ ìˆ˜ì§‘")
        
        # íƒ€ í™ˆì‡¼í•‘ì‚¬ í¸ì„± ìƒí’ˆ ì¡°íšŒ - Netezzaì—ì„œ ì‹¤ì‹œê°„ ì¡°íšŒ
        try:
            broadcast_time_str = context.get("broadcast_time") if context else None
            if broadcast_time_str:
                competitor_data = await netezza_conn.get_competitor_schedules(broadcast_time_str)
                competitor_products = [CompetitorProduct(**comp) for comp in competitor_data]
                logger.info(f"âœ… íƒ€ì‚¬ í¸ì„± {len(competitor_products)}ê°œ ìˆ˜ì§‘")
                print(f"âœ… íƒ€ì‚¬ í¸ì„± {len(competitor_products)}ê°œ ìˆ˜ì§‘")
            else:
                logger.warning(f"âš ï¸ broadcast_timeì´ contextì— ì—†ìŒ")
                competitor_products = []
        except Exception as e:
            logger.warning(f"âš ï¸ íƒ€ì‚¬ í¸ì„± ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            competitor_products = []
        
        # AI ê¸°ë°˜ ë„¤ì´ë²„/íƒ€ì‚¬ í¸ì„± 10ê°œ ì„ íƒ ë° í†µí•©
        selected_competitor_products = await self._select_and_merge_top_10(
            naver_products=naver_products,
            competitor_products=competitor_products,
            broadcast_time=broadcast_time_str,
            context=context
        )
        print(f"â±ï¸  [5-2ë‹¨ê³„] ë„¤ì´ë²„/íƒ€ì‚¬ ì„ íƒ: {time.time() - step_5_2_start:.2f}ì´ˆ")
        
        return BroadcastResponse(
            requestTime="",  # ë©”ì¸ì—ì„œ ì„¤ì •
            recommendations=recommendations,
            competitorProducts=selected_competitor_products
        )
    
    async def _select_and_merge_top_10(
        self,
        naver_products: List[NaverProduct],
        competitor_products: List[CompetitorProduct],
        broadcast_time: str,
        context: Dict[str, Any] = None
    ) -> List[CompetitorProduct]:
        """
        AIë¥¼ í™œìš©í•˜ì—¬ ë„¤ì´ë²„/íƒ€ì‚¬ í¸ì„± ì¤‘ 10ê°œë¥¼ ì„ íƒí•˜ê³  í†µí•©
        ë„¤ì´ë²„:íƒ€ì‚¬ = 5:5 ë¹„ìœ¨ ìœ ì§€ (í•œìª½ì´ ë¶€ì¡±í•˜ë©´ ë‹¤ë¥¸ìª½ìœ¼ë¡œ ì±„ì›€)
        """
        try:
            # 1. ë„¤ì´ë²„ ìƒí’ˆì„ íƒ€ì‚¬ í¸ì„± í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            naver_as_competitor = [
                self._convert_naver_to_competitor(naver, idx)
                for idx, naver in enumerate(naver_products)
            ]
            
            # 2. AIì—ê²Œ 10ê°œ ì„ íƒ ìš”ì²­
            selected_indices = await self._ai_select_top_10(
                naver_products=naver_products,
                competitor_products=competitor_products,
                broadcast_time=broadcast_time,
                context=context
            )
            
            # 3. ì„ íƒëœ í•­ëª© ì¶”ì¶œ (íƒ€ì‚¬ í¸ì„± ë¨¼ì €, ë„¤ì´ë²„ ë‚˜ì¤‘)
            result = []
            
            # íƒ€ì‚¬ ì„ íƒ í•­ëª© (ìš°ì„  ë°°ì¹˜)
            for idx in selected_indices.get("competitor_indices", []):
                if 0 <= idx < len(competitor_products):
                    result.append(competitor_products[idx])
            
            # ë„¤ì´ë²„ ì„ íƒ í•­ëª© (ë’¤ì— ë°°ì¹˜)
            for idx in selected_indices.get("naver_indices", []):
                if 0 <= idx < len(naver_as_competitor):
                    result.append(naver_as_competitor[idx])
            
            logger.info(f"âœ… AI ì„ íƒ ì™„ë£Œ: ë„¤ì´ë²„ {len(selected_indices.get('naver_indices', []))}ê°œ + íƒ€ì‚¬ {len(selected_indices.get('competitor_indices', []))}ê°œ = ì´ {len(result)}ê°œ")
            
            return result[:10]  # ìµœëŒ€ 10ê°œ
            
        except Exception as e:
            logger.error(f"âš ï¸ AI ì„ íƒ ì‹¤íŒ¨, í´ë°± ë¡œì§ ì‚¬ìš©: {str(e)}")
            # í´ë°±: ë„¤ì´ë²„ 5ê°œ + íƒ€ì‚¬ 5ê°œ ë‹¨ìˆœ ì„ íƒ
            return self._fallback_select_top_10(naver_products, competitor_products)
    
    def _convert_naver_to_competitor(self, naver: NaverProduct, index: int) -> CompetitorProduct:
        """ë„¤ì´ë²„ ìƒí’ˆì„ íƒ€ì‚¬ í¸ì„± í˜•ì‹(CompetitorProduct)ìœ¼ë¡œ ë³€í™˜"""
        return CompetitorProduct(
            company_name="ë„¤ì´ë²„ ìŠ¤í† ì–´",
            broadcast_title=f"[ë„¤ì´ë²„ ì¸ê¸° {index + 1}ìœ„] {naver.name[:50]}",
            start_time="",  # ë¹ˆì¹¸
            end_time="",    # ë¹ˆì¹¸
            duration_minutes=None,
            category_main=""  # ë„¤ì´ë²„ ìƒí’ˆì—ëŠ” ì¹´í…Œê³ ë¦¬ ì •ë³´ ì—†ìŒ
        )
    
    async def _ai_select_top_10(
        self,
        naver_products: List[NaverProduct],
        competitor_products: List[CompetitorProduct],
        broadcast_time: str,
        context: Dict[str, Any] = None
    ) -> Dict[str, List[int]]:
        """
        AIë¥¼ í™œìš©í•˜ì—¬ ë„¤ì´ë²„/íƒ€ì‚¬ í¸ì„± ì¤‘ 10ê°œì˜ ì¸ë±ìŠ¤ë¥¼ ì„ íƒ
        """
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ í™ˆì‡¼í•‘ ë°©ì†¡ í¸ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

# ë°ì´í„° ì´í•´
- **ë„¤ì´ë²„ ì¸ê¸° ìƒí’ˆ**: í˜„ì¬ ì‹œì ì˜ ì‹œì¥ íŠ¸ë Œë“œë¥¼ ë°˜ì˜í•œ ì‹¤ì‹œê°„ ë² ìŠ¤íŠ¸ ìƒí’ˆ (ì‹œê°„ ë¬´ê´€)
- **íƒ€ì‚¬ í™ˆì‡¼í•‘ í¸ì„±**: íŠ¹ì • ë°©ì†¡ ì‹œê°„ëŒ€ì˜ ì‹¤ì œ í¸ì„± ì •ë³´ (ì‹œê°„ ê¸°ë°˜)

# ì„ íƒ ê¸°ì¤€
1. **ë¹„ìœ¨**: ë„¤ì´ë²„:íƒ€ì‚¬ = 5:5ë¥¼ ìµœëŒ€í•œ ìœ ì§€ (í•œìª½ ë¶€ì¡± ì‹œ ë‹¤ë¥¸ìª½ìœ¼ë¡œ ì±„ì›€)
2. **ì‹œê°„ ì í•©ì„±**: ìš”ì²­ëœ ë°©ì†¡ ì‹œê°„ëŒ€ì— ì í•©í•œ ìƒí’ˆ/í¸ì„± ì„ íƒ
3. **íŠ¸ë Œë“œ ë°˜ì˜**: ë„¤ì´ë²„ ì¸ê¸° ìƒí’ˆì„ í†µí•´ í˜„ì¬ ì‹œì¥ íŠ¸ë Œë“œ íŒŒì•…
4. **ì¹´í…Œê³ ë¦¬ ê· í˜•**: ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬ë¡œ ì‹œì²­ì ì„ íƒí­ í™•ëŒ€
5. **ê²½ìŸ ë¶„ì„**: íƒ€ì‚¬ í¸ì„±ì„ ì°¸ê³ í•˜ì—¬ ì°¨ë³„í™” ë˜ëŠ” ë²¤ì¹˜ë§ˆí‚¹

# ì„ íƒ ì „ëµ
- ë„¤ì´ë²„ ì¸ê¸° ìƒí’ˆ ì¤‘ ë°©ì†¡ ì‹œê°„ëŒ€ì™€ ì–´ìš¸ë¦¬ëŠ” íŠ¸ë Œë“œ ìƒí’ˆ ì„ íƒ
- íƒ€ì‚¬ í¸ì„± ì¤‘ í•´ë‹¹ ì‹œê°„ëŒ€ì— ê²€ì¦ëœ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ ì°¸ê³ 
- í˜„ì¬ íŠ¸ë Œë“œ(ë„¤ì´ë²„)ì™€ ì‹¤ì œ í¸ì„±(íƒ€ì‚¬)ì˜ ê· í˜• ìœ ì§€

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
  "naver_indices": [ì¸ë±ìŠ¤ ë°°ì—´],
  "competitor_indices": [ì¸ë±ìŠ¤ ë°°ì—´],
  "selection_summary": {{
    "time_match": "ì‹œê°„ëŒ€ ì í•©ì„± íŒë‹¨",
    "diversity": "ì„ íƒí•œ ìƒí’ˆë“¤ì˜ ë‹¤ì–‘ì„± ì„¤ëª…",
    "trend_analysis": "íŠ¸ë Œë“œ ë°˜ì˜ ë°©ì‹"
  }},
  "selection_reason": "ì „ì²´ ì„ íƒ ê·¼ê±° 2-3ë¬¸ì¥"
}}"""),
            ("user", """ë°©ì†¡ ì‹œê°„: {broadcast_time}

ë„¤ì´ë²„ ì¸ê¸° ìƒí’ˆ ({naver_count}ê°œ):
{naver_summary}

íƒ€ì‚¬ í™ˆì‡¼í•‘ í¸ì„± ({competitor_count}ê°œ):
{competitor_summary}

ìœ„ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ë°©ì†¡ ì‹œê°„({broadcast_time})ì— ìµœì í™”ëœ 10ê°œë¥¼ ì„ íƒí•˜ì„¸ìš”.""")
        ])
        
        # ë„¤ì´ë²„ ìƒí’ˆ ìš”ì•½
        naver_summary = "\n".join([
            f"[{i}] {p.name[:40]} | ê°€ê²©: {p.sale_price:,}ì› | í• ì¸: {p.discount_ratio}% | íŒë§¤ëŸ‰: {p.cumulation_sale_count}"
            for i, p in enumerate(naver_products[:20])  # ìµœëŒ€ 20ê°œë§Œ ì „ë‹¬
        ])
        
        # íƒ€ì‚¬ í¸ì„± ìš”ì•½
        competitor_summary = "\n".join([
            f"[{i}] {c.company_name} | {c.broadcast_title[:40]} | {c.start_time} ~ {c.end_time} | {c.category_main or 'ë¯¸ë¶„ë¥˜'}"
            for i, c in enumerate(competitor_products[:20])  # ìµœëŒ€ 20ê°œë§Œ ì „ë‹¬
        ])
        
        # LLM í˜¸ì¶œ
        chain = prompt_template | self.llm | JsonOutputParser()
        
        result = await chain.ainvoke({
            "broadcast_time": broadcast_time or "ë¯¸ì§€ì •",
            "naver_count": len(naver_products),
            "competitor_count": len(competitor_products),
            "naver_summary": naver_summary or "ì—†ìŒ",
            "competitor_summary": competitor_summary or "ì—†ìŒ"
        })
        
        logger.info(f"AI ì„ íƒ ê·¼ê±°: {result.get('selection_reason', 'ì—†ìŒ')}")
        
        return result
    
    def _fallback_select_top_10(
        self,
        naver_products: List[NaverProduct],
        competitor_products: List[CompetitorProduct]
    ) -> List[CompetitorProduct]:
        """AI ì‹¤íŒ¨ ì‹œ í´ë°±: ë‹¨ìˆœ 5:5 ì„ íƒ (íƒ€ì‚¬ ë¨¼ì €, ë„¤ì´ë²„ ë‚˜ì¤‘)"""
        result = []
        
        # íƒ€ì‚¬ 5ê°œ (ë˜ëŠ” ê°€ëŠ¥í•œ ë§Œí¼) - ìš°ì„  ë°°ì¹˜
        competitor_count = min(5, len(competitor_products))
        for i in range(competitor_count):
            result.append(competitor_products[i])
        
        # ë„¤ì´ë²„ 5ê°œ (ë˜ëŠ” ê°€ëŠ¥í•œ ë§Œí¼) - ë’¤ì— ë°°ì¹˜
        naver_count = min(5, len(naver_products))
        for i in range(naver_count):
            result.append(self._convert_naver_to_competitor(naver_products[i], i))
        
        # 10ê°œ ë¯¸ë§Œì´ë©´ ë‚˜ë¨¸ì§€ë¡œ ì±„ì›€
        if len(result) < 10:
            remaining = 10 - len(result)
            if competitor_count < len(competitor_products):
                for i in range(competitor_count, min(competitor_count + remaining, len(competitor_products))):
                    result.append(competitor_products[i])
            elif naver_count < len(naver_products):
                for i in range(naver_count, min(naver_count + remaining, len(naver_products))):
                    result.append(self._convert_naver_to_competitor(naver_products[i], i))
        
        logger.info(f"í´ë°± ì„ íƒ: íƒ€ì‚¬ ìš°ì„ , ì´ {len(result)}ê°œ")
        return result[:10]
    
    async def _generate_batch_reasons_with_langchain(self, candidates: List[Dict[str, Any]], context: Dict[str, Any] = None) -> List[str]:
        """ë°°ì¹˜ë¡œ ì—¬ëŸ¬ ìƒí’ˆì˜ ì¶”ì²œ ê·¼ê±°ë¥¼ í•œ ë²ˆì— ìƒì„± (ì†ë„ ê°œì„ )"""
        try:
            # ì»¨í…ìŠ¤íŠ¸ ì •ë³´
            time_slot = context.get("time_slot", "") if context else ""
            weather = context.get("weather", {}).get("weather", "") if context else ""
            holiday_name = context.get("holiday_name") if context else None
            
            # í‚¤ì›Œë“œ ë§¤í•‘ ì •ë³´ (í™•ì¥ëœ í‚¤ì›Œë“œ â†’ ì›ë³¸ í‚¤ì›Œë“œ)
            keyword_mapping = context.get("keyword_mapping", {}) if context else {}
            original_keywords = context.get("original_keywords", []) if context else []
            
            # ìƒí’ˆ ì •ë³´ ìš”ì•½
            products_summary = []
            for candidate in candidates:
                product = candidate["product"]
                rank = candidate.get("rank", 0)
                predicted_sales = candidate.get("predicted_sales", 0)
                similarity_score = candidate.get("similarity_score", 0)
                final_score = candidate.get("final_score", 0)
                trend_keyword = candidate.get("trend_keyword", "")
                
                # íŠ¸ë Œë“œ í‚¤ì›Œë“œì˜ ì›ë³¸ í‚¤ì›Œë“œ ì°¾ê¸°
                original_keyword = keyword_mapping.get(trend_keyword, trend_keyword) if trend_keyword else ""
                
                products_summary.append({
                    "rank": rank,
                    "product_name": product.get("product_name", ""),
                    "category": product.get("category_main", ""),
                    "predicted_sales": int(predicted_sales/10000) if predicted_sales else 0,
                    "similarity_score": f"{similarity_score:.3f}",
                    "final_score": f"{final_score:.3f}",
                    "trend_keyword": trend_keyword,
                    "original_keyword": original_keyword  # ì›ë³¸ í‚¤ì›Œë“œ ì¶”ê°€
                })
            
            # ë°°ì¹˜ í”„ë¡¬í”„íŠ¸ ìƒì„±
            batch_prompt = ChatPromptTemplate.from_messages([
                ("system", """ë‹¹ì‹ ì€ í™ˆì‡¼í•‘ ë°©ì†¡ í¸ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì—¬ëŸ¬ ìƒí’ˆì˜ ì¶”ì²œ ê·¼ê±°ë¥¼ í•œ ë²ˆì— ì‘ì„±í•˜ì„¸ìš”.

# í•µì‹¬ ì›ì¹™
1. **ê° ìƒí’ˆë§ˆë‹¤ 100ì ì´ë‚´** ê°„ê²°í•˜ê²Œ ì‘ì„±
2. ì „ë¬¸ì ì´ê³  ê°ê´€ì ì¸ í†¤ ìœ ì§€
3. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ë°ì´í„° í™œìš©
4. **ê° ìƒí’ˆë§ˆë‹¤ ì™„ì „íˆ ë‹¤ë¥¸ ê´€ì ê³¼ í‘œí˜„ ì‚¬ìš©**
5. ê°™ì€ íŒ¨í„´ì´ë‚˜ ë¬¸êµ¬ ë°˜ë³µ ì ˆëŒ€ ê¸ˆì§€

# í™œìš© ê°€ëŠ¥í•œ ìš”ì†Œë“¤
- ì˜ˆì¸¡ ë§¤ì¶œ ìˆ˜ì¹˜ (í•„ìˆ˜)
- ì¹´í…Œê³ ë¦¬ íŠ¹ì„± (í•„ìˆ˜)
- **ì›ë³¸ í‚¤ì›Œë“œ** (ë§¤ìš° ì¤‘ìš”! ìˆì„ ê²½ìš° ë°˜ë“œì‹œ í™œìš©)
  * ì˜ˆ: ìƒí’ˆì´ "ì´ˆì½œë¦¿"ì´ê³  ì›ë³¸ í‚¤ì›Œë“œê°€ "ìˆ˜ëŠ¥ ê°„ì‹"ì´ë©´
    â†’ "ìˆ˜ëŠ¥ ê°„ì‹ìœ¼ë¡œ ì í•©í•œ ì´ˆì½œë¦¿"ì²˜ëŸ¼ í‘œí˜„
  * ì˜ˆ: ìƒí’ˆì´ "íŒ¨ë”©"ì´ê³  ì›ë³¸ í‚¤ì›Œë“œê°€ "ê²¨ìš¸ íŒ¨ì…˜"ì´ë©´
    â†’ "ê²¨ìš¸ íŒ¨ì…˜ íŠ¸ë Œë“œì— ë§ëŠ” íŒ¨ë”©"ì²˜ëŸ¼ í‘œí˜„
- ê³µíœ´ì¼ (ìˆì„ ê²½ìš° í•„ìˆ˜ ì–¸ê¸‰)
- ì‹œê°„ëŒ€ íŠ¹ì„± (ì €ë…/ì˜¤ì „/ì˜¤í›„) - ì‹ ì¤‘í•˜ê²Œ íŒë‹¨
- ë‚ ì”¨/ê³„ì ˆ (ì„ íƒì )

# ê¸ˆì§€ ì‚¬í•­
- "AI ë¶„ì„ ê²°ê³¼"ë¡œ ì‹œì‘í•˜ì§€ ë§ˆì„¸ìš”
- í…œí”Œë¦¿ì²˜ëŸ¼ ë³´ì´ëŠ” ë°˜ë³µì  í‘œí˜„ ê¸ˆì§€
- ê³¼ì¥ëœ í‘œí˜„ ê¸ˆì§€
- ê¸°ìˆ  ìš©ì–´ ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€ (ìœ ì‚¬ë„, ì ìˆ˜, ë¹„ìœ¨ ë“±)

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
  "reasons": [
    "1ë²ˆ ìƒí’ˆ ì¶”ì²œ ê·¼ê±°",
    "2ë²ˆ ìƒí’ˆ ì¶”ì²œ ê·¼ê±°",
    ...
  ]
}}""")
,
                ("human", """ì‹œê°„ëŒ€: {time_slot}
ë‚ ì”¨: {weather}
ê³µíœ´ì¼: {holiday_name}

ì¶”ì²œ ìƒí’ˆ ëª©ë¡:
{products_info}

ìœ„ {count}ê°œ ìƒí’ˆ ê°ê°ì— ëŒ€í•´ ë…ì°½ì ì¸ ì¶”ì²œ ê·¼ê±°ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
**ì›ë³¸ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ í™œìš©í•˜ì„¸ìš”!**""")
            ])
            
            # ìƒí’ˆ ì •ë³´ í¬ë§·íŒ… (ì›ë³¸ í‚¤ì›Œë“œ í¬í•¨)
            products_info = "\n".join([
                f"{p['rank']}. {p['product_name'][:40]} | ì¹´í…Œê³ ë¦¬: {p['category']} | ì˜ˆì¸¡ë§¤ì¶œ: {p['predicted_sales']}ë§Œì› | ì›ë³¸í‚¤ì›Œë“œ: {p['original_keyword'] or 'ì—†ìŒ'}"
                for p in products_summary
            ])
            
            chain = batch_prompt | self.llm | JsonOutputParser()
            
            result = await chain.ainvoke({
                "time_slot": time_slot or "ë¯¸ì§€ì •",
                "weather": weather or "ë³´í†µ",
                "holiday_name": holiday_name if holiday_name else "ì—†ìŒ",
                "products_info": products_info,
                "count": len(candidates)
            })
            
            reasons = result.get("reasons", [])
            print(f"âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(reasons)}ê°œ ê·¼ê±° ìƒì„±")
            
            # ê°œìˆ˜ê°€ ë¶€ì¡±í•˜ë©´ ê¸°ë³¸ ë©”ì‹œì§€ë¡œ ì±„ì›€
            while len(reasons) < len(candidates):
                idx = len(reasons)
                reasons.append(f"{candidates[idx]['product'].get('category_main', 'ìƒí’ˆ')} ì¶”ì²œ")
            
            return reasons[:len(candidates)]
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ê·¼ê±° ìƒì„± ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            # í´ë°±: ê¸°ë³¸ ë©”ì‹œì§€
            print("âš ï¸ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨, ê¸°ë³¸ ë©”ì‹œì§€ë¡œ í´ë°±...")
            return [f"{c['product'].get('category_main', 'ìƒí’ˆ')} ì¶”ì²œ" for c in candidates]
    
    def _prepare_features_for_product(self, product: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """1ê°œ ìƒí’ˆì˜ XGBoost feature ì¤€ë¹„ (ì˜ˆì¸¡ì€ ì•ˆ í•¨)"""
        broadcast_dt = context["broadcast_dt"]
        
        print(f"=== [_prepare_features_for_product] í˜¸ì¶œë¨: {product.get('product_name', 'Unknown')[:30]} ===")
        
        # ë¡œê·¸ ìŠ¤ì¼€ì¼ë§ ì ìš© (í•™ìŠµ ì‹œì™€ ë™ì¼)
        product_price = product.get("product_price", product.get("price", 100000))
        product_price_log = np.log1p(product_price)
        
        category_main = product.get("category_main", product.get("category", "Unknown"))
        time_slot = context["time_slot"]
        
        return {
            # Numeric features (ë‹¨ìˆœí™”)
            "product_price_log": product_price_log,
            "hour": broadcast_dt.hour,
            "temperature": context["weather"].get("temperature", 20),
            "precipitation": context["weather"].get("precipitation", 0),
            
            # Categorical features
            "product_lgroup": category_main,
            "product_mgroup": product.get("category_middle", "Unknown"),
            "product_sgroup": product.get("category_sub", "Unknown"),
            "brand": product.get("brand", "Unknown"),
            "product_type": product.get("product_type", "ìœ í˜•"),
            "time_slot": time_slot,
            "day_of_week": ["ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ", "í† ", "ì¼"][broadcast_dt.weekday()],
            "season": context["season"],
            "weather": context["weather"].get("weather", "Clear"),
            
            # Boolean features
            "is_weekend": 1 if broadcast_dt.weekday() >= 5 else 0,
            "is_holiday": 0
        }
    
    async def _predict_product_sales(self, product: Dict[str, Any], context: Dict[str, Any]) -> float:
        """ê°œë³„ ìƒí’ˆ XGBoost ë§¤ì¶œ ì˜ˆì¸¡"""
        try:
            import pandas as pd
            
            # Feature ì¤€ë¹„
            features = self._prepare_features_for_product(product, context)
            product_data = pd.DataFrame([features])
            
            logger.info(f"=== XGBoost ë§¤ì¶œ ì˜ˆì¸¡ ì…ë ¥ ë°ì´í„° ===")
            logger.info(f"ìƒí’ˆ: {product.get('product_name', 'Unknown')}")
            logger.info(f"ì¹´í…Œê³ ë¦¬: {product.get('category_main', 'Unknown')}")
            logger.info(f"ê°€ê²©: {product.get('product_price', 100000):,}ì›")
            logger.info(f"ê³¼ê±° í‰ê·  ë§¤ì¶œ: {product.get('avg_sales', 30000000):,}ì›")
            logger.info(f"ë°©ì†¡ ì‹œê°„: {context['broadcast_dt'].hour}ì‹œ")
            logger.info(f"ë‚ ì”¨: {context['weather'].get('weather', 'Clear')}, {context['weather'].get('temperature', 20)}Â°C")
            
            # XGBoost íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì˜ˆì¸¡ (ì „ì²˜ë¦¬ í¬í•¨)
            predicted_sales_log = self.model.predict(product_data)[0]
            # ë¡œê·¸ ì—­ë³€í™˜ (í•™ìŠµ ì‹œ log1p ì‚¬ìš©)
            predicted_sales = np.expm1(predicted_sales_log)
            logger.info(f"=== XGBoost ì˜ˆì¸¡ ê²°ê³¼ ===")
            logger.info(f"ì˜ˆì¸¡ ë§¤ì¶œ: {predicted_sales:,.0f}ì› ({predicted_sales/100000000:.2f}ì–µ)")
            
            return float(predicted_sales)
            
        except Exception as e:
            logger.error(f"ìƒí’ˆ ë§¤ì¶œ ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
            logger.error(f"ìƒí’ˆ ì •ë³´: {product.get('product_name', 'Unknown')}")
            import traceback
            logger.error(f"ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
            return 30000000  # ê¸°ë³¸ê°’ (0.3ì–µ)
    
    async def _predict_products_sales_batch(self, products: List[Dict[str, Any]], context: Dict[str, Any]) -> List[float]:
        """ì—¬ëŸ¬ ìƒí’ˆ XGBoost ë§¤ì¶œ ì˜ˆì¸¡ (ë°°ì¹˜ ì²˜ë¦¬)"""
        try:
            import pandas as pd
            
            if not products:
                return []
            
            # ëª¨ë“  ìƒí’ˆì˜ featuresë¥¼ í•œ ë²ˆì— ì¤€ë¹„
            features_list = [
                self._prepare_features_for_product(product, context)
                for product in products
            ]
            
            batch_df = pd.DataFrame(features_list)
            
            print(f"=== [ë°°ì¹˜ ì˜ˆì¸¡] {len(products)}ê°œ ìƒí’ˆ ì¼ê´„ ì˜ˆì¸¡ ì‹œì‘ ===")
            
            # ì…ë ¥ í”¼ì²˜ ìƒ˜í”Œ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
            print(f"=== [ì…ë ¥ í”¼ì²˜ ìƒ˜í”Œ] ===")
            for i, (product, features) in enumerate(zip(products[:3], features_list[:3])):
                print(f"  ìƒí’ˆ {i+1}: {product.get('product_name', '')[:30]}")
                print(f"    - product_price_log: {features['product_price_log']:.2f}")
                print(f"    - hour: {features['hour']}")
                print(f"    - ì¹´í…Œê³ ë¦¬: {features['product_lgroup']}")
            
            # XGBoost ë°°ì¹˜ ì˜ˆì¸¡ (í•œ ë²ˆì— ì²˜ë¦¬)
            predicted_sales_log = self.model.predict(batch_df)
            # ë¡œê·¸ ì—­ë³€í™˜ (í•™ìŠµ ì‹œ log1p ì‚¬ìš©)
            predicted_sales_array = np.expm1(predicted_sales_log)
            
            print(f"=== [ë°°ì¹˜ ì˜ˆì¸¡] ì™„ë£Œ ===")
            print(f"  í‰ê· : {predicted_sales_array.mean()/10000:.0f}ë§Œì›")
            print(f"  ìµœì†Œ: {predicted_sales_array.min()/10000:.0f}ë§Œì›")
            print(f"  ìµœëŒ€: {predicted_sales_array.max()/10000:.0f}ë§Œì›")
            print(f"  í‘œì¤€í¸ì°¨: {predicted_sales_array.std()/10000:.0f}ë§Œì›")
            
            # ì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ ì¶œë ¥
            print(f"=== [ì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ] ===")
            for i, (product, sales) in enumerate(zip(products[:5], predicted_sales_array[:5])):
                print(f"  {i+1}. {product.get('product_name', '')[:30]:30s} â†’ {sales/10000:.0f}ë§Œì›")
            
            return [float(sales) for sales in predicted_sales_array]
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ë§¤ì¶œ ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(f"ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return [30000000.0] * len(products)
    
    async def _get_all_categories_from_db(self) -> List[str]:
        """PostgreSQLì—ì„œ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì¡°íšŒ"""
        try:
            query = text("""
                SELECT DISTINCT category_main
                FROM broadcast_training_dataset
                WHERE category_main IS NOT NULL
                ORDER BY category_main
            """)
            
            with self.engine.connect() as conn:
                result = conn.execute(query).fetchall()
            
            categories = [row[0] for row in result]
            return categories
            
        except Exception as e:
            logger.error(f"ì „ì²´ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    async def _get_ace_products_from_category(self, category: str, limit: int = 5) -> List[Dict[str, Any]]:
        """ì¹´í…Œê³ ë¦¬ë³„ ì—ì´ìŠ¤ ìƒí’ˆ ì¡°íšŒ"""
        try:
            query = text("""
                SELECT product_code, product_name, category_main, category_middle, category_sub,
                       AVG(gross_profit) as avg_sales, COUNT(*) as broadcast_count,
                       tape_code, tape_name, MAX(price) as price, brand
                FROM broadcast_training_dataset 
                WHERE category_main = :category
                GROUP BY product_code, product_name, category_main, category_middle, category_sub,
                         tape_code, tape_name, brand
                ORDER BY avg_sales DESC 
                LIMIT :limit
            """)
            
            with self.engine.connect() as conn:
                result = conn.execute(query, {"category": category, "limit": limit}).fetchall()
                
            products = []
            for row in result:
                products.append({
                    "product_code": row[0],
                    "product_name": row[1],
                    "category_main": row[2],
                    "category_middle": row[3],
                    "category_sub": row[4],
                    "avg_sales": float(row[5]),
                    "broadcast_count": int(row[6]),
                    "tape_code": row[7],
                    "tape_name": row[8],
                    "price": float(row[9]) if row[9] else None,
                    "brand": row[10] if len(row) > 10 else None
                })
            
            return products
            
        except Exception as e:
            logger.error(f"ì—ì´ìŠ¤ ìƒí’ˆ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    def _get_time_slot(self, dt: datetime) -> str:
        """ì‹œê°„ëŒ€ ë¶„ë¥˜"""
        hour = dt.hour
        if 6 <= hour < 9:
            return "ì•„ì¹¨"
        elif 9 <= hour < 12:
            return "ì˜¤ì „"
        elif 12 <= hour < 14:
            return "ì ì‹¬"
        elif 14 <= hour < 18:
            return "ì˜¤í›„"
        elif 18 <= hour < 22:
            return "ì €ë…"
        else:
            return "ì•¼ê°„"
    
    def _get_season(self, month: int) -> str:
        """ê³„ì ˆ ë¶„ë¥˜"""
        if 3 <= month <= 5:
            return "ë´„"
        elif 6 <= month <= 8:
            return "ì—¬ë¦„"
        elif 9 <= month <= 11:
            return "ê°€ì„"
        else:
            return "ê²¨ìš¸"
