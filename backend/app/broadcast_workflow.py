"""
ë°©ì†¡ í¸ì„± AI ì¶”ì²œ ì›Œí¬í”Œë¡œìš°
LangChain ê¸°ë°˜ 2ë‹¨ê³„ ì›Œí¬í”Œë¡œìš°: AI ë°©í–¥ íƒìƒ‰ + ê³ ì† ë­í‚¹
"""

import asyncio
import json
import logging
import time
from datetime import datetime, timedelta
from typing import List, Dict, Any, Optional, Tuple
import pandas as pd
import numpy as np
from sqlalchemy import create_engine, text
import os

from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import JsonOutputParser
from langchain_core.prompts import ChatPromptTemplate
from .external_apis import ExternalAPIManager

from .dependencies import get_product_embedder
from . import broadcast_recommender as br
from .schemas import BroadcastResponse, BroadcastRecommendation, ProductInfo, BusinessMetrics, NaverProduct, CompetitorProduct, LastBroadcastMetrics
from .external_products_service import ExternalProductsService
from .services.broadcast_history_service import BroadcastHistoryService
from .netezza_config import netezza_conn

logger = logging.getLogger(__name__)

class BroadcastWorkflow:
    """ë°©ì†¡ í¸ì„± AI ì¶”ì²œ ì›Œí¬í”Œë¡œìš°"""
    
    def __init__(self, model):
        self.model = model  # XGBoost ëª¨ë¸
        self.product_embedder = get_product_embedder()
        
        # AI íŠ¸ë Œë“œ ìºì‹œ (ì‹œê°„ëŒ€ë³„)
        self._ai_trends_cache = {}
        self._cache_ttl = 3600  # 1ì‹œê°„ (ì´ˆ)
        
        # LangChain LLM ì´ˆê¸°í™”
        self.llm = ChatOpenAI(
            model="gpt-4o-mini",
            temperature=0.5,
            openai_api_key=os.getenv("OPENAI_API_KEY")
        )
        
        # DB ì—°ê²°
        self.engine = create_engine(os.getenv("POSTGRES_URI"))
        
        # ì™¸ë¶€ ìƒí’ˆ ì„œë¹„ìŠ¤
        self.external_products_service = ExternalProductsService()
        
        # ë°©ì†¡ ì´ë ¥ ì„œë¹„ìŠ¤ (Netezza)
        self.broadcast_history_service = BroadcastHistoryService()
    
    async def process_broadcast_recommendation(
        self, 
        broadcast_time: str, 
        recommendation_count: int = 5,
        trend_weight: float = 0.3,  # íŠ¸ë Œë“œ ê°€ì¤‘ì¹˜ (0.3 = 30%)
        selling_weight: float = 0.7   # ë§¤ì¶œ ì˜ˆì¸¡ ê°€ì¤‘ì¹˜ (0.7 = 70%)
    ) -> BroadcastResponse:
        """ë©”ì¸ ì›Œí¬í”Œë¡œìš°: ë°©ì†¡ ì‹œê°„ ê¸°ë°˜ ì¶”ì²œ
        
        Args:
            broadcast_time: ë°©ì†¡ ì‹œê°„
            recommendation_count: ì¶”ì²œ ê°œìˆ˜
            trend_weight: íŠ¸ë Œë“œ ê°€ì¤‘ì¹˜ (0.0~1.0, ê¸°ë³¸ 0.3)
            selling_weight: ë§¤ì¶œ ì˜ˆì¸¡ ê°€ì¤‘ì¹˜ (0.0~1.0, ê¸°ë³¸ 0.7)
                - ì˜ˆ: trend_weight=0.3, selling_weight=0.7 â†’ íŠ¸ë Œë“œ 30%, ë§¤ì¶œ 70%
                - ì˜ˆ: trend_weight=0.5, selling_weight=0.5 â†’ ê· í˜• (50:50)
        """
        
        import time
        workflow_start = time.time()
        
        print("=== [DEBUG] process_broadcast_recommendation ì‹œì‘ ===")
        request_time = datetime.now().isoformat()
        logger.info(f"ë°©ì†¡ ì¶”ì²œ ì›Œí¬í”Œë¡œìš° ì‹œì‘: {broadcast_time}")
        print(f"=== [DEBUG] broadcast_time: {broadcast_time}, recommendation_count: {recommendation_count} ===")
        
        try:
            # 1ë‹¨ê³„: ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ë° í†µí•© í‚¤ì›Œë“œ ìƒì„±
            step_start = time.time()
            print("=== [DEBUG] _collect_context_and_keywords í˜¸ì¶œ ===")
            context = await self._collect_context_and_keywords(broadcast_time)
            print(f"â±ï¸  [1ë‹¨ê³„] ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘: {time.time() - step_start:.2f}ì´ˆ")
            print(f"=== [DEBUG] í†µí•© í‚¤ì›Œë“œ: {len(context.get('unified_keywords', []))}ê°œ ===")
            
            # 2. í†µí•© ê²€ìƒ‰ ì‹¤í–‰ (1íšŒ)
            step_start = time.time()
            print("=== [DEBUG] _execute_unified_search í˜¸ì¶œ ===")
            search_result = await self._execute_unified_search(context, context.get("unified_keywords", []))
            print(f"â±ï¸  [2ë‹¨ê³„] í†µí•© ê²€ìƒ‰: {time.time() - step_start:.2f}ì´ˆ")
            print(f"=== [DEBUG] ê²€ìƒ‰ ì™„ë£Œ - ì§ì ‘ë§¤ì¹­: {len(search_result['direct_products'])}ê°œ, ì¹´í…Œê³ ë¦¬: {len(search_result['category_groups'])}ê°œ ===")
            
            # ê²€ìƒ‰ì— ì‚¬ìš©ëœ í‚¤ì›Œë“œë¥¼ contextì— ì €ì¥
            context["search_keywords"] = search_result.get("search_keywords", [])
            
            # 3. í›„ë³´êµ° ìƒì„± (ê°€ì¤‘ì¹˜ ê¸°ë°˜ ë¹„ìœ¨ ì¡°ì •)
            step_start = time.time()
            print("=== [DEBUG] _generate_unified_candidates í˜¸ì¶œ ===")
            max_trend = max(1, int(recommendation_count * trend_weight))  # ìµœì†Œ 1ê°œ
            max_sales = recommendation_count - max_trend + 3  # ì—¬ìœ ë¶„ ì¶”ê°€
            print(f"=== [DEBUG] ê°€ì¤‘ì¹˜ ì ìš©: íŠ¸ë Œë“œ {max_trend}ê°œ ({trend_weight:.0%}), ë§¤ì¶œ {max_sales}ê°œ ({selling_weight:.0%}) ===")
            
            candidate_products, category_scores = await self._generate_unified_candidates(
                search_result,
                context,
                max_trend_match=max_trend,
                max_sales_prediction=max_sales
            )
            print(f"â±ï¸  [3ë‹¨ê³„] í›„ë³´êµ° ìƒì„±: {time.time() - step_start:.2f}ì´ˆ")
            print(f"=== [DEBUG] í›„ë³´êµ° ìƒì„± ì™„ë£Œ: {len(candidate_products)}ê°œ ===")
            
            # 4. ìµœì¢… ë­í‚¹ ê³„ì‚°
            step_start = time.time()
            ranked_products = await self._rank_final_candidates(
                candidate_products,
                category_scores=category_scores,
                context=context
            )
            print(f"â±ï¸  [4ë‹¨ê³„] ìµœì¢… ë­í‚¹: {time.time() - step_start:.2f}ì´ˆ")
            
            # 5. API ì‘ë‹µ ìƒì„±
            step_start = time.time()
            response = await self._format_response(ranked_products[:recommendation_count], context)
            response.requestTime = request_time
            step_time = time.time() - step_start
            print(f"â±ï¸  [5ë‹¨ê³„] ì‘ë‹µ ìƒì„± ì´: {step_time:.2f}ì´ˆ")
            
            total_time = time.time() - workflow_start
            print(f"â±ï¸  ===== ì›Œí¬í”Œë¡œìš° ì´ ì‹œê°„: {total_time:.2f}ì´ˆ =====")
            
            logger.info(f"ë°©ì†¡ ì¶”ì²œ ì™„ë£Œ: {len(ranked_products)}ê°œ ì¶”ì²œ")
            return response
            
        except Exception as e:
            print(f"=== [DEBUG] ì˜ˆì™¸ ë°œìƒ: {type(e).__name__}: {e} ===")
            import traceback
            traceback.print_exc()
            logger.error(f"ë°©ì†¡ ì¶”ì²œ ì›Œí¬í”Œë¡œìš° ì˜¤ë¥˜: {e}")
            # OpenAI API ê´€ë ¨ ì˜¤ë¥˜ëŠ” ìƒìœ„ë¡œ ì „íŒŒ (503 ì—ëŸ¬ ë°˜í™˜ìš©)
            if "AI ì„œë¹„ìŠ¤" in str(e) or "OpenAI" in str(e) or "í• ë‹¹ëŸ‰" in str(e):
                raise e
            # ê¸°íƒ€ ë‚´ë¶€ ì˜¤ë¥˜ëŠ” 500 ì—ëŸ¬ë¡œ ì²˜ë¦¬
            raise Exception(f"ë‚´ë¶€ ì„œë²„ ì˜¤ë¥˜: {e}")
    
    async def _collect_context_and_keywords(self, broadcast_time: str) -> Dict[str, Any]:
        """ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ë° í†µí•© í‚¤ì›Œë“œ ìƒì„± (ê°œì„ ëœ ë²„ì „)"""
        
        # ë°©ì†¡ ì‹œê°„ íŒŒì‹±
        broadcast_dt = datetime.fromisoformat(broadcast_time.replace('Z', '+00:00'))
        
        # DBì—ì„œ ê³µíœ´ì¼ ì •ë³´ ì¡°íšŒ
        holiday_name = await self._get_holiday_from_db(broadcast_dt.date())
        
        context = {
            "broadcast_time": broadcast_time,
            "broadcast_dt": broadcast_dt,
            "hour": broadcast_dt.hour,
            "weekday": broadcast_dt.weekday(),
            "season": self._get_season(broadcast_dt.month),
            "holiday_name": holiday_name  # ê³µíœ´ì¼ ì •ë³´ ì¶”ê°€
        }
        
        # ë‚ ì”¨ ì •ë³´ ìˆ˜ì§‘
        weather_info = br.get_weather_by_date(broadcast_dt.date())
        context["weather"] = weather_info

        # ì‹œê°„ëŒ€ ì •ë³´
        time_slot = self._get_time_slot(broadcast_dt)
        day_type = "ì£¼ë§" if broadcast_dt.weekday() >= 5 else "í‰ì¼"
        context["time_slot"] = time_slot
        context["day_type"] = day_type

        # AI ê¸°ë°˜ íŠ¸ë Œë“œ ìƒì„± (LLM API) - ìºì‹± ì ìš©
        cache_key = f"{broadcast_dt.hour}_{weather_info.get('weather', 'Clear')}"
        current_time = datetime.now().timestamp()
        
        # ìºì‹œ í™•ì¸
        if cache_key in self._ai_trends_cache:
            cached_data, cached_time = self._ai_trends_cache[cache_key]
            if current_time - cached_time < self._cache_ttl:
                context["ai_trends"] = cached_data
                logger.info(f"âœ… AI íŠ¸ë Œë“œ ìºì‹œ íˆíŠ¸ ({cache_key}): {len(cached_data)}ê°œ í‚¤ì›Œë“œ")
            else:
                # ìºì‹œ ë§Œë£Œ
                del self._ai_trends_cache[cache_key]
                logger.info(f"â° AI íŠ¸ë Œë“œ ìºì‹œ ë§Œë£Œ ({cache_key})")
                context["ai_trends"] = None
        else:
            context["ai_trends"] = None
        
        # ìºì‹œ ë¯¸ìŠ¤ ì‹œ API í˜¸ì¶œ
        if context["ai_trends"] is None:
            api_manager = ExternalAPIManager()
            if api_manager.llm_trend_api:
                try:
                    import time
                    api_start = time.time()
                    # ë°©ì†¡ ì‹œê°„ê³¼ ë‚ ì”¨ ì •ë³´ë¥¼ ì „ë‹¬í•˜ì—¬ ë§¥ë½ ê¸°ë°˜ íŠ¸ë Œë“œ ìƒì„±
                    llm_trends = await api_manager.llm_trend_api.get_trending_searches(
                        hour=broadcast_dt.hour,
                        weather_info=weather_info
                    )
                    api_time = time.time() - api_start
                    # AIê°€ ìƒì„±í•œ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì¶”ê°€
                    context["ai_trends"] = [t["keyword"] for t in llm_trends]
                    # ìºì‹œ ì €ì¥
                    self._ai_trends_cache[cache_key] = (context["ai_trends"], current_time)
                    logger.info(f"ğŸ”¥ AI íŠ¸ë Œë“œ ìƒì„± ì™„ë£Œ ({broadcast_dt.hour}ì‹œ, {weather_info.get('weather', 'N/A')}): {len(llm_trends)}ê°œ í‚¤ì›Œë“œ (ì†Œìš”: {api_time:.2f}ì´ˆ)")
                    logger.info(f"AI íŠ¸ë Œë“œ: {context['ai_trends'][:5]}...")  # ìƒìœ„ 5ê°œë§Œ ë¡œê·¸
                except Exception as e:
                    logger.error(f"AI íŠ¸ë Œë“œ ìƒì„± ì‹¤íŒ¨: {e}")
                    context["ai_trends"] = []
            else:
                logger.warning("OpenAI API í‚¤ ì—†ìŒ - AI íŠ¸ë Œë“œ ìƒì„± ê±´ë„ˆëœ€")
                context["ai_trends"] = []

        # ì»¨í…ìŠ¤íŠ¸ ë¡œê·¸ ì¶œë ¥
        logger.info(f"ì»¨í…ìŠ¤íŠ¸ ìˆ˜ì§‘ ì™„ë£Œ - ê³„ì ˆ: {context['season']}, ì‹œê°„ëŒ€: {time_slot}, ìš”ì¼: {day_type}")
        if holiday_name:
            logger.info(f"ğŸ‰ ê³µíœ´ì¼: {holiday_name}")
        logger.info(f"ë‚ ì”¨: {weather_info.get('weather', 'N/A')}")
        
        # í†µí•© í‚¤ì›Œë“œ ìƒì„± (ì»¨í…ìŠ¤íŠ¸ ìš°ì„ , AI íŠ¸ë Œë“œëŠ” ë³´ì¡°)
        unified_keywords = []
        
        # 1. ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í‚¤ì›Œë“œ ìƒì„± (ë‚ ì§œ/ì‹œê°„/ë‚ ì”¨ ê¸°ë°˜ - ìš°ì„ ìˆœìœ„ ë†’ìŒ)
        context_keywords = await self._generate_context_keywords(context)
        if context_keywords:
            unified_keywords.extend(context_keywords)
            logger.info(f"[ìš°ì„ ìˆœìœ„ 1] ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ {len(context_keywords)}ê°œ ì¶”ê°€")
        
        # 2. AI íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì¶”ê°€ (ì‹¤ì‹œê°„ íŠ¸ë Œë“œ - ë³´ì¡° ì—­í• , ê°œìˆ˜ ì œí•œ)
        if context.get("ai_trends"):
            ai_trend_limit = 3  # 10ê°œ â†’ 3ê°œë¡œ ì¶•ì†Œ
            unified_keywords.extend(context["ai_trends"][:ai_trend_limit])
            logger.info(f"[ìš°ì„ ìˆœìœ„ 2] AI íŠ¸ë Œë“œ í‚¤ì›Œë“œ {len(context['ai_trends'][:ai_trend_limit])}ê°œ ì¶”ê°€ (ë³´ì¡°)")
        
        # 3. ì¤‘ë³µ ì œê±° ë° ì €ì¥
        context["unified_keywords"] = list(dict.fromkeys(unified_keywords))  # ìˆœì„œ ìœ ì§€ ì¤‘ë³µ ì œê±°
        logger.info(f"í†µí•© í‚¤ì›Œë“œ ìƒì„± ì™„ë£Œ: ì´ {len(context['unified_keywords'])}ê°œ")
        logger.info(f"í†µí•© í‚¤ì›Œë“œ (ìš°ì„ ìˆœìœ„ìˆœ): {context['unified_keywords']}")

        return context
    
    async def _get_holiday_from_db(self, target_date) -> Optional[str]:
        """DBì—ì„œ ê³µíœ´ì¼ ì •ë³´ ì¡°íšŒ"""
        try:
            with self.engine.connect() as conn:
                query = text("""
                    SELECT holiday_name 
                    FROM TAIHOLIDAYS 
                    WHERE holiday_date = :target_date
                """)
                result = conn.execute(query, {"target_date": target_date})
                row = result.fetchone()
                
                if row:
                    holiday_name = row[0]
                    logger.info(f"ê³µíœ´ì¼ ì¡°íšŒ ì„±ê³µ: {target_date} -> {holiday_name}")
                    return holiday_name
                else:
                    return None
        except Exception as e:
            logger.error(f"ê³µíœ´ì¼ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return None
    
    def _get_season(self, month: int) -> str:
        """ê³„ì ˆ ì •ë³´ ë°˜í™˜"""
        if month in [12, 1, 2]:
            return "ê²¨ìš¸"
        elif month in [3, 4, 5]:
            return "ë´„"
        elif month in [6, 7, 8]:
            return "ì—¬ë¦„"
        else:
            return "ê°€ì„"
    
    def _get_time_slot(self, dt: datetime) -> str:
        """ì‹œê°„ëŒ€ ì •ë³´ ë°˜í™˜"""
        hour = dt.hour
        if 6 <= hour < 12:
            return "ì˜¤ì „"
        elif 12 <= hour < 18:
            return "ì˜¤í›„"
        elif 18 <= hour < 24:
            return "ì €ë…"
        else:
            return "ìƒˆë²½"

    # _classify_keywords_with_langchain í•¨ìˆ˜ ì œê±°ë¨
    # ì´ì œ _generate_base_context_keywordsì—ì„œ í‚¤ì›Œë“œ ìƒì„±ê³¼ í™•ì¥ì„ í†µí•© ì²˜ë¦¬
    
    async def _execute_unified_search(self, context: Dict[str, Any], unified_keywords: List[str]) -> Dict[str, Any]:
        """ë‹¤ë‹¨ê³„ Qdrant ê²€ìƒ‰: í‚¤ì›Œë“œë¥¼ ê·¸ë£¹ë³„ë¡œ ë‚˜ëˆ ì„œ ê²€ìƒ‰í•˜ì—¬ ì„ë² ë”© í¬ì„ ë°©ì§€"""
        
        print(f"=== [DEBUG Multi-Stage Search] ì‹œì‘, keywords: {len(unified_keywords)}ê°œ ===")
        
        if not unified_keywords:
            logger.warning("í†µí•© í‚¤ì›Œë“œ ì—†ìŒ - ë¹ˆ ê²°ê³¼ ë°˜í™˜")
            return {"direct_products": [], "category_groups": {}}
        
        try:
            # í‚¤ì›Œë“œë¥¼ 3ê°œ ê·¸ë£¹ìœ¼ë¡œ ë‚˜ëˆ„ê¸°
            # 1ë‹¨ê³„: í•µì‹¬ í‚¤ì›Œë“œ (ì²˜ìŒ 5ê°œ)
            # 2ë‹¨ê³„: ì¤‘ê°„ í‚¤ì›Œë“œ (ë‹¤ìŒ 5ê°œ)
            # 3ë‹¨ê³„: ë³´ì™„ í‚¤ì›Œë“œ (ë‚˜ë¨¸ì§€)
            
            group1 = unified_keywords[:5]   # í•µì‹¬
            group2 = unified_keywords[5:10]  # ì¤‘ê°„
            group3 = unified_keywords[10:]   # ë³´ì™„
            
            all_results = []
            seen_products = set()
            
            # 1ë‹¨ê³„: í•µì‹¬ í‚¤ì›Œë“œ ê²€ìƒ‰ (ê³ ìœ ì‚¬ë„ ê¸°ëŒ€)
            if group1:
                query1 = " ".join(group1)
                print(f"=== [1ë‹¨ê³„ ê²€ìƒ‰] í•µì‹¬ í‚¤ì›Œë“œ: {group1} ===")
                results1 = self.product_embedder.search_products(
                    trend_keywords=[query1],
                    top_k=30,  # 20 â†’ 30 ì¦ê°€
                    score_threshold=0.4,
                    only_ready_products=True
                )
                for r in results1:
                    code = r.get("product_code")
                    if code not in seen_products:
                        all_results.append(r)
                        seen_products.add(code)
                print(f"  â†’ {len(results1)}ê°œ ë°œê²¬ (ëˆ„ì : {len(all_results)}ê°œ)")
            
            # 2ë‹¨ê³„: ì¤‘ê°„ í‚¤ì›Œë“œ ê²€ìƒ‰
            if group2:
                query2 = " ".join(group2)
                print(f"=== [2ë‹¨ê³„ ê²€ìƒ‰] ì¤‘ê°„ í‚¤ì›Œë“œ: {group2} ===")
                results2 = self.product_embedder.search_products(
                    trend_keywords=[query2],
                    top_k=30,  # 20 â†’ 30 ì¦ê°€
                    score_threshold=0.3,
                    only_ready_products=True
                )
                for r in results2:
                    code = r.get("product_code")
                    if code not in seen_products:
                        all_results.append(r)
                        seen_products.add(code)
                print(f"  â†’ {len(results2)}ê°œ ë°œê²¬ (ëˆ„ì : {len(all_results)}ê°œ)")
            
            # 3ë‹¨ê³„: ë³´ì™„ í‚¤ì›Œë“œ ê²€ìƒ‰
            if group3:
                query3 = " ".join(group3)
                print(f"=== [3ë‹¨ê³„ ê²€ìƒ‰] ë³´ì™„ í‚¤ì›Œë“œ: {group3} ===")
                results3 = self.product_embedder.search_products(
                    trend_keywords=[query3],
                    top_k=25,  # 15 â†’ 25 ì¦ê°€
                    score_threshold=0.3,
                    only_ready_products=True
                )
                for r in results3:
                    code = r.get("product_code")
                    if code not in seen_products:
                        all_results.append(r)
                        seen_products.add(code)
                print(f"  â†’ {len(results3)}ê°œ ë°œê²¬ (ëˆ„ì : {len(all_results)}ê°œ)")
            
            print(f"=== [ë‹¤ë‹¨ê³„ ê²€ìƒ‰ ì™„ë£Œ] ì´ {len(all_results)}ê°œ ìƒí’ˆ ===")
            
            # ìœ ì‚¬ë„ ë¶„í¬ í™•ì¸ (ë””ë²„ê¹…)
            if all_results:
                similarities = [p.get("similarity_score", 0) for p in all_results]
                print(f"[ìœ ì‚¬ë„ ë¶„í¬] ìµœê³ : {max(similarities):.3f}, í‰ê· : {sum(similarities)/len(similarities):.3f}, ìµœì €: {min(similarities):.3f}")
                print(f"[ìƒìœ„ 5ê°œ ìœ ì‚¬ë„]")
                for i, p in enumerate(all_results[:5], 1):
                    sim = p.get("similarity_score", 0)
                    name = p.get("product_name", "")[:40]
                    tape = "ğŸ“¼" if (p.get("tape_code") and p.get("tape_name")) else "âŒ"
                    print(f"  {i}. {name} | ìœ ì‚¬ë„: {sim:.3f} | í…Œì´í”„: {tape}")
            
            # ìœ ì‚¬ë„ ê¸°ë°˜ ë¶„ë¥˜
            direct_products = []      # ê³ ìœ ì‚¬ë„: ì§ì ‘ ì¶”ì²œ
            category_groups = {}      # ì¤‘ìœ ì‚¬ë„: ì¹´í…Œê³ ë¦¬ë³„ ê·¸ë£¹
            
            # ìœ ì‚¬ë„ ì„ê³„ê°’
            HIGH_SIMILARITY_THRESHOLD = 0.45  # ì‹¤ì œ ìœ ì‚¬ë„ ë¶„í¬(ìµœê³  0.498)ì— ë§ì¶¤
            
            for product in all_results:
                similarity = product.get("similarity_score", 0)
                category = product.get("category_main", "ê¸°íƒ€")
                
                # ê³ ìœ ì‚¬ë„ ìƒí’ˆ: ì§ì ‘ ë§¤ì¹­
                if similarity >= HIGH_SIMILARITY_THRESHOLD:
                    if product.get("tape_code") and product.get("tape_name"):
                        direct_products.append({
                            **product,
                            "source": "direct_match",
                            "similarity_score": similarity
                        })
                        print(f"  âœ… ì§ì ‘ë§¤ì¹­: {product.get('product_name')[:30]} (ìœ ì‚¬ë„: {similarity:.2f})")
                
                # ì¤‘ìœ ì‚¬ë„: ì¹´í…Œê³ ë¦¬ ê·¸ë£¹í•‘
                if category not in category_groups:
                    category_groups[category] = []
                category_groups[category].append(product)
            
            print(f"=== [ë¶„ë¥˜ ì™„ë£Œ] ì§ì ‘ë§¤ì¹­: {len(direct_products)}ê°œ, ì¹´í…Œê³ ë¦¬: {len(category_groups)}ê°œ ===")
            
            return {
                "direct_products": direct_products,
                "category_groups": category_groups,
                "search_keywords": unified_keywords[:5]
            }
            
        except Exception as e:
            logger.error(f"ë‹¤ë‹¨ê³„ ê²€ìƒ‰ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(f"ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
            return {"direct_products": [], "category_groups": {}}
    
    async def _get_realtime_trend_keywords(self) -> List[str]:
        """ì‹¤ì‹œê°„ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ìˆ˜ì§‘ (OpenAI Web Search)"""
        from openai import OpenAI
        
        try:
            client = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
            
            prompt = """ë‹¹ì‹ ì€ 20ë…„ì°¨ í•œêµ­ ì‡¼í•‘ íŠ¸ë Œë“œ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ì„ë¬´: ì§€ê¸ˆ ì´ ìˆœê°„ í•œêµ­ì—ì„œ ì¸ê¸° ìˆëŠ” ì‡¼í•‘ ê´€ë ¨ í‚¤ì›Œë“œë¥¼ ì°¾ìœ¼ì„¸ìš”**

ì›¹ ê²€ìƒ‰ìœ¼ë¡œ ë‹¤ìŒ ì •ë³´ë¥¼ ìˆ˜ì§‘í•˜ì„¸ìš”:
- í•œêµ­ ì‹¤ì‹œê°„ ì¸ê¸° ê²€ìƒ‰ì–´
- í˜„ì¬ ì´ìŠˆê°€ ë˜ëŠ” ì´ë²¤íŠ¸ (ìŠ¤í¬ì¸ , ë‚ ì”¨ ì´ìŠˆ, ì‚¬íšŒ ì´ë²¤íŠ¸ ë“±)
- ì‡¼í•‘ íŠ¸ë Œë“œ í‚¤ì›Œë“œ

**ì¤‘ìš”:**
- ì‡¼í•‘/ìƒí’ˆê³¼ ì—°ê´€ ê°€ëŠ¥í•œ í‚¤ì›Œë“œë§Œ ì¶”ì¶œ
- 3-5ê°œì˜ í•µì‹¬ í‚¤ì›Œë“œë§Œ ì„ ë³„
- ë°˜ë“œì‹œ JSON ë°°ì—´ë¡œë§Œ ë°˜í™˜: ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", ...]

**ì˜ˆì‹œ:**
- ê°€ì„ì•¼êµ¬ ê²½ê¸° ì¤‘ â†’ ["ì•¼êµ¬", "ì¹˜í‚¨", "ë§¥ì£¼", "ì‘ì›ìš©í’ˆ"]
- í•œíŒŒì£¼ì˜ë³´ â†’ ["ë‚œë°©", "ì˜¨ì—´ê¸°", "í•«íŒ©"]
- í¬ë¦¬ìŠ¤ë§ˆìŠ¤ ì‹œì¦Œ â†’ ["í¬ë¦¬ìŠ¤ë§ˆìŠ¤", "ì„ ë¬¼", "íŒŒí‹°ìš©í’ˆ"]
"""
            
            print("=" * 80)
            print("[2ë‹¨ê³„ - OpenAI Web Search] ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ìˆ˜ì§‘ ì‹œì‘")
            print("=" * 80)
            print(f"[í”„ë¡¬í”„íŠ¸]\n{prompt}")
            print("=" * 80)
            logger.info(f"[2ë‹¨ê³„] ì‹¤ì‹œê°„ íŠ¸ë Œë“œ í”„ë¡¬í”„íŠ¸: {prompt[:200]}...")
            
            response = client.responses.create(
                model="gpt-4o",
                tools=[{
                    "type": "web_search_preview",
                    "search_context_size": "medium",
                    "user_location": {
                        "type": "approximate",
                        "country": "KR",
                        "timezone": "Asia/Seoul"
                    }
                }],
                input=prompt,
                max_output_tokens=200
            )
            
            result_text = response.output_text
            print("=" * 80)
            print(f"[2ë‹¨ê³„ - ì‘ë‹µ] {result_text}")
            print("=" * 80)
            logger.info(f"[2ë‹¨ê³„] ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ì‘ë‹µ: {result_text}")
            
            # JSON ë°°ì—´ ì¶”ì¶œ
            import json
            import re
            json_match = re.search(r'\[.*?\]', result_text, re.DOTALL)
            if json_match:
                keywords = json.loads(json_match.group())
                print(f"[2ë‹¨ê³„ - ì¶”ì¶œ ì„±ê³µ] í‚¤ì›Œë“œ: {keywords}")
                logger.info(f"[2ë‹¨ê³„] ì‹¤ì‹œê°„ íŠ¸ë Œë“œ í‚¤ì›Œë“œ ì¶”ì¶œ ì„±ê³µ: {keywords}")
                return keywords[:5]  # ìµœëŒ€ 5ê°œë§Œ
            else:
                print("[2ë‹¨ê³„ - ì‹¤íŒ¨] JSON ë°°ì—´ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                logger.warning("[2ë‹¨ê³„] ì‹¤ì‹œê°„ íŠ¸ë Œë“œì—ì„œ JSON ë°°ì—´ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                return []
                
        except Exception as e:
            print("=" * 80)
            print(f"[2ë‹¨ê³„ - ì˜¤ë¥˜] {e}")
            print("=" * 80)
            logger.error(f"[2ë‹¨ê³„] ì‹¤ì‹œê°„ íŠ¸ë Œë“œ ìˆ˜ì§‘ ì‹¤íŒ¨: {e}")
            import traceback
            logger.error(traceback.format_exc())
            return []
    
    async def _generate_base_context_keywords(self, context: Dict[str, Any]) -> List[str]:
        """ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ ì •ë³´ë¥¼ ê¸°ë°˜ìœ¼ë¡œ LangChainìœ¼ë¡œ ê²€ìƒ‰ í‚¤ì›Œë“œ ìƒì„±"""
        
        # ì»¨í…ìŠ¤íŠ¸ ì •ë³´ ì¶”ì¶œ (ì•ˆì „í•˜ê²Œ)
        weather_info = context.get("weather", {})
        logger.info(f"weather_info type: {type(weather_info)}, value: {weather_info}")
        
        if isinstance(weather_info, dict):
            weather = weather_info.get("weather", "ë§‘ìŒ")
            temperature = weather_info.get("temperature", 20)
        else:
            logger.warning(f"weather_info is not dict: {weather_info}")
            weather = "ë§‘ìŒ"
            temperature = 20
        
        time_slot = context.get("time_slot", "ì €ë…")
        day_type = context.get("day_type", "í‰ì¼")
        holiday_name = context.get("holiday_name")  # ê³µíœ´ì¼ ì •ë³´
        
        # ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
        broadcast_dt = context.get("broadcast_dt")
        month = broadcast_dt.month if broadcast_dt else 11
        day = broadcast_dt.day if broadcast_dt else 19
        
        logger.info(f"ì¶”ì¶œëœ ì •ë³´ - weather: {weather}, temp: {temperature}, time_slot: {time_slot}, month: {month}, day: {day}, day_type: {day_type}, holiday: {holiday_name}")
        
        # LangChain í”„ë¡¬í”„íŠ¸ (í‚¤ì›Œë“œ ìƒì„± + í™•ì¥ í†µí•©)
        keyword_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ 20ë…„ì°¨ í™ˆì‡¼í•‘ ìƒí’ˆ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì£¼ì–´ì§„ ìƒí™©ì— ë§ëŠ” **êµ¬ì²´ì ì¸ ìƒí’ˆëª… í‚¤ì›Œë“œ**ë¥¼ ìƒì„±í•˜ê³ , ì¶”ìƒì ì¸ í‚¤ì›Œë“œëŠ” í™•ì¥í•´ì£¼ì„¸ìš”.

**2ë‹¨ê³„ ì‘ì—…:**
1. ìƒí™©ì— ë§ëŠ” í‚¤ì›Œë“œ 10-15ê°œ ìƒì„±
2. ì¶”ìƒì  í‚¤ì›Œë“œë¥¼ êµ¬ì²´ì  ìƒí’ˆëª…ìœ¼ë¡œ í™•ì¥

**í•µì‹¬ ì›ì¹™: ì‹¤ì œ ìƒí’ˆëª…ì²˜ëŸ¼ êµ¬ì²´ì ìœ¼ë¡œ!**

âŒ ë‚˜ìœ ì˜ˆ (ì¶”ìƒì ):
- "ê²¨ìš¸ì¤€ë¹„", "ê±´ê°•ê´€ë¦¬", "ê°€ì¡±ëª¨ì„", "ë”°ëœ»í•œ", "í¸ë¦¬í•œ"

âœ… ì¢‹ì€ ì˜ˆ (êµ¬ì²´ì ):
- "íŒ¨ë”©", "ê¸°ëª¨ë°”ì§€", "ë‹´ìš”", "ì˜¤ë©”ê°€3", "ë½í† í•", "ì˜¨ì—´ê¸°", "ì „ê¸°ì¥íŒ"

**ì‹œì¦Œë³„ êµ¬ì²´ì  í‚¤ì›Œë“œ ì˜ˆì‹œ:**

11ì›”-12ì›” (ê²¨ìš¸):
- ì˜ë¥˜: "íŒ¨ë”©", "ê¸°ëª¨", "ëª©ë„ë¦¬", "ì¥ê°‘", "ê²¨ìš¸ì½”íŠ¸"
- ê°€ì „: "ì˜¨ì—´ê¸°", "ì „ê¸°ì¥íŒ", "íˆí„°", "ê°€ìŠµê¸°"
- ê±´ê°•: "ì˜¤ë©”ê°€3", "ìœ ì‚°ê· ", "í™ì‚¼", "ë¹„íƒ€ë¯¼", "ë©´ì—­"
- ì‹í’ˆ: "êµ°ê³ êµ¬ë§ˆ", "í˜¸ë¹µ", "ì–´ë¬µ", "í•«íŒ©"

7-8ì›” (ì—¬ë¦„):
- ì˜ë¥˜: "ë°˜íŒ”", "ë°˜ë°”ì§€", "ì›í”¼ìŠ¤", "ìƒŒë“¤"
- ê°€ì „: "ì„ í’ê¸°", "ì—ì–´ì»¨", "ì œìŠµê¸°", "ëƒ‰í’ê¸°"
- ê±´ê°•: "ìˆ˜ë¶„í¬ë¦¼", "ìì™¸ì„ ì°¨ë‹¨ì œ", "ë¹„íƒ€ë¯¼C"
- ì‹í’ˆ: "ì•„ì´ìŠ¤í¬ë¦¼", "ëƒ‰ë©´", "ìˆ˜ë°•", "ìŒë£Œ"

**í™•ì¥ ê·œì¹™:**
- "ìˆ˜ëŠ¥ ê°„ì‹" â†’ ["ì´ˆì½œë¦¿", "ê²¬ê³¼ë¥˜", "ì—ë„ˆì§€ë°”", "í™ì‚¼"]
- "ë¸”ë™í”„ë¼ì´ë°ì´" â†’ ["í• ì¸", "íŠ¹ê°€", "ì„¸ì¼"]
- "ê¹€ì¥ ì¬ë£Œ" â†’ ["ê¹€ì¹˜ëƒ‰ì¥ê³ ", "ì ˆì„ë°°ì¶”", "ê³ ì¶§ê°€ë£¨"]
- "ê²¨ìš¸ íŒ¨ì…˜" â†’ ["íŒ¨ë”©", "ê¸°ëª¨", "ì½”íŠ¸", "ëª©ë„ë¦¬"]
- ì´ë¯¸ êµ¬ì²´ì ì´ë©´ í™•ì¥ ë¶ˆí•„ìš”

**ì¤‘ìš” ì§€ì¹¨:**
1. ë¸Œëœë“œëª…ë„ í¬í•¨ ê°€ëŠ¥: "ë½í† í•", "ì¢…ê·¼ë‹¹", "ì¿ ì¿ ", "í•´í”¼ì½œ"
2. ìƒí’ˆ ì¹´í…Œê³ ë¦¬ëª…: "ê±´ê°•ì‹í’ˆ", "ìƒí™œê°€ì „", "ì˜ë¥˜", "ì‹í’ˆ"
3. ì‹œì¦Œ íŠ¹í™” ìƒí’ˆ: 11-12ì›”ì´ë©´ "í¬ë¦¬ìŠ¤ë§ˆìŠ¤", "ì—°ë§ì„ ë¬¼", "ìˆ˜ëŠ¥ê°„ì‹"
4. ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬ í¬í•¨ (ìµœì†Œ 3ê°œ ì´ìƒ ì¹´í…Œê³ ë¦¬)

**ì‹œê°„ëŒ€ë³„ ì¹´í…Œê³ ë¦¬ ìš°ì„ ìˆœìœ„ ë° ê°€ì¤‘ì¹˜ (ì¤‘ìš”!):**

ğŸŒ… ì•„ì¹¨ (06:00-09:59):
- ë§¤ìš° ì í•© (1.2): ê±´ê°•ì‹í’ˆ, ì¼ë°˜ì‹í’ˆ, ì£¼ë°©ìš©í’ˆ
- ë³´í†µ (0.9): ì˜ë¥˜, ê°€ì „
- ë¶€ì í•© (0.8): íŒ¨ì…˜ì¡í™”, ì‹ ë°œ
- ì˜ˆ: "ì˜¤ë©”ê°€3"(1.2), "ìœ ì‚°ê· "(1.2), "ì»¤í”¼"(1.2), "íŒ¨ë”©"(0.9)

ğŸŒ ì ì‹¬ (10:00-13:59):
- ë§¤ìš° ì í•© (1.2): ì¼ë°˜ì‹í’ˆ, ì£¼ë°©ìš©í’ˆ, ìƒí™œìš©í’ˆ
- ë³´í†µ (0.9): ì˜ë¥˜, ê°€ì „, ê±´ê°•ì‹í’ˆ
- ë¶€ì í•© (0.8): íŒ¨ì…˜ì¡í™”, ì‹ ë°œ
- ì˜ˆ: "ê°„í¸ì‹"(1.2), "ë„ì‹œë½"(1.2), "ì²­ì†Œìš©í’ˆ"(1.2), "íŒ¨ë”©"(0.9)

ğŸŒ¤ï¸ ì˜¤í›„ (14:00-17:59):
- ë§¤ìš° ì í•© (1.2): ê°€êµ¬/ì¹¨êµ¬, ìƒí™œìš©í’ˆ, ê°€ì „
- ë³´í†µ (1.0): ê±´ê°•ì‹í’ˆ, ì˜ë¥˜, ì‹í’ˆ
- ì˜ˆ: "ì¹¨ëŒ€"(1.2), "ë§¤íŠ¸ë¦¬ìŠ¤"(1.2), "ì²­ì†Œê¸°"(1.2), "íŒ¨ë”©"(1.0)

ğŸŒ™ ì €ë…/ë°¤ (18:00-05:59):
- ë§¤ìš° ì í•© (1.2): ì˜ë¥˜, íŒ¨ì…˜ì¡í™”/ë³´ì„, ì‹ ë°œ, í™”ì¥í’ˆ/ë·°í‹°
- ì í•© (1.1): ê±´ê°•ì‹í’ˆ, ê°€ì „, ê°€êµ¬/ì¹¨êµ¬
- ë³´í†µ (0.9): ì‹í’ˆ, ì£¼ë°©ìš©í’ˆ
- ì˜ˆ: "íŒ¨ë”©"(1.2), "ê¸°ëª¨"(1.2), "ëª©ë„ë¦¬"(1.2), "ìŠ¤í‚¨ì¼€ì–´"(1.2), "í™”ì¥í’ˆ"(1.2), "í™ì‚¼"(1.1)

**ê°€ì¤‘ì¹˜ ê·œì¹™:**
- 1.2: í•´ë‹¹ ì‹œê°„ëŒ€ì— ë§¤ìš° ì í•©í•œ ì¹´í…Œê³ ë¦¬
- 1.1: ì í•©í•œ ì¹´í…Œê³ ë¦¬
- 1.0: ë³´í†µ (ê¸°ë³¸ê°’)
- 0.9: ë‹¤ì†Œ ë¶€ì í•©
- 0.8: ë¶€ì í•©

JSON í˜•ì‹ìœ¼ë¡œ ë°˜í™˜ (ê° í‚¤ì›Œë“œì— ê°€ì¤‘ì¹˜ í¬í•¨):
{{
  "keywords": [
    {{"keyword": "í‚¤ì›Œë“œ1", "weight": 1.2}},
    {{"keyword": "í‚¤ì›Œë“œ2", "weight": 1.0}},
    {{"keyword": "í‚¤ì›Œë“œ3", "weight": 0.9}}
  ],
  "expanded": {{
    "ì¶”ìƒí‚¤ì›Œë“œ1": ["êµ¬ì²´1", "êµ¬ì²´2", "êµ¬ì²´3"],
    "ì¶”ìƒí‚¤ì›Œë“œ2": ["êµ¬ì²´1", "êµ¬ì²´2"]
  }}
}}"""),
            ("human", """ë‚ ì§œ: {month}ì›” {day}ì¼
ë‚ ì”¨: {weather}
ê¸°ì˜¨: {temperature}ë„
ì‹œê°„ëŒ€: {time_slot}
ìš”ì¼ íƒ€ì…: {day_type}
ê³µíœ´ì¼: {holiday_name}

ìœ„ ìƒí™©ì— ì í•©í•œ ìƒí’ˆ ê²€ìƒ‰ í‚¤ì›Œë“œë¥¼ ìƒì„±í•´ì£¼ì„¸ìš”. 
**íŠ¹íˆ ì‹œê°„ëŒ€({time_slot})ë¥¼ ê³ ë ¤í•´ì„œ í•´ë‹¹ ì‹œê°„ëŒ€ì— ì í•©í•œ ì¹´í…Œê³ ë¦¬ì˜ í‚¤ì›Œë“œë¥¼ ìš°ì„ ì ìœ¼ë¡œ ìƒì„±í•˜ì„¸ìš”!**""")
        ])
        
        chain = keyword_prompt | self.llm | JsonOutputParser()
        
        try:
            # í”„ë¡¬í”„íŠ¸ ë¡œê¹… (ëˆˆì— ë„ê²Œ)
            prompt_vars = {
                "month": month,
                "day": day,
                "weather": weather,
                "temperature": temperature,
                "time_slot": time_slot,
                "day_type": day_type,
                "holiday_name": holiday_name if holiday_name else "ì—†ìŒ"
            }
            print("=" * 80)
            print("[1ë‹¨ê³„ - LangChain í”„ë¡¬í”„íŠ¸] ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ ìƒì„±")
            print("=" * 80)
            print(f"ì…ë ¥ ë³€ìˆ˜:")
            for key, value in prompt_vars.items():
                print(f"  - {key}: {value}")
            print("=" * 80)
            logger.info(f"[1ë‹¨ê³„] ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸ ë³€ìˆ˜: {prompt_vars}")
            
            result = await chain.ainvoke({
                "month": month,
                "day": day,
                "weather": weather,
                "temperature": temperature,
                "time_slot": time_slot,
                "day_type": day_type,
                "holiday_name": holiday_name if holiday_name else "ì—†ìŒ"
            })
            
            # ê²°ê³¼ íŒŒì‹±
            keyword_weights = {}  # í‚¤ì›Œë“œë³„ ê°€ì¤‘ì¹˜ ì €ì¥
            
            if isinstance(result, dict):
                keywords_data = result.get("keywords", [])
                expansion_map = result.get("expanded", {})
                
                # í‚¤ì›Œë“œì™€ ê°€ì¤‘ì¹˜ ë¶„ë¦¬
                keywords = []
                for item in keywords_data:
                    if isinstance(item, dict):
                        kw = item.get("keyword", "")
                        weight = item.get("weight", 1.0)
                        keywords.append(kw)
                        keyword_weights[kw] = weight
                    else:
                        # í´ë°±: ë¬¸ìì—´ë¡œ ì˜¨ ê²½ìš°
                        keywords.append(item)
                        keyword_weights[item] = 1.0
            else:
                # í´ë°±: ë¦¬ìŠ¤íŠ¸ë¡œ ì˜¨ ê²½ìš°
                keywords = result if isinstance(result, list) else []
                expansion_map = {}
                for kw in keywords:
                    keyword_weights[kw] = 1.0
            
            print("=" * 80)
            print(f"[1ë‹¨ê³„ - ì‘ë‹µ] LLM ìƒì„± í‚¤ì›Œë“œ (ê°€ì¤‘ì¹˜ í¬í•¨):")
            for kw in keywords[:10]:
                weight = keyword_weights.get(kw, 1.0)
                print(f"  - {kw}: {weight}x")
            print(f"[1ë‹¨ê³„ - ê²°ê³¼] ì´ {len(keywords)}ê°œ í‚¤ì›Œë“œ")
            print("=" * 80)
            
            # í™•ì¥ í‚¤ì›Œë“œ ì²˜ë¦¬ ë° ë§¤í•‘ ìƒì„±
            expanded_keywords = []
            keyword_mapping = {}
            
            print(f"[1ë‹¨ê³„ - í™•ì¥] LLM í™•ì¥ ê²°ê³¼:")
            for original_kw in keywords:
                # ì›ë³¸ í‚¤ì›Œë“œ ì¶”ê°€ (ê°€ì¤‘ì¹˜ ìœ ì§€)
                expanded_keywords.append(original_kw)
                keyword_mapping[original_kw] = original_kw
                
                # í™•ì¥ëœ í‚¤ì›Œë“œ ì¶”ê°€
                if original_kw in expansion_map:
                    expanded_list = expansion_map[original_kw]
                    print(f"  ğŸ”„ '{original_kw}' â†’ {expanded_list}")
                    expanded_keywords.extend(expanded_list)
                    
                    # ë§¤í•‘ ì €ì¥
                    for exp_kw in expanded_list:
                        keyword_mapping[exp_kw] = original_kw
            
            # ì¤‘ë³µ ì œê±°
            expanded_keywords = list(dict.fromkeys(expanded_keywords))
            
            print("=" * 80)
            print(f"[1ë‹¨ê³„ - LLM í™•ì¥ ì™„ë£Œ] ì›ë³¸ {len(keywords)}ê°œ â†’ í™•ì¥ {len(expanded_keywords)}ê°œ")
            print(f"[1ë‹¨ê³„ - í™•ì¥ í‚¤ì›Œë“œ] {expanded_keywords}")
            print("=" * 80)
            
            # RAG ë°©ì‹: ì‹¤ì œ DB ìƒí’ˆëª… ê¸°ë°˜ í‚¤ì›Œë“œ ì¬í™•ì¥
            rag_keywords = await self._extract_keywords_from_actual_products(expanded_keywords)
            
            # RAG í‚¤ì›Œë“œë„ ë§¤í•‘ì— ì¶”ê°€ (ì›ë³¸ í‚¤ì›Œë“œë¡œ ì—­ì¶”ì )
            for rag_kw in rag_keywords:
                if rag_kw not in keyword_mapping:
                    # RAGë¡œ ì¶”ì¶œëœ í‚¤ì›Œë“œëŠ” ê°€ì¥ ê´€ë ¨ ìˆëŠ” ì›ë³¸ í‚¤ì›Œë“œë¡œ ë§¤í•‘
                    # ê°„ë‹¨í•˜ê²Œ ì²« ë²ˆì§¸ ì›ë³¸ í‚¤ì›Œë“œë¡œ ë§¤í•‘ (ê°œì„  ê°€ëŠ¥)
                    keyword_mapping[rag_kw] = keywords[0] if keywords else rag_kw
            
            # ìµœì¢… í‚¤ì›Œë“œ ìˆœì„œ ìµœì í™”:
            # 1. RAG í‚¤ì›Œë“œ (ìµœìš°ì„ ! ì‹¤ì œ DB ìƒí’ˆëª… ê¸°ë°˜)
            # 2. ì›ë³¸ í‚¤ì›Œë“œ (LLM ìƒì„±)
            # 3. LLM í™•ì¥ í‚¤ì›Œë“œ (ë³´ì™„)
            final_keywords = []
            
            # 1ìˆœìœ„: RAG í‚¤ì›Œë“œ (ìµœìš°ì„ !)
            final_keywords.extend(rag_keywords)
            
            # 2ìˆœìœ„: ì›ë³¸ í‚¤ì›Œë“œ (RAGì— ì—†ëŠ” ê²ƒë§Œ)
            for orig_kw in keywords:
                if orig_kw not in final_keywords:
                    final_keywords.append(orig_kw)
            
            # 3ìˆœìœ„: LLM í™•ì¥ í‚¤ì›Œë“œ (RAG/ì›ë³¸ì— ì—†ëŠ” ê²ƒë§Œ)
            for exp_kw in expanded_keywords:
                if exp_kw not in final_keywords:
                    final_keywords.append(exp_kw)
            
            # contextì— ë§¤í•‘ ì •ë³´ ë° ê°€ì¤‘ì¹˜ ì €ì¥
            context["keyword_mapping"] = keyword_mapping
            context["original_keywords"] = keywords
            context["keyword_weights"] = keyword_weights  # ì‹œê°„ëŒ€ë³„ ê°€ì¤‘ì¹˜
            
            print("=" * 80)
            print(f"[1ë‹¨ê³„ - ìµœì¢… ì™„ë£Œ] ì›ë³¸ {len(keywords)}ê°œ â†’ LLM {len(expanded_keywords)}ê°œ â†’ RAG {len(rag_keywords)}ê°œ â†’ ìµœì¢… {len(final_keywords)}ê°œ")
            print(f"[í‚¤ì›Œë“œ ìˆœì„œ ìµœì í™” - RAG ìµœìš°ì„ !]")
            print(f"  ğŸ¥‡ 1ìˆœìœ„ (RAG): {rag_keywords[:5]}...")
            print(f"  ğŸ¥ˆ 2ìˆœìœ„ (ì›ë³¸): {[k for k in keywords if k not in rag_keywords][:5]}...")
            print(f"  ğŸ¥‰ 3ìˆœìœ„ (í™•ì¥): {[k for k in expanded_keywords if k not in rag_keywords and k not in keywords][:5]}...")
            print(f"[1ë‹¨ê³„ - ìµœì¢… í‚¤ì›Œë“œ ìˆœì„œ] {final_keywords[:20]}...")
            print(f"[1ë‹¨ê³„ - ë§¤í•‘] {len(keyword_mapping)}ê°œ ë§¤í•‘ ì €ì¥")
            print("=" * 80)
            
            logger.info(f"[1ë‹¨ê³„] ì»¨í…ìŠ¤íŠ¸ ê¸°ë°˜ í‚¤ì›Œë“œ ìƒì„± ì™„ë£Œ: {keywords}")
            logger.info(f"[1ë‹¨ê³„] LLM í™•ì¥: {expanded_keywords}")
            logger.info(f"[1ë‹¨ê³„] RAG ì¶”ì¶œ: {rag_keywords[:10]}")
            logger.info(f"[1ë‹¨ê³„] ìµœì¢… í‚¤ì›Œë“œ: {final_keywords[:15]}")
            logger.info(f"[1ë‹¨ê³„] í‚¤ì›Œë“œ ë§¤í•‘: {len(keyword_mapping)}ê°œ")
            return final_keywords
        except Exception as e:
            logger.error(f"ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ ìƒì„± ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(f"ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
            # í´ë°±: ì‹œê°„ëŒ€/ê³„ì ˆ ê¸°ë°˜ ì‹¤ìš©ì  í‚¤ì›Œë“œ
            fallback_keywords = []
            
            # ì‹œê°„ëŒ€ë³„ í‚¤ì›Œë“œ
            if time_slot == "ì €ë…":
                fallback_keywords.extend(["ì €ë…ì‹ì‚¬", "ì‹¤ë‚´í™œë™", "íœ´ì‹", "ê°€ì¡±ì‹œê°„"])
            elif time_slot == "ì˜¤ì „":
                fallback_keywords.extend(["ì•„ì¹¨", "ì¶œê·¼", "í™œë ¥", "ê±´ê°•"])
            elif time_slot == "ì˜¤í›„":
                fallback_keywords.extend(["ì ì‹¬", "ì•¼ì™¸í™œë™", "ìš´ë™", "ì‡¼í•‘"])
            else:
                fallback_keywords.extend(["ë°¤", "ìˆ˜ë©´", "íœ´ì‹"])
            
            # ê³„ì ˆë³„ í‚¤ì›Œë“œ
            if season == "ê²¨ìš¸":
                fallback_keywords.extend(["ë”°ëœ»í•œ", "ë³´ì˜¨", "ë‚œë°©"])
            elif season == "ì—¬ë¦„":
                fallback_keywords.extend(["ì‹œì›í•œ", "ëƒ‰ë°©", "íœ´ê°€"])
            elif season == "ë´„":
                fallback_keywords.extend(["ì‹ ì„ í•œ", "ì•¼ì™¸", "ê½ƒ"])
            else:
                fallback_keywords.extend(["ê°€ì„", "ê±´ê°•", "í™˜ì ˆê¸°"])
            
            print(f"[1ë‹¨ê³„ - í´ë°±] í´ë°± í‚¤ì›Œë“œ ì‚¬ìš©: {fallback_keywords}")
            logger.info(f"[1ë‹¨ê³„] í´ë°± í‚¤ì›Œë“œ ì‚¬ìš©: {fallback_keywords}")
            logger.info(f"[1ë‹¨ê³„] í´ë°± í‚¤ì›Œë“œ ê°œìˆ˜: {len(fallback_keywords)}")
            return fallback_keywords
    
    # ì£¼ì„: _expand_keywords_to_product_terms í•¨ìˆ˜ëŠ” ì œê±°ë¨
    # ì´ì œ _generate_base_context_keywordsì—ì„œ í‚¤ì›Œë“œ ìƒì„±ê³¼ í™•ì¥ì„ í•œ ë²ˆì— ì²˜ë¦¬
    
    async def _extract_keywords_from_actual_products(self, trend_keywords: List[str]) -> List[str]:
        """
        RAG ë°©ì‹: ì‹¤ì œ DB ìƒí’ˆëª… ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        1. íŠ¸ë Œë“œ í‚¤ì›Œë“œë¡œ ëŠìŠ¨í•˜ê²Œ ê²€ìƒ‰
        2. ê²€ìƒ‰ëœ ì‹¤ì œ ìƒí’ˆëª… ë¶„ì„
        3. LLMìœ¼ë¡œ ìœ ìš©í•œ í‚¤ì›Œë“œ ì¶”ì¶œ
        
        Returns:
            ì‹¤ì œ DBì— ì¡´ì¬í•˜ëŠ” ìƒí’ˆ ê¸°ë°˜ í‚¤ì›Œë“œ ë¦¬ìŠ¤íŠ¸
        """
        
        print("=" * 80)
        print("[RAG í‚¤ì›Œë“œ ì¶”ì¶œ] ì‹¤ì œ ìƒí’ˆëª… ê¸°ë°˜ í‚¤ì›Œë“œ ì¶”ì¶œ ì‹œì‘")
        print("=" * 80)
        
        try:
            # 1ë‹¨ê³„: ëŠìŠ¨í•œ ê²€ìƒ‰ (ìƒìœ„ 5ê°œ í‚¤ì›Œë“œë§Œ ì‚¬ìš©)
            query = " ".join(trend_keywords[:5])
            print(f"[1ë‹¨ê³„] ëŠìŠ¨í•œ ê²€ìƒ‰ ì¿¼ë¦¬: {query}")
            
            search_results = self.product_embedder.search_products(
                trend_keywords=[query],
                top_k=30,  # ì¶©ë¶„í•œ ìƒ˜í”Œ
                score_threshold=0.25,  # ë§¤ìš° ë‚®ì€ threshold
                only_ready_products=True
            )
            
            if not search_results:
                print("[RAG] ê²€ìƒ‰ ê²°ê³¼ ì—†ìŒ - ì›ë³¸ í‚¤ì›Œë“œ ë°˜í™˜")
                return trend_keywords
            
            # 2ë‹¨ê³„: ì‹¤ì œ ìƒí’ˆëª… ì¶”ì¶œ
            actual_product_names = [
                result.get("product_name", "")
                for result in search_results[:20]  # ìƒìœ„ 20ê°œë§Œ
            ]
            
            print(f"[2ë‹¨ê³„] ê²€ìƒ‰ëœ ìƒí’ˆ {len(actual_product_names)}ê°œ:")
            for i, name in enumerate(actual_product_names[:5], 1):
                print(f"  {i}. {name[:50]}")
            
            # 3ë‹¨ê³„: LLMìœ¼ë¡œ í‚¤ì›Œë“œ ì¶”ì¶œ
            extraction_prompt = ChatPromptTemplate.from_messages([
                ("system", """ë‹¹ì‹ ì€ í™ˆì‡¼í•‘ ìƒí’ˆ ê²€ìƒ‰ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

**ì„ë¬´**: ì‹¤ì œ DB ìƒí’ˆëª…ë“¤ì„ ë¶„ì„í•´ì„œ ê²€ìƒ‰ì— ìœ ìš©í•œ í‚¤ì›Œë“œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.

**ì¶”ì¶œ ê·œì¹™**:
1. ë¸Œëœë“œëª… ì¶”ì¶œ (ì˜ˆ: "ì¿ ì¿ ", "í•„ë¦½ìŠ¤", "ë½í† í•")
2. ìƒí’ˆ ì¹´í…Œê³ ë¦¬ (ì˜ˆ: "ì••ë ¥ì†¥", "ì—ì–´í”„ë¼ì´ì–´", "ìœ ì‚°ê· ")
3. í•µì‹¬ í‚¤ì›Œë“œ (ì˜ˆ: "IH", "XXL", "í”„ë¡œë°”ì´ì˜¤í‹±ìŠ¤")
4. ì¤‘ë³µ ì œê±°

**ì˜ˆì‹œ**:
ìƒí’ˆëª…: "ì¿ ì¿  IH 10ì¸ìš© ì••ë ¥ë°¥ì†¥"
ì¶”ì¶œ: ["ì¿ ì¿ ", "ì••ë ¥ì†¥", "ë°¥ì†¥", "IH"]

ìƒí’ˆëª…: "í•„ë¦½ìŠ¤ ì—ì–´í”„ë¼ì´ì–´ XXL 7.3L"
ì¶”ì¶œ: ["í•„ë¦½ìŠ¤", "ì—ì–´í”„ë¼ì´ì–´", "íŠ€ê¹€ê¸°", "XXL"]

JSON í˜•ì‹:
{{"keywords": ["í‚¤ì›Œë“œ1", "í‚¤ì›Œë“œ2", ...]}}""")
,
                ("human", """íŠ¸ë Œë“œ í‚¤ì›Œë“œ: {trend_keywords}

ìš°ë¦¬ DBì—ì„œ ê²€ìƒ‰ëœ ì‹¤ì œ ìƒí’ˆëª…ë“¤:
{product_names}

ìœ„ ìƒí’ˆëª…ë“¤ì„ ë¶„ì„í•´ì„œ ê²€ìƒ‰ì— ìœ ìš©í•œ í‚¤ì›Œë“œ 15-20ê°œë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.""")
            ])
            
            chain = extraction_prompt | self.llm | JsonOutputParser()
            
            result = await chain.ainvoke({
                "trend_keywords": ", ".join(trend_keywords[:5]),
                "product_names": "\n".join([f"{i+1}. {name}" for i, name in enumerate(actual_product_names)])
            })
            
            extracted_keywords = result.get("keywords", [])
            
            print("=" * 80)
            print(f"[3ë‹¨ê³„] LLM ì¶”ì¶œ ì™„ë£Œ: {len(extracted_keywords)}ê°œ í‚¤ì›Œë“œ")
            print(f"[ì¶”ì¶œ í‚¤ì›Œë“œ] {extracted_keywords[:10]}...")
            print("=" * 80)
            
            return extracted_keywords
            
        except Exception as e:
            logger.error(f"RAG í‚¤ì›Œë“œ ì¶”ì¶œ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(f"ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
            
            # í´ë°±: ì›ë³¸ í‚¤ì›Œë“œ ë°˜í™˜
            print(f"[RAG ì‹¤íŒ¨] ì›ë³¸ í‚¤ì›Œë“œ ì‚¬ìš©: {trend_keywords}")
            return trend_keywords
    
    async def _generate_context_keywords(self, context: Dict[str, Any]) -> List[str]:
        """í†µí•© í‚¤ì›Œë“œ ìƒì„±: 1ë‹¨ê³„(ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸) + 2ë‹¨ê³„(ì‹¤ì‹œê°„ íŠ¸ë Œë“œ)"""
        
        print("=" * 80)
        print("[í†µí•© í‚¤ì›Œë“œ ìƒì„±] 1ë‹¨ê³„: ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ")
        print("=" * 80)
        
        # 1ë‹¨ê³„: ê¸°ë³¸ ì»¨í…ìŠ¤íŠ¸ í‚¤ì›Œë“œ (ë‚ ì”¨, ì‹œê°„ëŒ€, ê³„ì ˆ, ê³µíœ´ì¼)
        base_keywords = await self._generate_base_context_keywords(context)
        logger.info(f"1ë‹¨ê³„ ê¸°ë³¸ í‚¤ì›Œë“œ: {base_keywords}")
        
        print("=" * 80)
        print("[í†µí•© í‚¤ì›Œë“œ ìƒì„±] 2ë‹¨ê³„: ì‹¤ì‹œê°„ íŠ¸ë Œë“œ í‚¤ì›Œë“œ")
        print("=" * 80)
        
        # 2ë‹¨ê³„: ì‹¤ì‹œê°„ íŠ¸ë Œë“œ í‚¤ì›Œë“œ (OpenAI Web Search)
        realtime_keywords = await self._get_realtime_trend_keywords()
        logger.info(f"2ë‹¨ê³„ ì‹¤ì‹œê°„ íŠ¸ë Œë“œ: {realtime_keywords}")
        
        # í†µí•©: RAG ìµœìš°ì„  ìœ ì§€! (base_keywords ë‚´ë¶€ ìˆœì„œ: RAG â†’ ì›ë³¸ â†’ í™•ì¥)
        # ì›¹ íŠ¸ë Œë“œëŠ” ë³´ì™„ìš©ìœ¼ë¡œ ë’¤ì— ë°°ì¹˜
        combined_keywords = base_keywords + realtime_keywords
        
        # ì¤‘ë³µ ì œê±° (ìˆœì„œ ìœ ì§€)
        unique_keywords = list(dict.fromkeys(combined_keywords))
        
        print("=" * 80)
        print(f"[í†µí•© í‚¤ì›Œë“œ ìˆœì„œ] RAG ìµœìš°ì„  â†’ ì›ë³¸ â†’ í™•ì¥ â†’ ì›¹ íŠ¸ë Œë“œ")
        print(f"  1ìˆœìœ„ (RAG): {base_keywords[:5]}...")
        print(f"  ë³´ì™„ (ì›¹): {realtime_keywords[:3]}...")
        print(f"[í†µí•© í‚¤ì›Œë“œ] ìµœì¢… {len(unique_keywords)}ê°œ")
        print(f"[ìµœì¢… ìˆœì„œ] {unique_keywords[:15]}...")
        print("=" * 80)
        logger.info(f"í†µí•© í‚¤ì›Œë“œ ìƒì„± ì™„ë£Œ: {unique_keywords[:20]}")
        
        return unique_keywords
    
    async def _generate_unified_candidates(
        self,
        search_result: Dict[str, Any],
        context: Dict[str, Any],
        max_trend_match: int = 8,  # ìœ ì‚¬ë„ ê¸°ë°˜ ìµœëŒ€ ê°œìˆ˜ (ì˜ë¥˜ í¸ì¤‘ ë°©ì§€)
        max_sales_prediction: int = 32  # ë§¤ì¶œì˜ˆì¸¡ ê¸°ë°˜ ìµœëŒ€ ê°œìˆ˜ (ë‹¤ì–‘ì„± í™•ë³´)
    ) -> tuple[List[Dict[str, Any]], Dict[str, Any]]:
        """í†µí•© í›„ë³´êµ° ìƒì„± - ëª¨ë“  ìƒí’ˆ XGBoost ì˜ˆì¸¡ í›„ ê°€ì¤‘ì¹˜ ì¡°ì •"""
        
        candidates = []
        seen_products = set()
        
        print(f"=== [DEBUG Unified Candidates] í›„ë³´êµ° ìƒì„± ì‹œì‘ (ëª©í‘œ: ìµœëŒ€ {max_trend_match + max_sales_prediction}ê°œ) ===")
        
        # 1. ëª¨ë“  ê²€ìƒ‰ ê²°ê³¼ë¥¼ í•˜ë‚˜ë¡œ í†µí•©
        all_products = []
        all_products.extend(search_result["direct_products"])  # ê³ ìœ ì‚¬ë„ ìƒí’ˆ
        
        # ì¹´í…Œê³ ë¦¬ ê·¸ë£¹ì˜ ëª¨ë“  ìƒí’ˆë„ ì¶”ê°€
        for category, products in search_result["category_groups"].items():
            all_products.extend(products)
        
        print(f"=== [DEBUG] í†µí•©ëœ ìƒí’ˆ ìˆ˜: {len(all_products)}ê°œ ===")
        
        # 2. ì¤‘ë³µ ì œê±° (ìƒí’ˆì½”ë“œ + ì†Œë¶„ë¥˜ + ë¸Œëœë“œ)
        unique_products = {}
        seen_category_brand_pairs = set()  # (ì†Œë¶„ë¥˜, ë¸Œëœë“œ) ì¡°í•©
        
        for product in all_products:
            product_code = product.get("product_code")
            category_sub = product.get("category_sub", "")
            brand = product.get("brand", "")
            
            # ìƒí’ˆì½”ë“œ ì¤‘ë³µ ì²´í¬
            if product_code in unique_products:
                continue
            
            # ì†Œë¶„ë¥˜ + ë¸Œëœë“œ ì¡°í•© ì¤‘ë³µ ì²´í¬ (ë‹¤ì–‘ì„± ë³´ì¥)
            category_brand_key = (category_sub, brand)
            if category_sub and brand and category_brand_key in seen_category_brand_pairs:
                logger.info(f"ì†Œë¶„ë¥˜+ë¸Œëœë“œ ì¤‘ë³µ ì œì™¸: {product.get('product_name', '')[:30]} (ì†Œë¶„ë¥˜: {category_sub}, ë¸Œëœë“œ: {brand})")
                continue
            
            # í†µê³¼í•œ ê²½ìš° ì¶”ê°€
            unique_products[product_code] = product
            if category_sub and brand:
                seen_category_brand_pairs.add(category_brand_key)
        
        print(f"=== [DEBUG] ì¤‘ë³µ ì œê±° í›„: {len(unique_products)}ê°œ (ì†Œë¶„ë¥˜+ë¸Œëœë“œ ë‹¤ì–‘ì„± ë³´ì¥) ===")
        
        # 3. ë°°ì¹˜ ì˜ˆì¸¡ ì¤€ë¹„ (ìƒìœ„ 40ê°œ - ì˜ë¥˜ í¸ì¤‘ ë°©ì§€)
        products_list = list(unique_products.values())[:40]
        print(f"=== [DEBUG] ë°°ì¹˜ ì˜ˆì¸¡ ëŒ€ìƒ: {len(products_list)}ê°œ ===")
        
        # 4. ë°°ì¹˜ XGBoost ì˜ˆì¸¡ (í•œ ë²ˆì— ì²˜ë¦¬)
        predicted_sales_list = await self._predict_products_sales_batch(products_list, context)
        
        # 5. ì˜ˆì¸¡ ê²°ê³¼ì™€ ìƒí’ˆ ë§¤ì¹­ + ì ìˆ˜ ê³„ì‚°
        for i, product in enumerate(products_list):
            similarity = product.get("similarity_score", 0.5)
            predicted_sales = predicted_sales_list[i]
            
            # ì ìˆ˜ ê³„ì‚° (ìœ ì‚¬ë„ vs ë§¤ì¶œ ê°€ì¤‘ì¹˜ ì¡°ì •)
            if similarity >= 0.7:
                # ê³ ìœ ì‚¬ë„: ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜ ë†’ì„
                final_score = (
                    similarity * 0.7 +  # ìœ ì‚¬ë„ 70%
                    (predicted_sales / 100000000) * 0.3  # ë§¤ì¶œ 30% (ì •ê·œí™”: 1ì–µ ê¸°ì¤€)
                )
                source = "trend_match"
                print(f"  [ê³ ìœ ì‚¬ë„] {product.get('product_name')[:20]}: ìœ ì‚¬ë„={similarity:.2f}, ë§¤ì¶œ={predicted_sales/10000:.0f}ë§Œì›, ì ìˆ˜={final_score:.3f}")
            else:
                # ì €ìœ ì‚¬ë„: ë§¤ì¶œ ê°€ì¤‘ì¹˜ ë†’ì„
                final_score = (
                    similarity * 0.3 +  # ìœ ì‚¬ë„ 30%
                    (predicted_sales / 100000000) * 0.7  # ë§¤ì¶œ 70%
                )
                source = "sales_prediction"
                print(f"  [ì €ìœ ì‚¬ë„] {product.get('product_name')[:20]}: ìœ ì‚¬ë„={similarity:.2f}, ë§¤ì¶œ={predicted_sales/10000:.0f}ë§Œì›, ì ìˆ˜={final_score:.3f}")
            
            candidates.append({
                "product": product,
                "source": source,
                "similarity_score": similarity,
                "predicted_sales": predicted_sales,
                "final_score": final_score
            })
        
        # 4. ì ìˆ˜ìˆœ ì •ë ¬
        candidates.sort(key=lambda x: x["final_score"], reverse=True)
        
        print(f"=== [DEBUG] ì´ {len(candidates)}ê°œ í›„ë³´ ìƒì„± ì™„ë£Œ, ì ìˆ˜ìˆœ ì •ë ¬ë¨ ===")
        
        # 5. ì¹´í…Œê³ ë¦¬ë³„ ì ìˆ˜ ê³„ì‚° (ë‚´ë¶€ ì‚¬ìš©ìš©)
        category_scores = {}
        category_sales = {}
        for candidate in candidates:
            category = candidate["product"].get("category_main", "ê¸°íƒ€")
            if category == "ê¸°íƒ€" or not category:
                continue
            if category not in category_sales:
                category_sales[category] = []
            category_sales[category].append(candidate["predicted_sales"])
        
        for category, sales_list in category_sales.items():
            avg_sales = sum(sales_list) / len(sales_list)
            category_scores[category] = {"predicted_sales": avg_sales}
        
        return candidates, category_scores
    
    async def _predict_categories_with_xgboost(
        self, 
        category_groups: Dict[str, List[Dict]], 
        context: Dict[str, Any]
    ) -> Dict[str, Dict[str, float]]:
        """ì¹´í…Œê³ ë¦¬ë³„ XGBoost ë§¤ì¶œ ì˜ˆì¸¡"""
        
        category_scores = {}
        broadcast_dt = context["broadcast_dt"]
        
        for category, products in category_groups.items():
            if not products:
                continue
            
            try:
                # ëŒ€í‘œ ìƒí’ˆìœ¼ë¡œ ì¹´í…Œê³ ë¦¬ ë§¤ì¶œ ì˜ˆì¸¡
                representative_product = products[0]
                predicted_sales = await self._predict_product_sales(representative_product, context)
                
                # ì¹´í…Œê³ ë¦¬ ë‚´ ìƒí’ˆ ìˆ˜ë¡œ ë³´ì •
                adjusted_sales = predicted_sales * min(len(products) / 5, 2.0)
                
                category_scores[category] = {
                    "predicted_sales": adjusted_sales,
                    "product_count": len(products),
                    "avg_similarity": sum(p.get("similarity_score", 0) for p in products) / len(products)
                }
                
                print(f"  - ì¹´í…Œê³ ë¦¬ '{category}': {int(adjusted_sales/10000)}ë§Œì› (ìƒí’ˆ: {len(products)}ê°œ)")
                
            except Exception as e:
                logger.error(f"ì¹´í…Œê³ ë¦¬ '{category}' ì˜ˆì¸¡ ì‹¤íŒ¨: {e}")
                category_scores[category] = {
                    "predicted_sales": 10000000,  # ê¸°ë³¸ê°’ 1000ë§Œì›
                    "product_count": len(products),
                    "avg_similarity": 0.4
                }
        
        return category_scores
    
    async def _generate_candidates(self, promising_categories: List[Any], trend_products: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """í›„ë³´êµ° ìƒì„± ë° í†µí•© (ë ˆê±°ì‹œ, ì‚¬ìš© ì•ˆ í•¨)"""
        candidates = []
        
        # ìœ ë§ ì¹´í…Œê³ ë¦¬ì—ì„œ ì—ì´ìŠ¤ ìƒí’ˆ ì„ ë°œ
        for category in promising_categories[:3]:
            ace_products = await self._get_ace_products_from_category(category.name, 5)
            
            for product in ace_products:
                candidates.append({
                    "product": product,
                    "source": "category",
                    "base_score": product.get("predicted_sales_score", 0.5),
                    "trend_boost": 1.0
                })
        
        return candidates
    
    async def _rank_final_candidates(self, candidates: List[Dict[str, Any]], category_scores: Dict[str, Any], context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ìµœì¢… ë­í‚¹ ê³„ì‚° - ì‹œì¦Œ ì í•©ì„± + ì¹´í…Œê³ ë¦¬+ë¸Œëœë“œ ë‹¤ì–‘ì„± ì ìš©"""
        
        print(f"=== [DEBUG _rank_final_candidates] ì´ë¯¸ ì ìˆ˜ìˆœìœ¼ë¡œ ì •ë ¬ëœ {len(candidates)}ê°œ í›„ë³´ ìˆ˜ì‹  ===")
        
        # 0. ì‹œì¦Œ ì í•©ì„± í•„í„°ë§ (LLM ë°°ì¹˜ íŒë‹¨) - ìƒìœ„ 40ê°œ í›„ë³´ì— ëŒ€í•´
        top_candidates = candidates[:40]  # ì¶©ë¶„í•œ í›„ë³´êµ° ì¤€ë¹„ (ì‹œì¦Œ í•„í„° + ì¤‘ë³µ ì œê±° ê³ ë ¤)
        print(f"\n=== [ì‹œì¦Œ ì í•©ì„± ê²€ì‚¬] ìƒìœ„ {len(top_candidates)}ê°œ í›„ë³´ ê²€ì‚¬ ì‹œì‘ ===")
        
        season_filtered = await self._filter_by_season_suitability(top_candidates, context)
        print(f"=== [ì‹œì¦Œ ì í•©ì„± ê²€ì‚¬] {len(top_candidates)}ê°œ â†’ {len(season_filtered)}ê°œ (ë¶€ì í•© {len(top_candidates) - len(season_filtered)}ê°œ ì œê±°) ===\n")
        
        # 1. ì¹´í…Œê³ ë¦¬+ë¸Œëœë“œ ì¤‘ë³µ ì œê±° + ëŒ€ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ ì¿¼í„° ì œí•œ
        category_brand_seen = set()
        category_count = {}  # ëŒ€ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ë³„ ê°œìˆ˜
        filtered_candidates = []
        
        for candidate in season_filtered:
            product = candidate["product"]
            category = product.get("category_main", "Unknown")
            brand = product.get("brand", "Unknown")
            key = f"{category}_{brand}"
            
            # 1-1. ê°™ì€ ì¹´í…Œê³ ë¦¬+ë¸Œëœë“œ ì¡°í•©ì€ 1ê°œë§Œ í—ˆìš© (ë‹¤ì–‘ì„± ë³´ì¥)
            if key in category_brand_seen:
                print(f"  âš ï¸ ë¸Œëœë“œ ì¤‘ë³µ ì œê±°: {product.get('product_name')[:30]} (ì¹´í…Œê³ ë¦¬: {category}, ë¸Œëœë“œ: {brand})")
                continue
            
            # 1-2. ê°™ì€ ëŒ€ë¶„ë¥˜ ì¹´í…Œê³ ë¦¬ëŠ” ìµœëŒ€ 4ê°œê¹Œì§€ë§Œ í—ˆìš©
            current_count = category_count.get(category, 0)
            if current_count >= 4:
                print(f"  âš ï¸ ì¹´í…Œê³ ë¦¬ ì¿¼í„° ì´ˆê³¼: {product.get('product_name')[:30]} (ì¹´í…Œê³ ë¦¬: {category}, ì´ë¯¸ {current_count}ê°œ)")
                continue
            
            # í†µê³¼: í›„ë³´ì— ì¶”ê°€
            filtered_candidates.append(candidate)
            category_brand_seen.add(key)
            category_count[category] = current_count + 1
        
        print(f"=== [ë‹¤ì–‘ì„± í•„í„°ë§] {len(season_filtered)}ê°œ â†’ {len(filtered_candidates)}ê°œ (ì¤‘ë³µ {len(season_filtered) - len(filtered_candidates)}ê°œ ì œê±°) ===")
        print(f"=== [ì¹´í…Œê³ ë¦¬ ë¶„í¬] {category_count} ===")
        
        for i, candidate in enumerate(filtered_candidates[:5]):
            product = candidate['product']
            print(f"  {i+1}ìœ„: {product.get('product_name')[:25]} | {product.get('category_main', 'N/A')} | {product.get('brand', 'N/A')} (ì ìˆ˜: {candidate['final_score']:.3f})")
        
        return filtered_candidates
    
    async def _filter_by_season_suitability(self, candidates: List[Dict[str, Any]], context: Dict[str, Any]) -> List[Dict[str, Any]]:
        """ì‹œì¦Œ ì í•©ì„± í•„í„°ë§ - LLM ë°°ì¹˜ íŒë‹¨"""
        
        if not candidates:
            return []
        
        # í˜„ì¬ ë‚ ì§œ ì •ë³´ ì¶”ì¶œ
        broadcast_dt = context.get("broadcast_dt")
        month = broadcast_dt.month if broadcast_dt else 11
        day = broadcast_dt.day if broadcast_dt else 19
        holiday_name = context.get("holiday_name")
        
        # ìƒí’ˆ ì •ë³´ ì¤€ë¹„ (ìƒí’ˆëª… + í…Œì´í”„ëª…)
        products_info = []
        for i, candidate in enumerate(candidates):
            product = candidate["product"]
            products_info.append({
                "index": i,
                "product_name": product.get("product_name", ""),
                "tape_name": product.get("tape_name", ""),
                "category": product.get("category_main", "")
            })
        
        # LLM í”„ë¡¬í”„íŠ¸
        season_prompt = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ 20ë…„ì°¨ í™ˆì‡¼í•‘ ë°©ì†¡ í¸ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
í˜„ì¬ ë‚ ì§œ/ê³„ì ˆì— ì–´ìš¸ë¦¬ì§€ ì•ŠëŠ” ìƒí’ˆì„ ì°¾ì•„ì£¼ì„¸ìš”.

**ì œì™¸ ê¸°ì¤€ (ìƒí’ˆì˜ ì‹¤ì œ íŠ¹ì„± ì¤‘ì‹¬):**

1. ëª…ì ˆ ë¶ˆì¼ì¹˜: íŠ¹ì • ëª…ì ˆ ìƒí’ˆì¸ë° í˜„ì¬ ëª…ì ˆê³¼ ë§ì§€ ì•ŠëŠ” ê²½ìš°
   - ì˜ˆ: 11ì›”ì— "ì‹ ë…„íŠ¹ì§‘", "ì„¤ë‚ ", "ì¶”ì„" í¬í•¨ ìƒí’ˆ
   - ì˜ˆ: 7ì›”ì— "í¬ë¦¬ìŠ¤ë§ˆìŠ¤" í¬í•¨ ìƒí’ˆ
   - ì„ í–‰ íŒë§¤ í—ˆìš©: 12ì›” ë§ ì‹ ë…„íŠ¹ì§‘ â­•, 12ì›” ì¤‘ìˆœ í¬ë¦¬ìŠ¤ë§ˆìŠ¤ â­•, 8ì›” ë§ ì¶”ì„ â­•

2. ê³„ì ˆ/ë‚ ì”¨ ë¶€ì í•© ìƒí’ˆ (ìƒí’ˆ íŠ¹ì„±ìœ¼ë¡œë§Œ íŒë‹¨):
   
   **ê²¨ìš¸ì² (11ì›”~2ì›”) - ì¶”ìš´ ë‚ ì”¨ì— ì œì™¸í•  ê²ƒ:**
   - ì—¬ë¦„ ëƒ‰ë°©: "ëƒ‰ê°", "ì¿¨ë§", "ì‹œì›í•œ", "ëƒ‰ë°©", "í”¼ì„œìš©", "ì—¬ë¦„ìš©"
   - ì—¬ë¦„ ì˜ë¥˜: "ë°˜íŒ”", "ë°˜ë°”ì§€", "ë¯¼ì†Œë§¤", "ìƒŒë“¤" (ì‹¤ë‚´ìš© ì œì™¸)
   - ì—¬ë¦„ ì¹¨êµ¬: "ëƒ‰ê° íŒ¨ë“œ", "ì¿¨ë§¤íŠ¸"
   - ì˜ˆ: "ì¿¨ë“œë¦¼ ëƒ‰ê°íŒ¨ë“œ", "ì—¬ë¦„ ë°˜íŒ”í‹°", "í”¼ì„œìš© ì„ í’ê¸°"
   
   **ê²¨ìš¸ì² (11ì›”~2ì›”) - ì¶”ìš´ ë‚ ì”¨ì— ì í•© (í—ˆìš©):**
   - ë‚œë°© ìƒí’ˆ: "ì „ê¸°ì¥íŒ", "ì „ê¸°ë‹´ìš”", "ì˜¨ì—´", "ë‚œë°©", "ë³´ì˜¨"
   - ê²¨ìš¸ ì˜ë¥˜: "íŒ¨ë”©", "ê¸°ëª¨", "ê²¨ìš¸", "ì½”íŠ¸", "ëª©ë„ë¦¬", "ì¥ê°‘", "ë‘êº¼ìš´"
   - ì˜ˆ: "ì „ê¸°ë§¤íŠ¸", "ì˜¨ì—´ë§ˆì‚¬ì§€ê¸°", "íŒ¨ë”©", "ê¸°ëª¨ë°”ì§€" â†’ ëª¨ë‘ OK!
   
   **ì—¬ë¦„ì² (6ì›”~8ì›”) - ë”ìš´ ë‚ ì”¨ì— ì œì™¸í•  ê²ƒ:**
   - ë‚œë°© ìƒí’ˆ: "ì „ê¸°ì¥íŒ", "ì „ê¸°ë‹´ìš”", "ì˜¨ì—´", "ë‚œë°©"
   - ê²¨ìš¸ ì˜ë¥˜: "íŒ¨ë”©", "ê¸°ëª¨", "ê²¨ìš¸", "ë‘êº¼ìš´ ì½”íŠ¸", "ëª©ë„ë¦¬"
   - ì˜ˆ: "ê²¨ìš¸ íŒ¨ë”©", "ê¸°ëª¨ ë°”ì§€", "ì „ê¸°ì¥íŒ"
   
   **ë´„/ê°€ì„(3~5ì›”, 9~10ì›”) - í™˜ì ˆê¸°:**
   - 3~5ì›”: ê²¨ìš¸ ë‚œë°© ìƒí’ˆ ì œì™¸, ì—¬ë¦„ ëƒ‰ë°© ìƒí’ˆ OK
   - 9~10ì›”: ì—¬ë¦„ ëƒ‰ë°© ìƒí’ˆ ì œì™¸, ê²¨ìš¸ ë‚œë°© ìƒí’ˆ OK

**ì¤‘ìš” - ì‹œì¦Œ ì½”ë“œ(SS/FW)ëŠ” ë¬´ì‹œí•˜ì„¸ìš”:**
- "25SS", "24FW" ê°™ì€ ì½”ë“œëŠ” ì°¸ê³ ë§Œ í•˜ê³ , ìƒí’ˆì˜ ì‹¤ì œ íŠ¹ì„±ìœ¼ë¡œ íŒë‹¨
- ì˜ˆ: "25SS ê¸°ëª¨ ë°”ì§€" â†’ ê¸°ëª¨ê°€ ìˆìœ¼ë©´ ê²¨ìš¸ì— OK
- ì˜ˆ: "24FW ë°˜íŒ”í‹°" â†’ ë°˜íŒ”ì´ë©´ ê²¨ìš¸ì— ì œì™¸
- ì˜ˆ: "23SS íŒ¨ë”©" â†’ íŒ¨ë”©ì´ë©´ ì—¬ë¦„ì— ì œì™¸

# ì„ í–‰ íŒë§¤ëŠ” í—ˆìš© (1~2ì£¼ ì „)
- 12ì›” ë§ ì‹ ë…„íŠ¹ì§‘ â­•
- 12ì›” ì¤‘ìˆœ í¬ë¦¬ìŠ¤ë§ˆìŠ¤ ì¼€ì´í¬ â­•
- 8ì›” ë§ ì¶”ì„ì„ ë¬¼ì„¸íŠ¸ â­•

**ì œì™¸í•˜ì§€ ë§ ê²ƒ:**
- ì‚¬ê³„ì ˆ ìƒí’ˆ: ê±´ê°•ì‹í’ˆ, ìƒí™œìš©í’ˆ, ì‹í’ˆ, ê°€ì „ ë“±
- ì‹œì¦Œ í‚¤ì›Œë“œê°€ ì—†ëŠ” ì¼ë°˜ ìƒí’ˆ

JSON í˜•ì‹ìœ¼ë¡œ ì œì™¸í•  ìƒí’ˆì˜ ì¸ë±ìŠ¤ ë°°ì—´ì„ ë°˜í™˜í•˜ì„¸ìš”:
{{
  "exclude_indices": [ì¸ë±ìŠ¤ ë°°ì—´],
  "reasons": {{
    "ì¸ë±ìŠ¤": "ì œì™¸ ì´ìœ "
  }}
}}"""),
            ("human", """í˜„ì¬ ì •ë³´:
- ë‚ ì§œ: {month}ì›” {day}ì¼
- ê³µíœ´ì¼: {holiday_name}

ìƒí’ˆ ëª©ë¡:
{products_list}

ìœ„ ìƒí’ˆ ì¤‘ í˜„ì¬ ë‚ ì§œ/ì‹œì¦Œì— ì í•©í•˜ì§€ ì•Šì€ ìƒí’ˆì˜ ì¸ë±ìŠ¤ë¥¼ ì°¾ì•„ì£¼ì„¸ìš”.
ì˜ˆ: 11ì›” ì¤‘ìˆœì´ë©´ ê²¨ìš¸ ìƒí’ˆì€ OK, ì¶”ì„/ì„¤ë‚  ìƒí’ˆì€ ì œì™¸""")
        ])
        
        # ìƒí’ˆ ëª©ë¡ ë¬¸ìì—´ ìƒì„± (ìƒí’ˆëª… + í…Œì´í”„ëª…)
        products_list_str = "\n".join([
            f"{p['index']}. {p['product_name']}\n   í…Œì´í”„ëª…: {p['tape_name']}\n   ì¹´í…Œê³ ë¦¬: {p['category']}"
            for p in products_info
        ])
        
        chain = season_prompt | self.llm | JsonOutputParser()
        
        try:
            result = await chain.ainvoke({
                "month": month,
                "day": day,
                "holiday_name": holiday_name if holiday_name else "ì—†ìŒ",
                "products_list": products_list_str
            })
            
            exclude_indices = set(result.get("exclude_indices", []))
            reasons = result.get("reasons", {})
            
            # ì œì™¸ëœ ìƒí’ˆ ë¡œê·¸
            for idx in exclude_indices:
                if idx < len(candidates):
                    product_name = candidates[idx]["product"].get("product_name", "")[:40]
                    reason = reasons.get(str(idx), "ì‹œì¦Œ ë¶€ì í•©")
                    print(f"  âŒ ì œì™¸: {product_name} - {reason}")
            
            # í•„í„°ë§
            filtered = [c for i, c in enumerate(candidates) if i not in exclude_indices]
            return filtered
            
        except Exception as e:
            logger.error(f"ì‹œì¦Œ ì í•©ì„± íŒë‹¨ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(f"ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
            # ì˜¤ë¥˜ ì‹œ ëª¨ë“  í›„ë³´ ë°˜í™˜ (ì•ˆì „ì¥ì¹˜)
            return candidates
    
    def _calculate_competition_penalty(self, product: Dict[str, Any], all_candidates: List[Dict[str, Any]]) -> float:
        """ê²½ìŸ í˜ë„í‹° ì ìˆ˜ ê³„ì‚°"""
        category = product.get("category_main", "")
        same_category_count = sum(1 for c in all_candidates if c["product"].get("category_main") == category)
        
        # ê°™ì€ ì¹´í…Œê³ ë¦¬ ìƒí’ˆì´ ë§ì„ìˆ˜ë¡ í˜ë„í‹°
        if same_category_count <= 2:
            return 0.0
        elif same_category_count <= 4:
            return 0.1
        else:
            return 0.2
    
    async def _format_response(self, ranked_products: List[Dict[str, Any]], context: Dict[str, Any] = None) -> BroadcastResponse:
        """API ì‘ë‹µ ìƒì„± (ë¹„ë™ê¸°)"""
        print(f"=== [DEBUG _format_response] context keys: {context.keys() if context else 'None'} ===")
        if context:
            print(f"=== [DEBUG _format_response] generated_keywords: {context.get('generated_keywords', [])} ===")
        
        # 1. í…Œì´í”„ ì½”ë“œ ëª©ë¡ ì¶”ì¶œ
        tape_codes = [p["product"].get("tape_code") for p in ranked_products if p["product"].get("tape_code")]
        
        # 2. ìµœê·¼ ë°©ì†¡ ì‹¤ì  ë°°ì¹˜ ì¡°íšŒ (Netezza)
        broadcast_history_map = {}
        if tape_codes:
            logger.info(f" {len(tape_codes)}ê°œ í…Œì´í”„ì˜ ìµœê·¼ ë°©ì†¡ ì‹¤ì  ì¡°íšŒ ì¤‘...")
            broadcast_history_map = self.broadcast_history_service.get_latest_broadcasts_batch(tape_codes)
            logger.info(f" {sum(1 for v in broadcast_history_map.values() if v is not None)}ê°œ í…Œì´í”„ì˜ ì‹¤ì  ì¡°íšŒ ì„±ê³µ")
        
        recommendations = []
        
        # ìˆœìœ„ ì •ë³´ ì¶”ê°€ (ë°°ì¹˜ ì²˜ë¦¬ ì „)
        for i, candidate in enumerate(ranked_products):
            candidate["rank"] = i + 1
            candidate["total_count"] = len(ranked_products)
        
        # [5-1ë‹¨ê³„] ë°°ì¹˜ë¡œ ëª¨ë“  ìƒí’ˆì˜ ì¶”ì²œ ê·¼ê±° ìƒì„± (í•œ ë²ˆì˜ LLM í˜¸ì¶œ)
        step_5_1_start = time.time()
        print("\n" + "=" * 80)
        print(f"[5-1ë‹¨ê³„] LLM ë°°ì¹˜ ì²˜ë¦¬ - {len(ranked_products)}ê°œ ìƒí’ˆì˜ ì¶”ì²œ ê·¼ê±° ìƒì„±")
        print("=" * 80)
        reasoning_list = await self._generate_batch_reasons_with_langchain(
            ranked_products,
            context or {"time_slot": "ì €ë…", "weather": {"weather": "í­ì—¼"}}
        )
        print(f"â±ï¸  [5-1ë‹¨ê³„] ì¶”ì²œ ê·¼ê±° ìƒì„±: {time.time() - step_5_1_start:.2f}ì´ˆ")
        
        for i, candidate in enumerate(ranked_products):
            product = candidate["product"]
            reasoning_summary = reasoning_list[i] if i < len(reasoning_list) else f"{product.get('category_main', 'ìƒí’ˆ')} ì¶”ì²œ"
            
            # ìµœê·¼ ë°©ì†¡ ì‹¤ì  ì¡°íšŒ
            tape_code = product.get("tape_code")
            last_broadcast_data = broadcast_history_map.get(tape_code) if tape_code else None
            last_broadcast = None
            
            if last_broadcast_data:
                try:
                    last_broadcast = LastBroadcastMetrics(**last_broadcast_data)
                    logger.debug(f"âœ… í…Œì´í”„ {tape_code}ì˜ ìµœê·¼ ë°©ì†¡ ì‹¤ì  ì¶”ê°€")
                except Exception as e:
                    logger.warning(f"âš ï¸ í…Œì´í”„ {tape_code}ì˜ ì‹¤ì  ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {e}")
            
            recommendation = BroadcastRecommendation(
                rank=i+1,
                productInfo=ProductInfo(
                    productId=product.get("product_code", "Unknown"),
                    productName=product.get("product_name", "Unknown"),
                    category=product.get("category_main", "Unknown"),
                    categoryMiddle=product.get("category_middle"),
                    categorySub=product.get("category_sub"),
                    brand=product.get("brand"),
                    price=product.get("price"),
                    tapeCode=product.get("tape_code"),
                    tapeName=product.get("tape_name")
                ),
                reasoning=reasoning_summary,
                businessMetrics=BusinessMetrics(
                    aiPredictedSales=f"{round(candidate['predicted_sales']/10000, 1):,.1f}ë§Œì›",  # AI ì˜ˆì¸¡ ë§¤ì¶œ (XGBoost, ì†Œìˆ˜ì  1ìë¦¬)
                    lastBroadcast=last_broadcast  # ìµœê·¼ ë°©ì†¡ ì‹¤ì  ì¶”ê°€
                )
            )

            # ì¶”ì²œ ê²°ê³¼ ìš”ì•½ ë¡œê·¸ (ì‹œì—°/ë¶„ì„ìš©) - ë‹¤ë¥¸ ë‹¨ê³„ ë¡œê·¸ì™€ ë™ì¼í•˜ê²Œ print ì‚¬ìš©
            try:
                print(
                    f"[RECOMMENDATION] #{recommendation.rank} "
                    f"{recommendation.productInfo.productName} | "
                    f"ì¹´í…Œê³ ë¦¬: {recommendation.productInfo.category} | "
                    f"ì˜ˆì¸¡ë§¤ì¶œ: {recommendation.businessMetrics.aiPredictedSales} | "
                    f"ìµœì¢…ì ìˆ˜: {candidate.get('final_score', 0.0):.3f} | "
                    f"ê·¼ê±°: {recommendation.reasoning}"
                )
            except Exception:
                # ë¡œê¹… ì˜¤ë¥˜ê°€ ì¶”ì²œ ë¡œì§ì— ì˜í–¥ ì£¼ì§€ ì•Šë„ë¡ ë°©ì–´
                pass

            recommendations.append(recommendation)
        
        # [5-2ë‹¨ê³„] ë„¤ì´ë²„/íƒ€ì‚¬ í¸ì„± ì¡°íšŒ ë° AI ì„ íƒ
        step_5_2_start = time.time()
        print("\n" + "=" * 80)
        print(f"[5-2ë‹¨ê³„] ë„¤ì´ë²„/íƒ€ì‚¬ í¸ì„± ì¡°íšŒ ë° AI ì„ íƒ")
        print("=" * 80)
        
        # ë„¤ì´ë²„ ë² ìŠ¤íŠ¸ ìƒí’ˆ ì¡°íšŒ
        naver_products_data = self.external_products_service.get_latest_best_products(limit=10)
        naver_products = [NaverProduct(**product) for product in naver_products_data]
        logger.info(f"âœ… ë„¤ì´ë²„ ìƒí’ˆ {len(naver_products)}ê°œ ìˆ˜ì§‘")
        print(f"âœ… ë„¤ì´ë²„ ìƒí’ˆ {len(naver_products)}ê°œ ìˆ˜ì§‘")
        
        # íƒ€ í™ˆì‡¼í•‘ì‚¬ í¸ì„± ìƒí’ˆ ì¡°íšŒ - Netezzaì—ì„œ ì‹¤ì‹œê°„ ì¡°íšŒ
        try:
            broadcast_time_str = context.get("broadcast_time") if context else None
            if broadcast_time_str:
                competitor_data = await netezza_conn.get_competitor_schedules(broadcast_time_str)
                competitor_products = [CompetitorProduct(**comp) for comp in competitor_data]
                logger.info(f"âœ… íƒ€ì‚¬ í¸ì„± {len(competitor_products)}ê°œ ìˆ˜ì§‘")
                print(f"âœ… íƒ€ì‚¬ í¸ì„± {len(competitor_products)}ê°œ ìˆ˜ì§‘")
            else:
                logger.warning(f"âš ï¸ broadcast_timeì´ contextì— ì—†ìŒ")
                competitor_products = []
        except Exception as e:
            logger.warning(f"âš ï¸ íƒ€ì‚¬ í¸ì„± ì¡°íšŒ ì‹¤íŒ¨: {str(e)}")
            competitor_products = []
        
        # AI ê¸°ë°˜ ë„¤ì´ë²„/íƒ€ì‚¬ í¸ì„± 10ê°œ ì„ íƒ ë° í†µí•©
        selected_competitor_products = await self._select_and_merge_top_10(
            naver_products=naver_products,
            competitor_products=competitor_products,
            broadcast_time=broadcast_time_str,
            context=context
        )
        print(f"â±ï¸  [5-2ë‹¨ê³„] ë„¤ì´ë²„/íƒ€ì‚¬ ì„ íƒ: {time.time() - step_5_2_start:.2f}ì´ˆ")
        
        return BroadcastResponse(
            requestTime="",  # ë©”ì¸ì—ì„œ ì„¤ì •
            recommendations=recommendations,
            competitorProducts=selected_competitor_products
        )
    
    async def _select_and_merge_top_10(
        self,
        naver_products: List[NaverProduct],
        competitor_products: List[CompetitorProduct],
        broadcast_time: str,
        context: Dict[str, Any] = None
    ) -> List[CompetitorProduct]:
        """
        AIë¥¼ í™œìš©í•˜ì—¬ ë„¤ì´ë²„/íƒ€ì‚¬ í¸ì„± ì¤‘ 10ê°œë¥¼ ì„ íƒí•˜ê³  í†µí•©
        ë„¤ì´ë²„:íƒ€ì‚¬ = 5:5 ë¹„ìœ¨ ìœ ì§€ (í•œìª½ì´ ë¶€ì¡±í•˜ë©´ ë‹¤ë¥¸ìª½ìœ¼ë¡œ ì±„ì›€)
        """
        try:
            # 1. ë„¤ì´ë²„ ìƒí’ˆì„ íƒ€ì‚¬ í¸ì„± í˜•ì‹ìœ¼ë¡œ ë³€í™˜
            naver_as_competitor = [
                self._convert_naver_to_competitor(naver, idx)
                for idx, naver in enumerate(naver_products)
            ]
            
            # 2. AIì—ê²Œ 10ê°œ ì„ íƒ ìš”ì²­
            selected_indices = await self._ai_select_top_10(
                naver_products=naver_products,
                competitor_products=competitor_products,
                broadcast_time=broadcast_time,
                context=context
            )
            
            # 3. ì„ íƒëœ í•­ëª© ì¶”ì¶œ (íƒ€ì‚¬ í¸ì„± ë¨¼ì €, ë„¤ì´ë²„ ë‚˜ì¤‘)
            result = []
            
            # íƒ€ì‚¬ ì„ íƒ í•­ëª© (ìš°ì„  ë°°ì¹˜)
            for idx in selected_indices.get("competitor_indices", []):
                if 0 <= idx < len(competitor_products):
                    result.append(competitor_products[idx])
            
            # ë„¤ì´ë²„ ì„ íƒ í•­ëª© (ë’¤ì— ë°°ì¹˜)
            for idx in selected_indices.get("naver_indices", []):
                if 0 <= idx < len(naver_as_competitor):
                    result.append(naver_as_competitor[idx])
            
            logger.info(f"âœ… AI ì„ íƒ ì™„ë£Œ: ë„¤ì´ë²„ {len(selected_indices.get('naver_indices', []))}ê°œ + íƒ€ì‚¬ {len(selected_indices.get('competitor_indices', []))}ê°œ = ì´ {len(result)}ê°œ")
            
            return result[:10]  # ìµœëŒ€ 10ê°œ
            
        except Exception as e:
            logger.error(f"âš ï¸ AI ì„ íƒ ì‹¤íŒ¨, í´ë°± ë¡œì§ ì‚¬ìš©: {str(e)}")
            # í´ë°±: ë„¤ì´ë²„ 5ê°œ + íƒ€ì‚¬ 5ê°œ ë‹¨ìˆœ ì„ íƒ
            return self._fallback_select_top_10(naver_products, competitor_products)
    
    def _convert_naver_to_competitor(self, naver: NaverProduct, index: int) -> CompetitorProduct:
        """ë„¤ì´ë²„ ìƒí’ˆì„ íƒ€ì‚¬ í¸ì„± í˜•ì‹(CompetitorProduct)ìœ¼ë¡œ ë³€í™˜"""
        return CompetitorProduct(
            company_name="ë„¤ì´ë²„ ìŠ¤í† ì–´",
            broadcast_title=f"[ë„¤ì´ë²„ ì¸ê¸° {index + 1}ìœ„] {naver.name[:50]}",
            start_time="",  # ë¹ˆì¹¸
            end_time="",    # ë¹ˆì¹¸
            duration_minutes=None,
            category_main=""  # ë„¤ì´ë²„ ìƒí’ˆì—ëŠ” ì¹´í…Œê³ ë¦¬ ì •ë³´ ì—†ìŒ
        )
    
    async def _ai_select_top_10(
        self,
        naver_products: List[NaverProduct],
        competitor_products: List[CompetitorProduct],
        broadcast_time: str,
        context: Dict[str, Any] = None
    ) -> Dict[str, List[int]]:
        """
        AIë¥¼ í™œìš©í•˜ì—¬ ë„¤ì´ë²„/íƒ€ì‚¬ í¸ì„± ì¤‘ 10ê°œì˜ ì¸ë±ìŠ¤ë¥¼ ì„ íƒ
        """
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt_template = ChatPromptTemplate.from_messages([
            ("system", """ë‹¹ì‹ ì€ 20ë…„ ê²½ë ¥ì˜ í™ˆì‡¼í•‘ ë°©ì†¡ í¸ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.

# ë°ì´í„° ì´í•´
- **ë„¤ì´ë²„ ì¸ê¸° ìƒí’ˆ**: í˜„ì¬ ì‹œì ì˜ ì‹œì¥ íŠ¸ë Œë“œë¥¼ ë°˜ì˜í•œ ì‹¤ì‹œê°„ ë² ìŠ¤íŠ¸ ìƒí’ˆ (ì‹œê°„ ë¬´ê´€)
- **íƒ€ì‚¬ í™ˆì‡¼í•‘ í¸ì„±**: íŠ¹ì • ë°©ì†¡ ì‹œê°„ëŒ€ì˜ ì‹¤ì œ í¸ì„± ì •ë³´ (ì‹œê°„ ê¸°ë°˜)

# ì„ íƒ ê¸°ì¤€
1. **ë¹„ìœ¨**: ë„¤ì´ë²„:íƒ€ì‚¬ = 5:5ë¥¼ ìµœëŒ€í•œ ìœ ì§€ (í•œìª½ ë¶€ì¡± ì‹œ ë‹¤ë¥¸ìª½ìœ¼ë¡œ ì±„ì›€)
2. **ì‹œê°„ ì í•©ì„±**: ìš”ì²­ëœ ë°©ì†¡ ì‹œê°„ëŒ€ì— ì í•©í•œ ìƒí’ˆ/í¸ì„± ì„ íƒ
3. **íŠ¸ë Œë“œ ë°˜ì˜**: ë„¤ì´ë²„ ì¸ê¸° ìƒí’ˆì„ í†µí•´ í˜„ì¬ ì‹œì¥ íŠ¸ë Œë“œ íŒŒì•…
4. **ì¹´í…Œê³ ë¦¬ ê· í˜•**: ë‹¤ì–‘í•œ ì¹´í…Œê³ ë¦¬ë¡œ ì‹œì²­ì ì„ íƒí­ í™•ëŒ€
5. **ê²½ìŸ ë¶„ì„**: íƒ€ì‚¬ í¸ì„±ì„ ì°¸ê³ í•˜ì—¬ ì°¨ë³„í™” ë˜ëŠ” ë²¤ì¹˜ë§ˆí‚¹

# ì„ íƒ ì „ëµ
- ë„¤ì´ë²„ ì¸ê¸° ìƒí’ˆ ì¤‘ ë°©ì†¡ ì‹œê°„ëŒ€ì™€ ì–´ìš¸ë¦¬ëŠ” íŠ¸ë Œë“œ ìƒí’ˆ ì„ íƒ
- íƒ€ì‚¬ í¸ì„± ì¤‘ í•´ë‹¹ ì‹œê°„ëŒ€ì— ê²€ì¦ëœ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ ì°¸ê³ 
- í˜„ì¬ íŠ¸ë Œë“œ(ë„¤ì´ë²„)ì™€ ì‹¤ì œ í¸ì„±(íƒ€ì‚¬)ì˜ ê· í˜• ìœ ì§€

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
  "naver_indices": [ì¸ë±ìŠ¤ ë°°ì—´],
  "competitor_indices": [ì¸ë±ìŠ¤ ë°°ì—´],
  "selection_summary": {{
    "time_match": "ì‹œê°„ëŒ€ ì í•©ì„± íŒë‹¨",
    "diversity": "ì„ íƒí•œ ìƒí’ˆë“¤ì˜ ë‹¤ì–‘ì„± ì„¤ëª…",
    "trend_analysis": "íŠ¸ë Œë“œ ë°˜ì˜ ë°©ì‹"
  }},
  "selection_reason": "ì „ì²´ ì„ íƒ ê·¼ê±° 2-3ë¬¸ì¥"
}}"""),
            ("user", """ë°©ì†¡ ì‹œê°„: {broadcast_time}

ë„¤ì´ë²„ ì¸ê¸° ìƒí’ˆ ({naver_count}ê°œ):
{naver_summary}

íƒ€ì‚¬ í™ˆì‡¼í•‘ í¸ì„± ({competitor_count}ê°œ):
{competitor_summary}

ìœ„ ì •ë³´ë¥¼ ì¢…í•©í•˜ì—¬ ë°©ì†¡ ì‹œê°„({broadcast_time})ì— ìµœì í™”ëœ 10ê°œë¥¼ ì„ íƒí•˜ì„¸ìš”.""")
        ])
        
        # ë„¤ì´ë²„ ìƒí’ˆ ìš”ì•½
        naver_summary = "\n".join([
            f"[{i}] {p.name[:40]} | ê°€ê²©: {p.sale_price:,}ì› | í• ì¸: {p.discount_ratio}% | íŒë§¤ëŸ‰: {p.cumulation_sale_count}"
            for i, p in enumerate(naver_products[:20])  # ìµœëŒ€ 20ê°œë§Œ ì „ë‹¬
        ])
        
        # íƒ€ì‚¬ í¸ì„± ìš”ì•½
        competitor_summary = "\n".join([
            f"[{i}] {c.company_name} | {c.broadcast_title[:40]} | {c.start_time} ~ {c.end_time} | {c.category_main or 'ë¯¸ë¶„ë¥˜'}"
            for i, c in enumerate(competitor_products[:20])  # ìµœëŒ€ 20ê°œë§Œ ì „ë‹¬
        ])
        
        # LLM í˜¸ì¶œ
        chain = prompt_template | self.llm | JsonOutputParser()
        
        result = await chain.ainvoke({
            "broadcast_time": broadcast_time or "ë¯¸ì§€ì •",
            "naver_count": len(naver_products),
            "competitor_count": len(competitor_products),
            "naver_summary": naver_summary or "ì—†ìŒ",
            "competitor_summary": competitor_summary or "ì—†ìŒ"
        })
        
        logger.info(f"AI ì„ íƒ ê·¼ê±°: {result.get('selection_reason', 'ì—†ìŒ')}")
        
        return result
    
    def _fallback_select_top_10(
        self,
        naver_products: List[NaverProduct],
        competitor_products: List[CompetitorProduct]
    ) -> List[CompetitorProduct]:
        """AI ì‹¤íŒ¨ ì‹œ í´ë°±: ë‹¨ìˆœ 5:5 ì„ íƒ (íƒ€ì‚¬ ë¨¼ì €, ë„¤ì´ë²„ ë‚˜ì¤‘)"""
        result = []
        
        # íƒ€ì‚¬ 5ê°œ (ë˜ëŠ” ê°€ëŠ¥í•œ ë§Œí¼) - ìš°ì„  ë°°ì¹˜
        competitor_count = min(5, len(competitor_products))
        for i in range(competitor_count):
            result.append(competitor_products[i])
        
        # ë„¤ì´ë²„ 5ê°œ (ë˜ëŠ” ê°€ëŠ¥í•œ ë§Œí¼) - ë’¤ì— ë°°ì¹˜
        naver_count = min(5, len(naver_products))
        for i in range(naver_count):
            result.append(self._convert_naver_to_competitor(naver_products[i], i))
        
        # 10ê°œ ë¯¸ë§Œì´ë©´ ë‚˜ë¨¸ì§€ë¡œ ì±„ì›€
        if len(result) < 10:
            remaining = 10 - len(result)
            if competitor_count < len(competitor_products):
                for i in range(competitor_count, min(competitor_count + remaining, len(competitor_products))):
                    result.append(competitor_products[i])
            elif naver_count < len(naver_products):
                for i in range(naver_count, min(naver_count + remaining, len(naver_products))):
                    result.append(self._convert_naver_to_competitor(naver_products[i], i))
        
        logger.info(f"í´ë°± ì„ íƒ: íƒ€ì‚¬ ìš°ì„ , ì´ {len(result)}ê°œ")
        return result[:10]
    
    def _generate_recommendation_reason(self, candidate: Dict[str, Any], context: Dict[str, Any] = None) -> str:
        """ê°œì„ ëœ ì¶”ì²œ ê·¼ê±° ìƒì„±"""
        product = candidate["product"]
        source = candidate["source"]
        trend_boost = candidate.get("trend_boost", 1.0)
        predicted_sales = candidate.get("predicted_sales", 0)
        final_score = candidate.get("final_score", 0)
        
        # ê¸°ë³¸ ì •ë³´ ì¶”ì¶œ
        category = product.get("category_main", "")
        product_name = product.get("product_name", "")
        trend_keyword = candidate.get("trend_keyword", "")
        tape_name = product.get("tape_name", "")
        
        # ì‹œê°„ëŒ€ ì •ë³´
        time_slot = context.get("time_slot", "") if context else ""
        weather = context.get("weather", {}).get("weather", "") if context else ""
        
        # ê·¼ê±° êµ¬ì„± ìš”ì†Œë“¤
        reasons = []
        
        # 1. íŠ¸ë Œë“œ ê´€ë ¨ ê·¼ê±°
        if source == "trend" and trend_keyword:
            if trend_boost > 1.3:
                reasons.append(f"'{trend_keyword}' íŠ¸ë Œë“œ ê¸‰ìƒìŠ¹ ë°˜ì˜")
            elif trend_boost > 1.1:
                reasons.append(f"'{trend_keyword}' íŠ¸ë Œë“œ ìƒìŠ¹ì„¸")
            else:
                reasons.append(f"'{trend_keyword}' í‚¤ì›Œë“œ ì—°ê´€ì„±")
        
        # 2. ì¹´í…Œê³ ë¦¬ ê´€ë ¨ ê·¼ê±°
        elif source == "category":
            reasons.append(f"{category} ì¹´í…Œê³ ë¦¬ ìœ ë§ ìƒí’ˆ")
        
        # 3. ë§¤ì¶œ ì˜ˆì¸¡ ê·¼ê±°
        if predicted_sales > 80000000:  # 8ì²œë§Œì› ì´ìƒ
            reasons.append("ë†’ì€ ë§¤ì¶œ ì˜ˆì¸¡")
        elif predicted_sales > 50000000:  # 5ì²œë§Œì› ì´ìƒ
            reasons.append("ì•ˆì •ì  ë§¤ì¶œ ì˜ˆì¸¡")
        
        # 4. ì‹œê°„ëŒ€ ì í•©ì„±
        if time_slot and weather:
            if time_slot == "ì €ë…" and category in ["ê±´ê°•ì‹í’ˆ", "í™”ì¥í’ˆ"]:
                reasons.append("ì €ë… ì‹œê°„ëŒ€ ìµœì ")
            elif time_slot == "ì˜¤í›„" and category in ["ê°€ì „ì œí’ˆ", "ìƒí™œìš©í’ˆ"]:
                reasons.append("ì˜¤í›„ ì‹œê°„ëŒ€ ì í•©")
            elif weather == "í­ì—¼" and category in ["ê°€ì „ì œí’ˆ"] and "ì„ í’ê¸°" in product_name:
                reasons.append("í­ì—¼ ë‚ ì”¨ ìµœì  ìƒí’ˆ")
        
        # 5. ë°©ì†¡í…Œì´í”„ ì •ë³´
        if tape_name:
            reasons.append("ë°©ì†¡í…Œì´í”„ ì¤€ë¹„ ì™„ë£Œ")
        
        # 6. AI ì‹ ë¢°ë„
        if final_score > 0.8:
            reasons.append("AI ë†’ì€ ì‹ ë¢°ë„")
        elif final_score > 0.6:
            reasons.append("AI ì¶”ì²œ ì í•©")
        
        # ê·¼ê±°ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ ë©”ì‹œì§€
        if not reasons:
            reasons.append("ì¢…í•© ë¶„ì„ ê²°ê³¼ ì¶”ì²œ")
        
        # ìµœëŒ€ 3ê°œ ê·¼ê±°ë§Œ ì‚¬ìš©
        return " + ".join(reasons[:3])
    
    def _generate_diverse_reason_templates(self, candidate: Dict[str, Any], context: Dict[str, Any] = None) -> List[str]:
        """ë‹¤ì–‘í•œ ì¶”ì²œ ê·¼ê±° í…œí”Œë¦¿ ìƒì„±"""
        product = candidate["product"]
        source = candidate["source"]
        trend_boost = candidate.get("trend_boost", 1.0)
        predicted_sales = candidate.get("predicted_sales", 0)
        
        # ê¸°ë³¸ ì •ë³´
        category = product.get("category_main", "")
        product_name = product.get("product_name", "")
        trend_keyword = candidate.get("trend_keyword", "")
        
        templates = []
        
        # íŠ¸ë Œë“œ ê¸°ë°˜ í…œí”Œë¦¿ë“¤
        if source == "trend" and trend_keyword:
            trend_templates = [
                f"'{trend_keyword}' ê²€ìƒ‰ëŸ‰ ê¸‰ì¦ìœ¼ë¡œ ë†’ì€ ê´€ì‹¬ë„ ì˜ˆìƒ",
                f"ì‹¤ì‹œê°„ '{trend_keyword}' íŠ¸ë Œë“œ ë°˜ì˜í•œ íƒ€ì´ë° ìƒí’ˆ",
                f"'{trend_keyword}' í‚¤ì›Œë“œ ì—°ê´€ ìƒí’ˆìœ¼ë¡œ ì‹œì²­ì ê´€ì‹¬ ì§‘ì¤‘",
                f"íŠ¸ë Œë“œ '{trend_keyword}'ì™€ ì™„ë²½ ë§¤ì¹­ë˜ëŠ” ìµœì  ìƒí’ˆ",
                f"'{trend_keyword}' í™”ì œì„± í™œìš©í•œ ì‹œì˜ì ì ˆí•œ í¸ì„±"
            ]
            templates.extend(trend_templates)
        
        # ë§¤ì¶œ ì˜ˆì¸¡ ê¸°ë°˜ í…œí”Œë¦¿ë“¤
        sales_million = int(predicted_sales / 1000000)
        if sales_million > 80:
            sales_templates = [
                f"AI ì˜ˆì¸¡ ë§¤ì¶œ {sales_million}ë°±ë§Œì›ìœ¼ë¡œ ìµœê³  ìˆ˜ìµ ê¸°ëŒ€",
                f"ê³¼ê±° ë°ì´í„° ë¶„ì„ ê²°ê³¼ {sales_million}ë°±ë§Œì› ë§¤ì¶œ ì˜ˆìƒ",
                f"ë¨¸ì‹ ëŸ¬ë‹ ëª¨ë¸ ì˜ˆì¸¡ {sales_million}ë°±ë§Œì› ê³ ìˆ˜ìµ ìƒí’ˆ"
            ]
        elif sales_million > 50:
            sales_templates = [
                f"ì•ˆì •ì  {sales_million}ë°±ë§Œì› ë§¤ì¶œ ì˜ˆì¸¡ìœ¼ë¡œ ë¦¬ìŠ¤í¬ ìµœì†Œí™”",
                f"ê²€ì¦ëœ {sales_million}ë°±ë§Œì› ìˆ˜ìµ ëª¨ë¸ ìƒí’ˆ",
                f"ì˜ˆì¸¡ ë§¤ì¶œ {sales_million}ë°±ë§Œì›ìœ¼ë¡œ ì•ˆì „í•œ í¸ì„± ì„ íƒ"
            ]
        else:
            sales_templates = [
                "ë°ì´í„° ê¸°ë°˜ ë§¤ì¶œ ì˜ˆì¸¡ìœ¼ë¡œ ê²€ì¦ëœ ìƒí’ˆ",
                "AI ë¶„ì„ ê²°ê³¼ ìˆ˜ìµì„± í™•ì¸ëœ ì¶”ì²œ ìƒí’ˆ",
                "ê³¼ê±° ì„±ê³¼ ë°ì´í„° ê¸°ë°˜ ì„ ë³„ëœ ìƒí’ˆ"
            ]
        templates.extend(sales_templates)
        
        # ì¹´í…Œê³ ë¦¬ ê¸°ë°˜ í…œí”Œë¦¿ë“¤
        category_templates = [
            f"{category} ë¶„ì•¼ ë² ìŠ¤íŠ¸ì…€ëŸ¬ ê²€ì¦ ìƒí’ˆ",
            f"{category} ì¹´í…Œê³ ë¦¬ ë‚´ ê²½ìŸë ¥ 1ìœ„ ìƒí’ˆ",
            f"{category} ì‹œì¥ì—ì„œ ì…ì¦ëœ ì¸ê¸° ìƒí’ˆ",
            f"{category} ì „ë¬¸ ìƒí’ˆìœ¼ë¡œ íƒ€ê²Ÿ ì‹œì²­ì í™•ë³´",
            f"{category} ë¶„ì•¼ í”„ë¦¬ë¯¸ì—„ ë¸Œëœë“œ ìƒí’ˆ"
        ]
        templates.extend(category_templates)
        
        # ë‚ ì”¨ ê¸°ë°˜ í…œí”Œë¦¿ (ì„ íƒì , AIê°€ íŒë‹¨ ëª»í•  ë•Œë§Œ ì‚¬ìš©)
        if context:
            weather = context.get("weather", {}).get("weather", "")
            
            # ê·¹ë‹¨ì  ë‚ ì”¨ë§Œ í…œí”Œë¦¿ ì œê³µ (AI í´ë°±ìš©)
            if weather in ["í­ì—¼", "í•œíŒŒ", "í­ìš°", "í­ì„¤"]:
                weather_templates = [
                    f"{weather} íŠ¹ìˆ˜ ìƒí™© ëŒ€ì‘ ìƒí’ˆ",
                    f"í˜„ì¬ {weather} ìƒí™©ì— í•„ìš”í•œ ì•„ì´í…œ"
                ]
                templates.extend(weather_templates)
        
        # ë°©ì†¡í…Œì´í”„ ê¸°ë°˜ í…œí”Œë¦¿ë“¤
        tape_name = product.get("tape_name", "")
        if tape_name:
            tape_templates = [
                f"ì „ìš© ë°©ì†¡í…Œì´í”„ '{tape_name}' ì™„ë²½ ì¤€ë¹„ ì™„ë£Œ",
                f"ê²€ì¦ëœ ë°©ì†¡ ì½˜í…ì¸ ë¡œ ì‹œì²­ì ëª°ì…ë„ ê·¹ëŒ€í™”",
                f"ì „ë¬¸ ì œì‘ ë°©ì†¡í…Œì´í”„ë¡œ ìƒí’ˆ ë§¤ë ¥ ì™„ë²½ ì „ë‹¬"
            ]
            templates.extend(tape_templates)
        
        return templates
    
    async def _generate_fallback_response(self, request_time: str, recommendation_count: int) -> BroadcastResponse:
        """API í• ë‹¹ëŸ‰ ì†Œì§„ ì‹œ ì„ì‹œ ë°ì´í„°ë¡œ ì¶”ì²œ ê·¼ê±° ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸"""
        
        # ì„ì‹œ ìƒí’ˆ ë°ì´í„° (ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì‹¤ì œ ì¡´ì¬í•˜ëŠ” ìƒí’ˆë“¤)
        mock_products = [
            {
                "product_code": "P001",
                "product_name": "í”„ë¦¬ë¯¸ì—„ ë‹¤ì´ì–´íŠ¸ ë³´ì¡°ì œ",
                "category_main": "ê±´ê°•ì‹í’ˆ",
                "tape_code": "T001",
                "tape_name": "í”„ë¦¬ë¯¸ì—„ ë‹¤ì´ì–´íŠ¸ ë³´ì¡°ì œ"
            },
            {
                "product_code": "P002", 
                "product_name": "í™ˆíŠ¸ë ˆì´ë‹ ì„¸íŠ¸",
                "category_main": "ìŠ¤í¬ì¸ ìš©í’ˆ",
                "tape_code": "T002",
                "tape_name": "í™ˆíŠ¸ë ˆì´ë‹ ì„¸íŠ¸ ì™„ì „ì •ë³µ"
            },
            {
                "product_code": "P005",
                "product_name": "ì‹œì›í•œ ì—¬ë¦„ ì„ í’ê¸°",
                "category_main": "ê°€ì „ì œí’ˆ",
                "tape_code": "T005",
                "tape_name": "ì‹œì›í•œ ì—¬ë¦„ë‚˜ê¸° ì„ í’ê¸°"
            }
        ]
        
        # ì„ì‹œ í›„ë³´ ë°ì´í„° ìƒì„±
        mock_candidates = []
        for i, product in enumerate(mock_products[:recommendation_count]):
            candidate = {
                "product": product,
                "source": "trend" if i == 0 else "category",
                "base_score": 0.8 - i * 0.1,
                "trend_boost": 1.3 if i == 0 else 1.0,
                "predicted_sales": 85000000 - i * 15000000,
                "final_score": 0.85 - i * 0.1,
                "trend_keyword": "ë‹¤ì´ì–´íŠ¸" if i == 0 else ""
            }
            mock_candidates.append(candidate)
        
        # ì»¨í…ìŠ¤íŠ¸ ìƒì„±
        context = {
            "time_slot": "ì €ë…",
            "weather": {"weather": "í­ì—¼"},
            "competitors": []
        }
        
        # ê°œì„ ëœ ì¶”ì²œ ê·¼ê±° ì‹œìŠ¤í…œìœ¼ë¡œ ì‘ë‹µ ìƒì„±
        response = await self._format_response(mock_candidates, context)
        response.requestTime = request_time
        
        logger.info(f"í´ë°± ì‘ë‹µ ìƒì„± ì™„ë£Œ: {len(mock_candidates)}ê°œ ì¶”ì²œ (ì¶”ì²œ ê·¼ê±° ì‹œìŠ¤í…œ í…ŒìŠ¤íŠ¸)")
        return response
    
    async def _generate_batch_reasons_with_langchain(self, candidates: List[Dict[str, Any]], context: Dict[str, Any] = None) -> List[str]:
        """ë°°ì¹˜ë¡œ ì—¬ëŸ¬ ìƒí’ˆì˜ ì¶”ì²œ ê·¼ê±°ë¥¼ í•œ ë²ˆì— ìƒì„± (ì†ë„ ê°œì„ )"""
        try:
            # ì»¨í…ìŠ¤íŠ¸ ì •ë³´
            time_slot = context.get("time_slot", "") if context else ""
            weather = context.get("weather", {}).get("weather", "") if context else ""
            holiday_name = context.get("holiday_name") if context else None
            
            # í‚¤ì›Œë“œ ë§¤í•‘ ì •ë³´ (í™•ì¥ëœ í‚¤ì›Œë“œ â†’ ì›ë³¸ í‚¤ì›Œë“œ)
            keyword_mapping = context.get("keyword_mapping", {}) if context else {}
            original_keywords = context.get("original_keywords", []) if context else []
            
            # ìƒí’ˆ ì •ë³´ ìš”ì•½
            products_summary = []
            for candidate in candidates:
                product = candidate["product"]
                rank = candidate.get("rank", 0)
                predicted_sales = candidate.get("predicted_sales", 0)
                similarity_score = candidate.get("similarity_score", 0)
                final_score = candidate.get("final_score", 0)
                trend_keyword = candidate.get("trend_keyword", "")
                
                # íŠ¸ë Œë“œ í‚¤ì›Œë“œì˜ ì›ë³¸ í‚¤ì›Œë“œ ì°¾ê¸°
                original_keyword = keyword_mapping.get(trend_keyword, trend_keyword) if trend_keyword else ""
                
                products_summary.append({
                    "rank": rank,
                    "product_name": product.get("product_name", ""),
                    "category": product.get("category_main", ""),
                    "predicted_sales": int(predicted_sales/10000) if predicted_sales else 0,
                    "similarity_score": f"{similarity_score:.3f}",
                    "final_score": f"{final_score:.3f}",
                    "trend_keyword": trend_keyword,
                    "original_keyword": original_keyword  # ì›ë³¸ í‚¤ì›Œë“œ ì¶”ê°€
                })
            
            # ë°°ì¹˜ í”„ë¡¬í”„íŠ¸ ìƒì„±
            batch_prompt = ChatPromptTemplate.from_messages([
                ("system", """ë‹¹ì‹ ì€ í™ˆì‡¼í•‘ ë°©ì†¡ í¸ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
ì—¬ëŸ¬ ìƒí’ˆì˜ ì¶”ì²œ ê·¼ê±°ë¥¼ í•œ ë²ˆì— ì‘ì„±í•˜ì„¸ìš”.

# í•µì‹¬ ì›ì¹™
1. **ê° ìƒí’ˆë§ˆë‹¤ 100ì ì´ë‚´** ê°„ê²°í•˜ê²Œ ì‘ì„±
2. ì „ë¬¸ì ì´ê³  ê°ê´€ì ì¸ í†¤ ìœ ì§€
3. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ë°ì´í„° í™œìš©
4. **ê° ìƒí’ˆë§ˆë‹¤ ì™„ì „íˆ ë‹¤ë¥¸ ê´€ì ê³¼ í‘œí˜„ ì‚¬ìš©**
5. ê°™ì€ íŒ¨í„´ì´ë‚˜ ë¬¸êµ¬ ë°˜ë³µ ì ˆëŒ€ ê¸ˆì§€

# í™œìš© ê°€ëŠ¥í•œ ìš”ì†Œë“¤
- ì˜ˆì¸¡ ë§¤ì¶œ ìˆ˜ì¹˜ (í•„ìˆ˜)
- ì¹´í…Œê³ ë¦¬ íŠ¹ì„± (í•„ìˆ˜)
- **ì›ë³¸ í‚¤ì›Œë“œ** (ë§¤ìš° ì¤‘ìš”! ìˆì„ ê²½ìš° ë°˜ë“œì‹œ í™œìš©)
  * ì˜ˆ: ìƒí’ˆì´ "ì´ˆì½œë¦¿"ì´ê³  ì›ë³¸ í‚¤ì›Œë“œê°€ "ìˆ˜ëŠ¥ ê°„ì‹"ì´ë©´
    â†’ "ìˆ˜ëŠ¥ ê°„ì‹ìœ¼ë¡œ ì í•©í•œ ì´ˆì½œë¦¿"ì²˜ëŸ¼ í‘œí˜„
  * ì˜ˆ: ìƒí’ˆì´ "íŒ¨ë”©"ì´ê³  ì›ë³¸ í‚¤ì›Œë“œê°€ "ê²¨ìš¸ íŒ¨ì…˜"ì´ë©´
    â†’ "ê²¨ìš¸ íŒ¨ì…˜ íŠ¸ë Œë“œì— ë§ëŠ” íŒ¨ë”©"ì²˜ëŸ¼ í‘œí˜„
- ê³µíœ´ì¼ (ìˆì„ ê²½ìš° í•„ìˆ˜ ì–¸ê¸‰)
- ì‹œê°„ëŒ€ íŠ¹ì„± (ì €ë…/ì˜¤ì „/ì˜¤í›„) - ì‹ ì¤‘í•˜ê²Œ íŒë‹¨
- ë‚ ì”¨/ê³„ì ˆ (ì„ íƒì )

# ê¸ˆì§€ ì‚¬í•­
- "AI ë¶„ì„ ê²°ê³¼"ë¡œ ì‹œì‘í•˜ì§€ ë§ˆì„¸ìš”
- í…œí”Œë¦¿ì²˜ëŸ¼ ë³´ì´ëŠ” ë°˜ë³µì  í‘œí˜„ ê¸ˆì§€
- ê³¼ì¥ëœ í‘œí˜„ ê¸ˆì§€
- ê¸°ìˆ  ìš©ì–´ ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€ (ìœ ì‚¬ë„, ì ìˆ˜, ë¹„ìœ¨ ë“±)

JSON í˜•ì‹ìœ¼ë¡œ ì‘ë‹µ:
{{
  "reasons": [
    "1ë²ˆ ìƒí’ˆ ì¶”ì²œ ê·¼ê±°",
    "2ë²ˆ ìƒí’ˆ ì¶”ì²œ ê·¼ê±°",
    ...
  ]
}}""")
,
                ("human", """ì‹œê°„ëŒ€: {time_slot}
ë‚ ì”¨: {weather}
ê³µíœ´ì¼: {holiday_name}

ì¶”ì²œ ìƒí’ˆ ëª©ë¡:
{products_info}

ìœ„ {count}ê°œ ìƒí’ˆ ê°ê°ì— ëŒ€í•´ ë…ì°½ì ì¸ ì¶”ì²œ ê·¼ê±°ë¥¼ ì‘ì„±í•˜ì„¸ìš”.
**ì›ë³¸ í‚¤ì›Œë“œê°€ ìˆìœ¼ë©´ ë°˜ë“œì‹œ í™œìš©í•˜ì„¸ìš”!**""")
            ])
            
            # ìƒí’ˆ ì •ë³´ í¬ë§·íŒ… (ì›ë³¸ í‚¤ì›Œë“œ í¬í•¨)
            products_info = "\n".join([
                f"{p['rank']}. {p['product_name'][:40]} | ì¹´í…Œê³ ë¦¬: {p['category']} | ì˜ˆì¸¡ë§¤ì¶œ: {p['predicted_sales']}ë§Œì› | ì›ë³¸í‚¤ì›Œë“œ: {p['original_keyword'] or 'ì—†ìŒ'}"
                for p in products_summary
            ])
            
            chain = batch_prompt | self.llm | JsonOutputParser()
            
            result = await chain.ainvoke({
                "time_slot": time_slot or "ë¯¸ì§€ì •",
                "weather": weather or "ë³´í†µ",
                "holiday_name": holiday_name if holiday_name else "ì—†ìŒ",
                "products_info": products_info,
                "count": len(candidates)
            })
            
            reasons = result.get("reasons", [])
            print(f"âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ: {len(reasons)}ê°œ ê·¼ê±° ìƒì„±")
            
            # ê°œìˆ˜ê°€ ë¶€ì¡±í•˜ë©´ ê¸°ë³¸ ë©”ì‹œì§€ë¡œ ì±„ì›€
            while len(reasons) < len(candidates):
                idx = len(reasons)
                reasons.append(f"{candidates[idx]['product'].get('category_main', 'ìƒí’ˆ')} ì¶”ì²œ")
            
            return reasons[:len(candidates)]
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ê·¼ê±° ìƒì„± ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()
            # í´ë°±: ê°œë³„ ìƒì„±
            print("âš ï¸ ë°°ì¹˜ ì²˜ë¦¬ ì‹¤íŒ¨, ê°œë³„ ìƒì„±ìœ¼ë¡œ í´ë°±...")
            return await self._generate_reasons_fallback(candidates, context)
    
    async def _generate_reasons_fallback(self, candidates: List[Dict[str, Any]], context: Dict[str, Any] = None) -> List[str]:
        """ë°°ì¹˜ ì‹¤íŒ¨ ì‹œ í´ë°±: ê°œë³„ ìƒì„±"""
        reasons = []
        for candidate in candidates:
            reason = await self._generate_dynamic_reason_with_langchain(candidate, context)
            reasons.append(reason)
        return reasons
    
    async def _generate_dynamic_reason_with_langchain(self, candidate: Dict[str, Any], context: Dict[str, Any] = None) -> str:
        """LangChainì„ í™œìš©í•œ ë™ì  ì¶”ì²œ ê·¼ê±° ìƒì„± (ê°œë³„, í´ë°±ìš©)"""
        try:
            product = candidate["product"]
            source = candidate["source"]
            predicted_sales = candidate.get("predicted_sales", 0)
            similarity_score = candidate.get("similarity_score", 0)
            final_score = candidate.get("final_score", 0)
            rank = candidate.get("rank", 0)
            
            # ìƒí’ˆ ì •ë³´
            category = product.get("category_main", "")
            product_name = product.get("product_name", "")
            trend_keyword = candidate.get("trend_keyword", "")
            
            # ì»¨í…ìŠ¤íŠ¸ ì •ë³´
            time_slot = context.get("time_slot", "") if context else ""
            weather = context.get("weather", {}).get("weather", "") if context else ""
            holiday_name = context.get("holiday_name") if context else None
            competitors = context.get("competitors", []) if context else []
            
            # ê²½ìŸ ìƒí™© ë¶„ì„
            competitor_categories = [comp.get("category_main", "") for comp in competitors]
            has_competition = category in competitor_categories
            
            # ì ìˆ˜ ë¶„ì„ (ì‹¤ì œ ê°€ì¤‘ì¹˜ ê¸°ë°˜)
            if similarity_score >= 0.7:
                # ê³ ìœ ì‚¬ë„: ìœ ì‚¬ë„ 70%, ë§¤ì¶œ 30%
                similarity_ratio = 0.7
                sales_ratio = 0.3
            else:
                # ì €ìœ ì‚¬ë„: ìœ ì‚¬ë„ 30%, ë§¤ì¶œ 70%
                similarity_ratio = 0.3
                sales_ratio = 0.7
            
            # í”„ë¡¬í”„íŠ¸ ë¡œê¹… (ëˆˆì— ë„ê²Œ)
            print("=" * 80)
            print("[LLM í”„ë¡¬í”„íŠ¸] ì¶”ì²œ ê·¼ê±° ìƒì„±")
            print("=" * 80)
            print(f"ìˆœìœ„: {rank}ìœ„ | ì¶”ì²œ íƒ€ì…: {source}")
            print(f"ìƒí’ˆ: {product_name}, ì¹´í…Œê³ ë¦¬: {category}")
            print(f"ìœ ì‚¬ë„: {similarity_score:.3f} | ë§¤ì¶œ: {int(predicted_sales/10000)}ë§Œì› | ìµœì¢…ì ìˆ˜: {final_score:.3f}")
            print(f"ì ìˆ˜ êµ¬ì„±: ìœ ì‚¬ë„ {similarity_ratio*100:.0f}% / ë§¤ì¶œ {sales_ratio*100:.0f}%")
            print(f"ì‹œê°„ëŒ€: {time_slot}, ë‚ ì”¨: {weather}, ê³µíœ´ì¼: {holiday_name or 'ì—†ìŒ'}")
            print("=" * 80)
            
            # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ìƒì„±
            reason_prompt = ChatPromptTemplate.from_messages([
                ("system", """ë‹¹ì‹ ì€ í™ˆì‡¼í•‘ ë°©ì†¡ í¸ì„± ì „ë¬¸ê°€ì…ë‹ˆë‹¤. 
ì£¼ì–´ì§„ ë°ì´í„°ë¥¼ ë°”íƒ•ìœ¼ë¡œ ê° ìƒí’ˆë§ˆë‹¤ ë…ì°½ì ì´ê³  ì„¤ë“ë ¥ ìˆëŠ” ì¶”ì²œ ê·¼ê±°ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

# í•µì‹¬ ì›ì¹™
1. **100ì ì´ë‚´** ê°„ê²°í•˜ê²Œ ì‘ì„±
2. ì „ë¬¸ì ì´ê³  ê°ê´€ì ì¸ í†¤ ìœ ì§€
3. êµ¬ì²´ì ì¸ ìˆ˜ì¹˜ì™€ ë°ì´í„° í™œìš©
4. **ê° ìƒí’ˆë§ˆë‹¤ ì™„ì „íˆ ë‹¤ë¥¸ ê´€ì ê³¼ í‘œí˜„ ì‚¬ìš©**
5. ê°™ì€ íŒ¨í„´ì´ë‚˜ ë¬¸êµ¬ ë°˜ë³µ ì ˆëŒ€ ê¸ˆì§€

# í™œìš© ê°€ëŠ¥í•œ ìš”ì†Œë“¤
- ì˜ˆì¸¡ ë§¤ì¶œ ìˆ˜ì¹˜ (í•„ìˆ˜)
- ì¹´í…Œê³ ë¦¬ íŠ¹ì„± (í•„ìˆ˜)
- ì ìˆ˜ êµ¬ì„± ë¹„ìœ¨ (ìœ ì‚¬ë„ vs ë§¤ì¶œ)
- íŠ¸ë Œë“œ í‚¤ì›Œë“œ (ìˆì„ ê²½ìš°)
- ê³µíœ´ì¼ (ìˆì„ ê²½ìš° í•„ìˆ˜ ì–¸ê¸‰)
- ì‹œê°„ëŒ€ íŠ¹ì„± (ì €ë…/ì˜¤ì „/ì˜¤í›„) - **ì‹ ì¤‘í•˜ê²Œ íŒë‹¨**
  * ì´ ìƒí’ˆ ì¹´í…Œê³ ë¦¬ê°€ í•´ë‹¹ ì‹œê°„ëŒ€ì— ì‹¤ì œë¡œ ì í•©í•œì§€ ìŠ¤ìŠ¤ë¡œ íŒë‹¨í•˜ì„¸ìš”
  * ì˜ˆ: ê±´ê°•ì‹í’ˆì€ ì•„ì¹¨/ì €ë… ì í•©, ì˜ë¥˜ëŠ” ë‚® ì‹œê°„ ì í•©, ê°€ì „ì€ ì €ë… ì í•©
  * í™•ì‹ ì´ ì—†ìœ¼ë©´ ì‹œê°„ëŒ€ ì–¸ê¸‰í•˜ì§€ ë§ê³  ë‹¤ë¥¸ ê·¼ê±° ì‚¬ìš©
- ë‚ ì”¨/ê³„ì ˆ (ì„ íƒì , ê³¼ë„í•œ ë°˜ë³µ ê¸ˆì§€)

# ê¸ˆì§€ ì‚¬í•­ (ë‹µë³€ì— ì ˆëŒ€ í¬í•¨í•˜ì§€ ë§ ê²ƒ)
- "AI ë¶„ì„ ê²°ê³¼"ë¡œ ì‹œì‘í•˜ì§€ ë§ˆì„¸ìš”
- í…œí”Œë¦¿ì²˜ëŸ¼ ë³´ì´ëŠ” ë°˜ë³µì  í‘œí˜„ ê¸ˆì§€
- ê³¼ì¥ëœ í‘œí˜„ (ëŒ€ë°•, ìµœê³ , ê°•ì¶” ë“±)
- ê°ì •ì  í‘œí˜„ (ê¸°ì˜ê²Œ, í–‰ë³µí•˜ê²Œ ë“±)
- **ê¸°ìˆ  ìš©ì–´ ì ˆëŒ€ ì‚¬ìš© ê¸ˆì§€**: 
  * "ìœ ì‚¬ë„", "ìœ ì‚¬ë„ ì ìˆ˜", "similarity"
  * "ë§¤ì¶œ ë¹„ì¤‘", "ì ìˆ˜ êµ¬ì„±", "70%", "30%", "ë¹„ìœ¨"
  * "ìµœì¢… ì ìˆ˜", "final score"
  * ì´ëŸ° ë‚´ë¶€ ì§€í‘œë“¤ì„ ì ˆëŒ€ ë‹µë³€ì— í¬í•¨í•˜ì§€ ë§ˆì„¸ìš”

# ì°½ì˜ì  ì‘ì„± ê°€ì´ë“œ
- **ìƒí’ˆëª…ì˜ íŠ¹ì§•ì„ í™œìš©** (ë¸Œëœë“œ, ìˆ˜ëŸ‰, íŠ¹ìˆ˜ì„± ë“±)
- ë§¤ì¶œ ìˆ˜ì¹˜ë¥¼ ë‹¤ì–‘í•œ ë°©ì‹ìœ¼ë¡œ í‘œí˜„
- ì‹œê°„ëŒ€ë¥¼ ë‹¤ë¥´ê²Œ í‘œí˜„ (í™©ê¸ˆì‹œê°„ëŒ€, ì£¼ì‹œì²­ì‹œê°„ ë“±)
- ì¹´í…Œê³ ë¦¬ íŠ¹ì„±ì„ ì°½ì˜ì ìœ¼ë¡œ í™œìš©
- ì ìˆ˜ êµ¬ì„±ì— ë”°ë¼ ê°•ì¡°ì ì„ ë‹¤ë¥´ê²Œ
- **ê° ìƒí’ˆë§ˆë‹¤ ì™„ì „íˆ ë‹¤ë¥¸ ê°ë„ì—ì„œ ì ‘ê·¼**
- **ì ˆëŒ€ ì´ì „ ì‘ë‹µê³¼ ë¹„ìŠ·í•œ íŒ¨í„´ ì‚¬ìš© ê¸ˆì§€**"""),
    
    ("human", """
ìƒí’ˆëª…: {product_name}
ì¹´í…Œê³ ë¦¬: {category}
ì¶”ì²œ ìˆœìœ„: {rank}ìœ„
ì¶”ì²œ íƒ€ì…: {source}
ì˜ˆì¸¡ ë§¤ì¶œ: {predicted_sales}ë§Œì›
ìœ ì‚¬ë„ ì ìˆ˜: {similarity_score}
ìµœì¢… ì ìˆ˜: {final_score}
ì ìˆ˜ êµ¬ì„±: ìœ ì‚¬ë„ {similarity_ratio}% / ë§¤ì¶œ {sales_ratio}%
ì‹œê°„ëŒ€: {time_slot}
ë‚ ì”¨: {weather}
ê³µíœ´ì¼: {holiday_name}
íŠ¸ë Œë“œ í‚¤ì›Œë“œ: {trend_keyword}

ìœ„ ë°ì´í„°ë¥¼ ë¶„ì„í•˜ì—¬ ì´ ìƒí’ˆë§Œì˜ ë…íŠ¹í•œ ì¶”ì²œ ê·¼ê±°ë¥¼ ì‘ì„±í•˜ì„¸ìš”.

**ì¤‘ìš”:**
- ë‹¤ë¥¸ ìƒí’ˆë“¤ê³¼ ì™„ì „íˆ ë‹¤ë¥¸ ì‹œì‘ ë¬¸êµ¬ ì‚¬ìš©
- ê°™ì€ ë‹¨ì–´ë‚˜ í‘œí˜„ ë°˜ë³µ ê¸ˆì§€
- ê³µíœ´ì¼ì´ ìˆìœ¼ë©´ ë°˜ë“œì‹œ ì–¸ê¸‰
- ì ìˆ˜ êµ¬ì„± ë¹„ìœ¨ì— ë”°ë¼ ê°•ì¡°ì  ë‹¤ë¥´ê²Œ
- 100ì ì´ë‚´ë¡œ ì‘ì„±

ì¶”ì²œ ê·¼ê±°:""")
            ])
            
            chain = reason_prompt | self.llm
            
            result = await chain.ainvoke({
                "product_name": product_name,
                "category": category,
                "rank": rank,
                "source": source,  # "trend_match" ë˜ëŠ” "sales_prediction"
                "predicted_sales": int(predicted_sales/10000) if predicted_sales else "ì—†ìŒ",
                "similarity_score": f"{similarity_score:.3f}",
                "final_score": f"{final_score:.3f}",
                "similarity_ratio": f"{similarity_ratio*100:.0f}",
                "sales_ratio": f"{sales_ratio*100:.0f}",
                "time_slot": time_slot or "ë¯¸ì§€ì •",
                "weather": weather or "ë³´í†µ",
                "holiday_name": holiday_name if holiday_name else "ì—†ìŒ",
                "trend_keyword": trend_keyword or "ì—†ìŒ"
            })
            
            return result.content.strip()
            
        except Exception as e:
            logger.error(f"ë™ì  ê·¼ê±° ìƒì„± ì˜¤ë¥˜: {e}")
            import traceback
            traceback.print_exc()  # ì—ëŸ¬ ìƒì„¸ ë¡œê·¸
            # í´ë°±: ê°„ë‹¨í•œ ê¸°ë³¸ ë©”ì‹œì§€ (í…œí”Œë¦¿ ì•„ë‹Œ)
            return f"{candidate['product'].get('category_main', 'ìƒí’ˆ')} ì¶”ì²œ"
    
    def _prepare_features_for_product(self, product: Dict[str, Any], context: Dict[str, Any]) -> Dict[str, Any]:
        """1ê°œ ìƒí’ˆì˜ XGBoost feature ì¤€ë¹„ (ì˜ˆì¸¡ì€ ì•ˆ í•¨)"""
        broadcast_dt = context["broadcast_dt"]
        
        print(f"=== [_prepare_features_for_product] í˜¸ì¶œë¨: {product.get('product_name', 'Unknown')[:30]} ===")
        
        # ë¡œê·¸ ìŠ¤ì¼€ì¼ë§ ì ìš© (í•™ìŠµ ì‹œì™€ ë™ì¼)
        product_price = product.get("product_price", product.get("price", 100000))
        product_price_log = np.log1p(product_price)
        
        category_main = product.get("category_main", product.get("category", "Unknown"))
        time_slot = context["time_slot"]
        
        return {
            # Numeric features (ë‹¨ìˆœí™”)
            "product_price_log": product_price_log,
            "hour": broadcast_dt.hour,
            "temperature": context["weather"].get("temperature", 20),
            "precipitation": context["weather"].get("precipitation", 0),
            
            # Categorical features
            "product_lgroup": category_main,
            "product_mgroup": product.get("category_middle", "Unknown"),
            "product_sgroup": product.get("category_sub", "Unknown"),
            "brand": product.get("brand", "Unknown"),
            "product_type": product.get("product_type", "ìœ í˜•"),
            "time_slot": time_slot,
            "day_of_week": ["ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ", "í† ", "ì¼"][broadcast_dt.weekday()],
            "season": context["season"],
            "weather": context["weather"].get("weather", "Clear"),
            
            # Boolean features
            "is_weekend": 1 if broadcast_dt.weekday() >= 5 else 0,
            "is_holiday": 0
        }
    
    async def _predict_product_sales(self, product: Dict[str, Any], context: Dict[str, Any]) -> float:
        """ê°œë³„ ìƒí’ˆ XGBoost ë§¤ì¶œ ì˜ˆì¸¡"""
        try:
            import pandas as pd
            
            # Feature ì¤€ë¹„
            features = self._prepare_features_for_product(product, context)
            product_data = pd.DataFrame([features])
            
            logger.info(f"=== XGBoost ë§¤ì¶œ ì˜ˆì¸¡ ì…ë ¥ ë°ì´í„° ===")
            logger.info(f"ìƒí’ˆ: {product.get('product_name', 'Unknown')}")
            logger.info(f"ì¹´í…Œê³ ë¦¬: {product.get('category_main', 'Unknown')}")
            logger.info(f"ê°€ê²©: {product.get('product_price', 100000):,}ì›")
            logger.info(f"ê³¼ê±° í‰ê·  ë§¤ì¶œ: {product.get('avg_sales', 30000000):,}ì›")
            logger.info(f"ë°©ì†¡ ì‹œê°„: {context['broadcast_dt'].hour}ì‹œ")
            logger.info(f"ë‚ ì”¨: {context['weather'].get('weather', 'Clear')}, {context['weather'].get('temperature', 20)}Â°C")
            
            # XGBoost íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì˜ˆì¸¡ (ì „ì²˜ë¦¬ í¬í•¨)
            predicted_sales_log = self.model.predict(product_data)[0]
            # ë¡œê·¸ ì—­ë³€í™˜ (í•™ìŠµ ì‹œ log1p ì‚¬ìš©)
            predicted_sales = np.expm1(predicted_sales_log)
            logger.info(f"=== XGBoost ì˜ˆì¸¡ ê²°ê³¼ ===")
            logger.info(f"ì˜ˆì¸¡ ë§¤ì¶œ: {predicted_sales:,.0f}ì› ({predicted_sales/100000000:.2f}ì–µ)")
            
            return float(predicted_sales)
            
        except Exception as e:
            logger.error(f"ìƒí’ˆ ë§¤ì¶œ ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
            logger.error(f"ìƒí’ˆ ì •ë³´: {product.get('product_name', 'Unknown')}")
            import traceback
            logger.error(f"ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
            return 30000000  # ê¸°ë³¸ê°’ (0.3ì–µ)
    
    async def _predict_products_sales_batch(self, products: List[Dict[str, Any]], context: Dict[str, Any]) -> List[float]:
        """ì—¬ëŸ¬ ìƒí’ˆ XGBoost ë§¤ì¶œ ì˜ˆì¸¡ (ë°°ì¹˜ ì²˜ë¦¬)"""
        try:
            import pandas as pd
            
            if not products:
                return []
            
            # ëª¨ë“  ìƒí’ˆì˜ featuresë¥¼ í•œ ë²ˆì— ì¤€ë¹„
            features_list = [
                self._prepare_features_for_product(product, context)
                for product in products
            ]
            
            batch_df = pd.DataFrame(features_list)
            
            print(f"=== [ë°°ì¹˜ ì˜ˆì¸¡] {len(products)}ê°œ ìƒí’ˆ ì¼ê´„ ì˜ˆì¸¡ ì‹œì‘ ===")
            
            # ì…ë ¥ í”¼ì²˜ ìƒ˜í”Œ ì¶œë ¥ (ë””ë²„ê¹…ìš©)
            print(f"=== [ì…ë ¥ í”¼ì²˜ ìƒ˜í”Œ] ===")
            for i, (product, features) in enumerate(zip(products[:3], features_list[:3])):
                print(f"  ìƒí’ˆ {i+1}: {product.get('product_name', '')[:30]}")
                print(f"    - product_price_log: {features['product_price_log']:.2f}")
                print(f"    - hour: {features['hour']}")
                print(f"    - ì¹´í…Œê³ ë¦¬: {features['product_lgroup']}")
            
            # XGBoost ë°°ì¹˜ ì˜ˆì¸¡ (í•œ ë²ˆì— ì²˜ë¦¬)
            predicted_sales_log = self.model.predict(batch_df)
            # ë¡œê·¸ ì—­ë³€í™˜ (í•™ìŠµ ì‹œ log1p ì‚¬ìš©)
            predicted_sales_array = np.expm1(predicted_sales_log)
            
            print(f"=== [ë°°ì¹˜ ì˜ˆì¸¡] ì™„ë£Œ ===")
            print(f"  í‰ê· : {predicted_sales_array.mean()/10000:.0f}ë§Œì›")
            print(f"  ìµœì†Œ: {predicted_sales_array.min()/10000:.0f}ë§Œì›")
            print(f"  ìµœëŒ€: {predicted_sales_array.max()/10000:.0f}ë§Œì›")
            print(f"  í‘œì¤€í¸ì°¨: {predicted_sales_array.std()/10000:.0f}ë§Œì›")
            
            # ì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ ì¶œë ¥
            print(f"=== [ì˜ˆì¸¡ ê²°ê³¼ ìƒ˜í”Œ] ===")
            for i, (product, sales) in enumerate(zip(products[:5], predicted_sales_array[:5])):
                print(f"  {i+1}. {product.get('product_name', '')[:30]:30s} â†’ {sales/10000:.0f}ë§Œì›")
            
            return [float(sales) for sales in predicted_sales_array]
            
        except Exception as e:
            logger.error(f"ë°°ì¹˜ ë§¤ì¶œ ì˜ˆì¸¡ ì˜¤ë¥˜: {e}")
            import traceback
            logger.error(f"ìƒì„¸ ì—ëŸ¬:\n{traceback.format_exc()}")
            # ê¸°ë³¸ê°’ ë°˜í™˜
            return [30000000.0] * len(products)
    
    async def _get_all_categories_from_db(self) -> List[str]:
        """PostgreSQLì—ì„œ ëª¨ë“  ì¹´í…Œê³ ë¦¬ ì¡°íšŒ"""
        try:
            query = text("""
                SELECT DISTINCT category_main
                FROM broadcast_training_dataset
                WHERE category_main IS NOT NULL
                ORDER BY category_main
            """)
            
            with self.engine.connect() as conn:
                result = conn.execute(query).fetchall()
            
            categories = [row[0] for row in result]
            return categories
            
        except Exception as e:
            logger.error(f"ì „ì²´ ì¹´í…Œê³ ë¦¬ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    async def _get_ace_products_from_category(self, category: str, limit: int = 5) -> List[Dict[str, Any]]:
        """ì¹´í…Œê³ ë¦¬ë³„ ì—ì´ìŠ¤ ìƒí’ˆ ì¡°íšŒ"""
        try:
            query = text("""
                SELECT product_code, product_name, category_main, category_middle, category_sub,
                       AVG(gross_profit) as avg_sales, COUNT(*) as broadcast_count,
                       tape_code, tape_name, MAX(price) as price, brand
                FROM broadcast_training_dataset 
                WHERE category_main = :category
                GROUP BY product_code, product_name, category_main, category_middle, category_sub,
                         tape_code, tape_name, brand
                ORDER BY avg_sales DESC 
                LIMIT :limit
            """)
            
            with self.engine.connect() as conn:
                result = conn.execute(query, {"category": category, "limit": limit}).fetchall()
                
            products = []
            for row in result:
                products.append({
                    "product_code": row[0],
                    "product_name": row[1],
                    "category_main": row[2],
                    "category_middle": row[3],
                    "category_sub": row[4],
                    "avg_sales": float(row[5]),
                    "broadcast_count": int(row[6]),
                    "tape_code": row[7],
                    "tape_name": row[8],
                    "price": float(row[9]) if row[9] else None,
                    "brand": row[10] if len(row) > 10 else None
                })
            
            return products
            
        except Exception as e:
            logger.error(f"ì—ì´ìŠ¤ ìƒí’ˆ ì¡°íšŒ ì˜¤ë¥˜: {e}")
            return []
    
    def _remove_duplicates(self, candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """ì¤‘ë³µ ì œê±° - ê°™ì€ ìƒí’ˆì½”ë“œ ë° ê°™ì€ (ì†Œë¶„ë¥˜ + ë¸Œëœë“œ) ì¡°í•© ì œê±°"""
        seen_products = set()
        seen_category_brand_pairs = set()  # (ì†Œë¶„ë¥˜, ë¸Œëœë“œ) ì¡°í•©
        unique_candidates = []
        
        for candidate in candidates:
            product_code = candidate.get("product_code", "")
            category_sub = candidate.get("category_sub", "")
            brand = candidate.get("brand", "")
            
            # ìƒí’ˆì½”ë“œ ì¤‘ë³µ ì²´í¬
            if product_code and product_code in seen_products:
                continue
            
            # ì†Œë¶„ë¥˜ + ë¸Œëœë“œ ì¡°í•© ì¤‘ë³µ ì²´í¬
            category_brand_key = (category_sub, brand)
            if category_sub and brand and category_brand_key in seen_category_brand_pairs:
                logger.info(f"ì†Œë¶„ë¥˜+ë¸Œëœë“œ ì¤‘ë³µ ì œì™¸: {candidate.get('product_name', '')} (ì†Œë¶„ë¥˜: {category_sub}, ë¸Œëœë“œ: {brand})")
                continue
            
            # í†µê³¼í•œ ê²½ìš° ì¶”ê°€
            if product_code:
                seen_products.add(product_code)
            if category_sub and brand:
                seen_category_brand_pairs.add(category_brand_key)
            unique_candidates.append(candidate)
        
        logger.info(f"ì¤‘ë³µ ì œê±° ì™„ë£Œ: {len(candidates)}ê°œ â†’ {len(unique_candidates)}ê°œ (ì†Œë¶„ë¥˜+ë¸Œëœë“œ ë‹¤ì–‘ì„± ë³´ì¥)")
        return unique_candidates
    
    def _rank_candidates(self, candidates: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """í›„ë³´ ë­í‚¹"""
        return sorted(candidates, key=lambda x: x.get("final_score", 0), reverse=True)
    
    def _get_time_slot(self, dt: datetime) -> str:
        """ì‹œê°„ëŒ€ ë¶„ë¥˜"""
        hour = dt.hour
        if 6 <= hour < 9:
            return "ì•„ì¹¨"
        elif 9 <= hour < 12:
            return "ì˜¤ì „"
        elif 12 <= hour < 14:
            return "ì ì‹¬"
        elif 14 <= hour < 18:
            return "ì˜¤í›„"
        elif 18 <= hour < 22:
            return "ì €ë…"
        else:
            return "ì•¼ê°„"
    
    def _get_season(self, month: int) -> str:
        """ê³„ì ˆ ë¶„ë¥˜"""
        if 3 <= month <= 5:
            return "ë´„"
        elif 6 <= month <= 8:
            return "ì—¬ë¦„"
        elif 9 <= month <= 11:
            return "ê°€ì„"
        else:
            return "ê²¨ìš¸"
