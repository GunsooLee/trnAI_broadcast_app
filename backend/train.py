import os
import sys
import datetime as dt
from pathlib import Path

import joblib
import numpy as np
import pandas as pd
from dotenv import load_dotenv
from sqlalchemy import create_engine, Engine
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from xgboost import XGBRegressor

# FastAPI ì•±ê³¼ ë™ì¼í•œ ìœ„ì¹˜ì˜ tokenizer_utilsë¥¼ ì°¸ì¡°í•  ìˆ˜ ìˆë„ë¡ ê²½ë¡œ ì¶”ê°€
# ì´ë ‡ê²Œ í•˜ë©´ app.tokenizer_utils í˜•íƒœê°€ ì•„ë‹Œ tokenizer_utilsë¡œ ë°”ë¡œ ì„í¬íŠ¸ ê°€ëŠ¥
sys.path.append(str(Path(__file__).parent / 'app'))
from tokenizer_utils import mecab_tokenizer

# --- ìƒìˆ˜ ì •ì˜ ---
MODEL_FILE_PROFIT = "xgb_broadcast_profit.joblib"
MODEL_FILE_EFFICIENCY = "xgb_broadcast_efficiency.joblib"
TABLE_NAME = "broadcast_training_dataset"

# --- DB ë° í™˜ê²½ë³€ìˆ˜ ì„¤ì • ---
def get_db_engine():
    """ìƒˆë¡œìš´ DB ì—”ì§„ì„ ìƒì„±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
    load_dotenv()
    db_uri = os.getenv("DB_URI") or os.getenv("POSTGRES_URI")
    
    # Docker ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œëŠ” í˜¸ìŠ¤íŠ¸ëª…ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
    if not db_uri:
        db_uri = "postgresql://TRN_AI:TRN_AI@trnAi_postgres:5432/TRNAI_DB"
    
    return create_engine(db_uri)

# --- ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ---
def _weekday_kr(date: dt.date) -> str:
    return ["ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ", "í† ", "ì¼"][date.weekday()]

def _time_slot(hour: int) -> str:
    if 6 <= hour < 12:
        return "ì˜¤ì „"
    elif 12 <= hour < 18:
        return "ì˜¤í›„"
    elif 18 <= hour < 24:
        return "ì €ë…"
    else:
        return "ì‹¬ì•¼"

def load_data(engine: Engine) -> pd.DataFrame:
    """DBì—ì„œ í•™ìŠµìš© ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ê¸°ë³¸ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    print("ë°ì´í„° ë¡œë”© ì‹œì‘...")

    # broadcast_training_datasetì—ì„œ ì§ì ‘ ë°ì´í„° ë¡œë“œ
    query = f"""
    WITH base AS (
        SELECT
            product_code,
            category_main as product_lgroup,
            category_middle as product_mgroup,
            category_sub as product_sgroup,
            product_name,
            brand,
            product_type,
            time_slot,
            day_of_week,
            season,
            is_weekend,
            hour,
            weather,
            temperature,
            precipitation,
            is_holiday,
            holiday_name,
            gross_profit,
            sales_efficiency,
            price as product_price,
            broadcast_date
        FROM broadcast_training_dataset
        WHERE gross_profit IS NOT NULL
    ),
    product_stats AS (
        SELECT
            product_code,
            AVG(gross_profit) AS product_avg_profit,
            COUNT(*) AS product_broadcast_count
        FROM base
        GROUP BY product_code
    ),
    category_timeslot_stats AS (
        SELECT
            product_mgroup,
            time_slot,
            AVG(gross_profit) AS category_timeslot_avg_profit
        FROM base
        GROUP BY product_mgroup, time_slot
    )
    SELECT
        b.*,
        ps.product_avg_profit,
        ps.product_broadcast_count,
        cts.category_timeslot_avg_profit
    FROM base b
    LEFT JOIN product_stats ps ON b.product_code = ps.product_code
    LEFT JOIN category_timeslot_stats cts ON b.product_mgroup = cts.product_mgroup AND b.time_slot = cts.time_slot
    """

    df = pd.read_sql(query, engine)
    
    # NULL ê°’ ì²˜ë¦¬
    df['product_avg_profit'] = df['product_avg_profit'].fillna(0)
    df['category_timeslot_avg_profit'] = df['category_timeslot_avg_profit'].fillna(0)
    df['product_broadcast_count'] = df['product_broadcast_count'].fillna(0)
    df['temperature'] = df['temperature'].fillna(df['temperature'].mean())
    df['precipitation'] = df['precipitation'].fillna(0)
    df['weather'] = df['weather'].fillna('Clear')
    df['brand'] = df['brand'].fillna('Unknown')
    df['product_type'] = df['product_type'].fillna('ìœ í˜•')
    df['holiday_name'] = df['holiday_name'].fillna('')
    
    print(f"ë°ì´í„° ë¡œë”© ì™„ë£Œ. ì´ {len(df)}ê°œ í–‰")
    return df

# --- ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ë¹Œë“œ ---
def build_pipeline() -> Pipeline:
    """Scikit-learn íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤."""
    print("ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ìƒì„±...")
    numeric_features = [
        "product_price",
        "product_avg_profit",
        "product_broadcast_count",
        "category_timeslot_avg_profit",
        "hour",
        "temperature",
        "precipitation",
    ]
    categorical_features = [
        "product_lgroup",
        "product_mgroup",
        "product_sgroup",
        "brand",
        "product_type",
        "time_slot",
        "day_of_week",
        "season",
        "weather",
    ]
    boolean_features = ["is_weekend", "is_holiday"]

    preprocessor = ColumnTransformer(
        [
            ("num", "passthrough", numeric_features),
            ("bool", "passthrough", boolean_features),
            ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_features),
        ],
        remainder="drop",
    )

    model = XGBRegressor(
        n_estimators=500,
        learning_rate=0.05,
        max_depth=6,
        subsample=0.8,
        colsample_bytree=0.8,
        random_state=42,
    )

    return Pipeline([("pre", preprocessor), ("model", model)])

# --- ëª¨ë¸ í•™ìŠµ ì‹¤í–‰ ---
def train() -> dict:
    """ì „ì²´ ëª¨ë¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤. (2ê°œ íƒ€ê²Ÿ: gross_profit, sales_efficiency)
    
    Returns:
        dict: í•™ìŠµ ê²°ê³¼ í†µê³„
    """
    import time
    start_time = time.time()
    
    engine = get_db_engine()
    df = load_data(engine)
    
    training_stats = {
        "total_records": len(df),
        "models": {}
    }

    # ê³µí†µ ì œê±° ì»¬ëŸ¼ (íƒ€ê²Ÿ ë³€ìˆ˜ ì œì™¸)
    common_drop_cols = [
        "product_code",
        "product_name",
        "holiday_name",  # is_holidayë¡œ ì¶©ë¶„
        "broadcast_date",  # ë‚ ì§œëŠ” í”¼ì²˜ë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (day_of_week, seasonìœ¼ë¡œ ëŒ€ì²´)
    ]
    
    # ========================================
    # ëª¨ë¸ 1: gross_profit ì˜ˆì¸¡ ëª¨ë¸
    # ========================================
    print("\n" + "="*60)
    print("ëª¨ë¸ 1: ë§¤ì¶œì´ì´ìµ(gross_profit) ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ")
    print("="*60)
    
    target1 = "gross_profit"
    drop_cols1 = common_drop_cols + ["sales_efficiency", target1]
    existing_drop_cols1 = [col for col in drop_cols1 if col in df.columns]
    
    X1 = df.drop(columns=existing_drop_cols1)
    y1 = df[target1]

    X1_train, X1_test, y1_train, y1_test = train_test_split(
        X1, y1, test_size=0.2, random_state=42
    )

    print("ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    pipe1 = build_pipeline()
    pipe1.fit(X1_train, y1_train)
    print("ëª¨ë¸ í•™ìŠµ ì™„ë£Œ.")

    y1_pred = pipe1.predict(X1_test)
    mae1 = mean_absolute_error(y1_test, y1_pred)
    rmse1 = np.sqrt(mean_squared_error(y1_test, y1_pred))
    r2_1 = r2_score(y1_test, y1_pred)
    
    print("\n=== ëª¨ë¸ 1 í‰ê°€ (gross_profit) ===")
    print(f"MAE : {mae1:,.2f} ì›")
    print(f"RMSE: {rmse1:,.2f} ì›")
    print(f"R2  : {r2_1:.4f}\n")

    # ëª¨ë¸ 1 ì €ì¥
    model_path1 = Path(__file__).parent / 'app' / MODEL_FILE_PROFIT
    joblib.dump(pipe1, model_path1)
    print(f"âœ… ëª¨ë¸ 1ì´ '{model_path1}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # í†µê³„ ì €ì¥
    training_stats["models"]["profit_model"] = {
        "train_records": len(X1_train),
        "test_records": len(X1_test),
        "mae": round(mae1, 2),
        "rmse": round(rmse1, 2),
        "r2_score": round(r2_1, 4)
    }

    # ========================================
    # ëª¨ë¸ 2: sales_efficiency ì˜ˆì¸¡ ëª¨ë¸
    # ========================================
    print("\n" + "="*60)
    print("ëª¨ë¸ 2: ë§¤ì¶œíš¨ìœ¨(sales_efficiency) ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ")
    print("="*60)
    
    target2 = "sales_efficiency"
    
    # sales_efficiencyê°€ NULLì¸ í–‰ ì œê±°
    df_efficiency = df[df[target2].notna()].copy()
    print(f"sales_efficiency ìœ íš¨ ë°ì´í„°: {len(df_efficiency)}ê°œ í–‰")
    
    drop_cols2 = common_drop_cols + ["gross_profit", target2]
    existing_drop_cols2 = [col for col in drop_cols2 if col in df_efficiency.columns]
    
    X2 = df_efficiency.drop(columns=existing_drop_cols2)
    y2 = df_efficiency[target2]

    X2_train, X2_test, y2_train, y2_test = train_test_split(
        X2, y2, test_size=0.2, random_state=42
    )

    print("ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    pipe2 = build_pipeline()
    pipe2.fit(X2_train, y2_train)
    print("ëª¨ë¸ í•™ìŠµ ì™„ë£Œ.")

    y2_pred = pipe2.predict(X2_test)
    mae2 = mean_absolute_error(y2_test, y2_pred)
    rmse2 = np.sqrt(mean_squared_error(y2_test, y2_pred))
    r2_2 = r2_score(y2_test, y2_pred)
    
    print("\n=== ëª¨ë¸ 2 í‰ê°€ (sales_efficiency) ===")
    print(f"MAE : {mae2:,.2f} ì›/ë¶„")
    print(f"RMSE: {rmse2:,.2f} ì›/ë¶„")
    print(f"R2  : {r2_2:.4f}\n")

    # ëª¨ë¸ 2 ì €ì¥
    model_path2 = Path(__file__).parent / 'app' / MODEL_FILE_EFFICIENCY
    joblib.dump(pipe2, model_path2)
    print(f"âœ… ëª¨ë¸ 2ê°€ '{model_path2}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # í†µê³„ ì €ì¥
    training_stats["models"]["efficiency_model"] = {
        "train_records": len(X2_train),
        "test_records": len(X2_test),
        "mae": round(mae2, 2),
        "rmse": round(rmse2, 2),
        "r2_score": round(r2_2, 4)
    }
    
    # ì´ ì†Œìš” ì‹œê°„
    elapsed_time = time.time() - start_time
    training_stats["training_time_seconds"] = round(elapsed_time, 2)
    
    print("\n" + "="*60)
    print("ğŸ‰ ì „ì²´ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
    print(f"â±ï¸  ì´ ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
    print("="*60)
    
    return training_stats

if __name__ == "__main__":
    train()
