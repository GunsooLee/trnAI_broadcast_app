import os
import sys
import datetime as dt
from pathlib import Path

import joblib
import numpy as np
import pandas as pd
from dotenv import load_dotenv
from sqlalchemy import create_engine, Engine
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.preprocessing import OneHotEncoder
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
from xgboost import XGBRegressor

# FastAPI ì•±ê³¼ ë™ì¼í•œ ìœ„ì¹˜ì˜ tokenizer_utilsë¥¼ ì°¸ì¡°í•  ìˆ˜ ìˆë„ë¡ ê²½ë¡œ ì¶”ê°€
# ì´ë ‡ê²Œ í•˜ë©´ app.tokenizer_utils í˜•íƒœê°€ ì•„ë‹Œ tokenizer_utilsë¡œ ë°”ë¡œ ì„í¬íŠ¸ ê°€ëŠ¥
sys.path.append(str(Path(__file__).parent / 'app'))
from tokenizer_utils import mecab_tokenizer

# --- ìƒìˆ˜ ì •ì˜ ---
MODEL_FILE_PROFIT = "xgb_broadcast_profit.joblib"
MODEL_FILE_EFFICIENCY = "xgb_broadcast_efficiency.joblib"
TABLE_NAME = "broadcast_training_dataset"

# --- DB ë° í™˜ê²½ë³€ìˆ˜ ì„¤ì • ---
def get_db_engine():
    """ìƒˆë¡œìš´ DB ì—”ì§„ì„ ìƒì„±í•˜ì—¬ ë°˜í™˜í•©ë‹ˆë‹¤."""
    load_dotenv()
    db_uri = os.getenv("DB_URI") or os.getenv("POSTGRES_URI")
    
    # Docker ì»¨í…Œì´ë„ˆ ë‚´ë¶€ì—ì„œëŠ” í˜¸ìŠ¤íŠ¸ëª…ì´ ë‹¤ë¥¼ ìˆ˜ ìˆìŒ
    if not db_uri:
        db_uri = "postgresql://TRN_AI:TRN_AI@trnAi_postgres:5432/TRNAI_DB"
    
    return create_engine(db_uri)

# --- ë°ì´í„° ë¡œë”© ë° ì „ì²˜ë¦¬ ---
def _weekday_kr(date: dt.date) -> str:
    return ["ì›”", "í™”", "ìˆ˜", "ëª©", "ê¸ˆ", "í† ", "ì¼"][date.weekday()]

def _time_slot(hour: int) -> str:
    if 6 <= hour < 12:
        return "ì˜¤ì „"
    elif 12 <= hour < 18:
        return "ì˜¤í›„"
    elif 18 <= hour < 24:
        return "ì €ë…"
    else:
        return "ì‹¬ì•¼"

def load_data(engine: Engine) -> pd.DataFrame:
    """DBì—ì„œ í•™ìŠµìš© ë°ì´í„°ë¥¼ ë¡œë“œí•˜ê³  ê¸°ë³¸ ì „ì²˜ë¦¬ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤."""
    print("ë°ì´í„° ë¡œë”© ì‹œì‘...")

    # broadcast_training_datasetì—ì„œ ì§ì ‘ ë°ì´í„° ë¡œë“œ (ë‹¨ìˆœí™”)
    query = f"""
    SELECT
        product_code,
        category_main as product_lgroup,
        category_middle as product_mgroup,
        category_sub as product_sgroup,
        product_name,
        brand,
        product_type,
        time_slot,
        day_of_week,
        season,
        is_weekend,
        hour,
        weather,
        temperature,
        precipitation,
        is_holiday,
        holiday_name,
        gross_profit,
        sales_efficiency,
        price as product_price,
        broadcast_date
    FROM broadcast_training_dataset
    WHERE gross_profit IS NOT NULL
      AND gross_profit <= 50000000  -- ì´ìƒì¹˜ ì œê±°: 5ì²œë§Œì› ì´í•˜ë§Œ
      AND gross_profit >= 1000000   -- í•˜ìœ„ ì´ìƒì¹˜ ì œê±°: 100ë§Œì› ì´ìƒë§Œ
    """

    df = pd.read_sql(query, engine)
    
    # NULL ê°’ ì²˜ë¦¬
    df['temperature'] = df['temperature'].fillna(df['temperature'].mean())
    df['precipitation'] = df['precipitation'].fillna(0)
    df['weather'] = df['weather'].fillna('Clear')
    df['brand'] = df['brand'].fillna('Unknown')
    df['product_type'] = df['product_type'].fillna('ìœ í˜•')
    df['holiday_name'] = df['holiday_name'].fillna('')
    
    # ê³¼ëŒ€ì˜ˆì¸¡ ë°©ì§€: ê°€ê²© ë¡œê·¸ ìŠ¤ì¼€ì¼ë§
    print("ê°€ê²© í”¼ì²˜ ë¡œê·¸ ìŠ¤ì¼€ì¼ë§ ì ìš©...")
    df['product_price_log'] = np.log1p(df['product_price'])
    print(f"  product_price: ì›ë³¸ í‰ê·  {df['product_price'].mean():,.0f}ì› â†’ ë¡œê·¸ {df['product_price_log'].mean():.2f}")
    
    print(f"ë°ì´í„° ë¡œë”© ì™„ë£Œ. ì´ {len(df)}ê°œ í–‰")
    return df

# --- ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ë¹Œë“œ ---
def build_pipeline() -> Pipeline:
    """Scikit-learn íŒŒì´í”„ë¼ì¸ì„ êµ¬ì¶•í•©ë‹ˆë‹¤."""
    print("ëª¨ë¸ íŒŒì´í”„ë¼ì¸ ìƒì„±...")
    numeric_features = [
        "product_broadcast_count",
        "category_timeslot_avg_profit_log",  # ë¡œê·¸ ìŠ¤ì¼€ì¼ë§ ë²„ì „ ì‚¬ìš©
        "product_price_log",  # ë¡œê·¸ ìŠ¤ì¼€ì¼ë§ ë²„ì „ ì‚¬ìš©
        "hour",
        "temperature",
        "precipitation",
    ]
    categorical_features = [
        "product_lgroup",
        "product_mgroup",
        "product_sgroup",
        "brand",
        "product_type",
        "time_slot",
        "day_of_week",
        "season",
        "weather",
    ]
    boolean_features = ["is_weekend", "is_holiday"]

    preprocessor = ColumnTransformer(
        [
            ("num", "passthrough", numeric_features),
            ("bool", "passthrough", boolean_features),
            ("cat", OneHotEncoder(handle_unknown="ignore"), categorical_features),
        ],
        remainder="drop",
    )

    model = XGBRegressor(
        n_estimators=500,
        learning_rate=0.05,
        max_depth=4,  # 6 â†’ 4 (ê³¼ì í•© ë°©ì§€)
        min_child_weight=5,  # 1 â†’ 5 (ê³¼ì í•© ë°©ì§€)
        gamma=0.2,  # ë¶„í•  ìµœì†Œ ì†ì‹¤ ê°ì†Œ
        subsample=0.8,
        colsample_bytree=0.8,
        reg_alpha=0.5,  # L1 ì •ê·œí™”
        reg_lambda=2.0,  # L2 ì •ê·œí™” ê°•í™”
        random_state=42,
    )

    return Pipeline([("pre", preprocessor), ("model", model)])

# --- ëª¨ë¸ í•™ìŠµ ì‹¤í–‰ ---
def train() -> dict:
    """ì „ì²´ ëª¨ë¸ í•™ìŠµ íŒŒì´í”„ë¼ì¸ì„ ì‹¤í–‰í•©ë‹ˆë‹¤. (2ê°œ íƒ€ê²Ÿ: gross_profit, sales_efficiency)
    
    Returns:
        dict: í•™ìŠµ ê²°ê³¼ í†µê³„
    """
    import time
    start_time = time.time()
    
    engine = get_db_engine()
    df = load_data(engine)
    
    training_stats = {
        "total_records": len(df),
        "models": {}
    }

    # ê³µí†µ ì œê±° ì»¬ëŸ¼ (íƒ€ê²Ÿ ë³€ìˆ˜ ì œì™¸)
    common_drop_cols = [
        "product_code",
        "product_name",
        "holiday_name",  # is_holidayë¡œ ì¶©ë¶„
        "broadcast_date",  # ë‚ ì§œëŠ” í”¼ì²˜ë¡œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ (day_of_week, seasonìœ¼ë¡œ ëŒ€ì²´)
    ]
    
    # ========================================
    # ëª¨ë¸ 1: gross_profit ì˜ˆì¸¡ ëª¨ë¸
    # ========================================
    print("\n" + "="*60)
    print("ëª¨ë¸ 1: ë§¤ì¶œì´ì´ìµ(gross_profit) ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ")
    print("="*60)
    
    target1 = "gross_profit"
    drop_cols1 = common_drop_cols + ["sales_efficiency", target1]
    existing_drop_cols1 = [col for col in drop_cols1 if col in df.columns]
    
    X1 = df.drop(columns=existing_drop_cols1)
    y1 = df[target1]
    
    # ë¡œê·¸ ë³€í™˜ ì ìš© (ê³¼ëŒ€ ì˜ˆì¸¡ ë°©ì§€)
    print("íƒ€ê²Ÿ ë³€ìˆ˜ ë¡œê·¸ ë³€í™˜ ì ìš©...")
    y1_log = np.log1p(y1)  # log(1 + y)
    print(f"  ì›ë³¸ í‰ê· : {y1.mean():,.0f}ì›, ë¡œê·¸ í‰ê· : {y1_log.mean():.2f}")

    X1_train, X1_test, y1_train_log, y1_test_log = train_test_split(
        X1, y1_log, test_size=0.2, random_state=42
    )
    
    # ì›ë³¸ y ê°’ë„ ë¶„ë¦¬ (í‰ê°€ìš©)
    _, _, y1_train_orig, y1_test_orig = train_test_split(
        X1, y1, test_size=0.2, random_state=42
    )

    print("ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    pipe1 = build_pipeline()
    pipe1.fit(X1_train, y1_train_log)
    print("ëª¨ë¸ í•™ìŠµ ì™„ë£Œ.")

    # ì˜ˆì¸¡ í›„ ì—­ë³€í™˜
    y1_pred_log = pipe1.predict(X1_test)
    y1_pred = np.expm1(y1_pred_log)  # exp(y) - 1
    
    mae1 = mean_absolute_error(y1_test_orig, y1_pred)
    rmse1 = np.sqrt(mean_squared_error(y1_test_orig, y1_pred))
    r2_1 = r2_score(y1_test_orig, y1_pred)
    
    print("\n=== ëª¨ë¸ 1 í‰ê°€ (gross_profit) ===")
    print(f"MAE : {mae1:,.2f} ì›")
    print(f"RMSE: {rmse1:,.2f} ì›")
    print(f"R2  : {r2_1:.4f}\n")

    # ëª¨ë¸ 1 ì €ì¥
    model_path1 = Path(__file__).parent / 'app' / MODEL_FILE_PROFIT
    joblib.dump(pipe1, model_path1)
    print(f"âœ… ëª¨ë¸ 1ì´ '{model_path1}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    
    # í†µê³„ ì €ì¥
    training_stats["models"]["profit_model"] = {
        "train_records": len(X1_train),
        "test_records": len(X1_test),
        "mae": round(mae1, 2),
        "rmse": round(rmse1, 2),
        "r2_score": round(r2_1, 4)
    }

    # ========================================
    # ëª¨ë¸ 2: sales_efficiency ì˜ˆì¸¡ ëª¨ë¸ (ì‚¬ìš© ì•ˆ í•¨ - ì£¼ì„ ì²˜ë¦¬)
    # ========================================
    # R2 Scoreê°€ 0.38ë¡œ ë§¤ìš° ë‚®ê³ , í˜„ì¬ APIì—ì„œ ì‚¬ìš©í•˜ì§€ ì•ŠìŒ
    # í•„ìš” ì‹œ ì£¼ì„ í•´ì œí•˜ì—¬ ì¬í™œì„±í™” ê°€ëŠ¥
    
    print("\nâ­ï¸  ëª¨ë¸ 2 (sales_efficiency) í•™ìŠµ ìŠ¤í‚µ (ì‚¬ìš© ì•ˆ í•¨)")
    
    # í†µê³„ ì €ì¥ (ë¹ˆ ê°’)
    training_stats["models"]["efficiency_model"] = {
        "train_records": 0,
        "test_records": 0,
        "mae": 0,
        "rmse": 0,
        "r2_score": 0,
        "status": "skipped"
    }
    
    # # ì•„ë˜ ì½”ë“œëŠ” í•„ìš” ì‹œ ì£¼ì„ í•´ì œ
    # print("\n" + "="*60)
    # print("ëª¨ë¸ 2: ë§¤ì¶œíš¨ìœ¨(sales_efficiency) ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ")
    # print("="*60)
    # 
    # target2 = "sales_efficiency"
    # 
    # # sales_efficiencyê°€ NULLì¸ í–‰ ì œê±°
    # df_efficiency = df[df[target2].notna()].copy()
    # print(f"sales_efficiency ìœ íš¨ ë°ì´í„°: {len(df_efficiency)}ê°œ í–‰")
    # 
    # drop_cols2 = common_drop_cols + ["gross_profit", target2]
    # existing_drop_cols2 = [col for col in drop_cols2 if col in df_efficiency.columns]
    # 
    # X2 = df_efficiency.drop(columns=existing_drop_cols2)
    # y2 = df_efficiency[target2]
    #
    # X2_train, X2_test, y2_train, y2_test = train_test_split(
    #     X2, y2, test_size=0.2, random_state=42
    # )
    #
    # print("ëª¨ë¸ í•™ìŠµ ì‹œì‘...")
    # pipe2 = build_pipeline()
    # pipe2.fit(X2_train, y2_train)
    # print("ëª¨ë¸ í•™ìŠµ ì™„ë£Œ.")
    #
    # y2_pred = pipe2.predict(X2_test)
    # mae2 = mean_absolute_error(y2_test, y2_pred)
    # rmse2 = np.sqrt(mean_squared_error(y2_test, y2_pred))
    # r2_2 = r2_score(y2_test, y2_pred)
    # 
    # print("\n=== ëª¨ë¸ 2 í‰ê°€ (sales_efficiency) ===")
    # print(f"MAE : {mae2:,.2f} ì›/ë¶„")
    # print(f"RMSE: {rmse2:,.2f} ì›/ë¶„")
    # print(f"R2  : {r2_2:.4f}\n")
    #
    # # ëª¨ë¸ 2 ì €ì¥
    # model_path2 = Path(__file__).parent / 'app' / MODEL_FILE_EFFICIENCY
    # joblib.dump(pipe2, model_path2)
    # print(f"âœ… ëª¨ë¸ 2ê°€ '{model_path2}'ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.")
    # 
    # # í†µê³„ ì €ì¥
    # training_stats["models"]["efficiency_model"] = {
    #     "train_records": len(X2_train),
    #     "test_records": len(X2_test),
    #     "mae": round(mae2, 2),
    #     "rmse": round(rmse2, 2),
    #     "r2_score": round(r2_2, 4)
    # }
    
    # ì´ ì†Œìš” ì‹œê°„
    elapsed_time = time.time() - start_time
    training_stats["training_time_seconds"] = round(elapsed_time, 2)
    
    print("\n" + "="*60)
    print("ğŸ‰ ì „ì²´ ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
    print(f"â±ï¸  ì´ ì†Œìš” ì‹œê°„: {elapsed_time:.2f}ì´ˆ")
    print("="*60)
    
    return training_stats

if __name__ == "__main__":
    train()
